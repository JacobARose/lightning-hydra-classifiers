# -*- coding: utf-8 -*-
"""multimodel-confusion-matrices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17OZooKX6KCLOHHWROgQYb57Uc0s-A6es

# sklearn Dummy classifiers demo
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
# from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve
from IPython.display import display

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
# X, y = load_iris(return_X_y=True)
# y[y != 1] = -1

from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate(y, y_hat, labels=None, title="Confusion Matrix", cbar_ax=None):
    print(classification_report(y,y_hat, zero_division=0))
    cm = confusion_matrix(y, y_hat, normalize='true')
    # cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    cmat = pd.DataFrame(cm)
    if labels:
        cmat.columns = labels
        cmat.set_index([pd.Index(labels, '')],inplace=True)

    # cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)
    # cmap = sns.dark_palette("blue", n_colors=len(cm), as_cmap=True, input="husl")
    cmap = sns.color_palette("OrRd", 10, as_cmap=True)

    
    ax = sns.heatmap(cmat,cmap=cmap, annot=True,
                     vmin=0, vmax=1, cbar_kws={"shrink": .8},
                     cbar=bool(cbar_ax is not None),
                     cbar_ax=cbar_ax) #"YlGnBu", annot=True)
    plt.title(title, loc='left', fontsize=18)

    return cmat

sns.choose_light_palette()

sns.choose_dark_palette()

def judge_model(x_train, y_train,
                x_test, y_test,
                model, name,
                plot=False,
                figsize=(20,10),
                cbar_ax=None,
                axis_label_fontsize=14):
    print(name)
    print('-'*20)
    # plt.subplots(1,2, figsize=figsize)
    # plt.figure(figsize=figsize)
    print('Training Performance')
    y_pred_train = model.predict(x_train)
    y_prob_train = model.predict_proba(x_train)
    train_acc = accuracy_score(y_train, y_pred_train) #model.predict(x_train))
    train_auc = roc_auc_score(y_train, y_prob_train, multi_class="ovo")
    plt.subplot(121)
    train_cm = evaluate(y_train, y_pred_train, labels=None, title="train Confusion Matrix\n"+f"macro_acc:{train_acc:.2f},auc:{train_auc:.2f}", cbar_ax=None)
    ax = plt.gca()
    # ax.set(ylabel="True Label", 
    #        xlabel="Predicted Label")
    ax.set_ylabel("True Label", fontsize=axis_label_fontsize)
    ax.set_xlabel("Predicted Label", fontsize=axis_label_fontsize)
    ax.set_yticks(range(len((set(y_train)))))
    ax.set_yticklabels(sorted(set(y_train)))
    print('-> Acc:',  train_acc)
    print('-> AUC:', train_auc)
    
    print('Testing Performance')
    y_pred_test = model.predict(x_test)
    y_prob_test = model.predict_proba(x_test)
    test_acc = accuracy_score(y_test, y_pred_test) #model.predict(x_test))
    test_auc = roc_auc_score(y_test, y_prob_test, multi_class="ovo")
    print('-> Acc:',  test_acc)
    print('-> AUC:', test_auc)
    plt.subplot(122)
    test_cm = evaluate(y_test, y_pred_test, labels=None, title="test Confusion Matrix\n"+f"macro_acc:{test_acc:.2f},auc:{test_auc:.2f}", cbar_ax=cbar_ax)
    ax = plt.gca()
    ax.set_yticks([]) # range(len((set(y_train)))))
    ax.set_yticklabels([]) # sorted(set(y_train))
    ax.set_xlabel("Predicted Label", fontsize=axis_label_fontsize)
    # set_yticklabels(heat_map.get_yticklabels(), rotation=35)
    
    ax.yaxis.set_label_position("right")
    ax.yaxis.tick_right()
    

    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.01, hspace=None)
    # if plot:
    #     fpr, tpr, thres = roc_curve(y_test, model.predict_proba(x_test)[:, 1])
    #     plt.figure(figsize=(4, 4))
    #     plt.plot(fpr, tpr, label='Test')
    #     plt.xlabel('FPR')
    #     plt.ylabel('TPR')
    #     plt.show()

    return {"test":
                   {
                   "acc": test_acc,
                   "auc": test_auc,
                   "cm": test_cm
                   },
            
            "train":
                   {
                    "acc":train_acc,
                    "auc":train_auc,
                    "cm":train_cm
                   }
            }

from rich import print as pp
from sklearn.datasets import make_classification
from sklearn.dummy import DummyClassifier
x, y = make_classification(n_samples=1000, n_classes=19, n_clusters_per_class=1, n_informative=10, random_state=42)
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, test_size=0.3, stratify=y)

figsize = (26,13)
all_results = []
# Baseline (AUC should be 0.5 because we're guessing even though the accuracies are different)
for strategy in ['stratified', 'most_frequent', 'prior', 'uniform']:
    dummy = DummyClassifier(strategy=strategy)
    dummy.fit(x_train, y_train)
    logs = {"strategy": strategy}
    plt.subplots(1,2, figsize=figsize, sharex=True, sharey=True)
    plt.suptitle(f"Dummy {strategy} Classifier", fontsize=24)
    cbar_ax = plt.gcf().add_axes([.91, .3, .03, .4])
    logs['results'] = judge_model(x_train, y_train,
                          x_test, y_test,
                          dummy, 'Dummy {}'.format(strategy),
                          plot=True, cbar_ax=cbar_ax)
    pp(f"Strategy: {strategy}" + "\n" + f"Results: " + "\n" + f"{results}")
    all_results.append(logs)

# clf = DummyClassifier(strategy='stratified')
# clf.fit(x, y)

# Commented out IPython magic to ensure Python compatibility.
# %debug

for log in all_results:
    print(f"strategy:{log['strategy']}")
    print(f"train:acc={log['results']['acc']}, auc={log['results']['auc']}")

import pandas as pd
import matplotlib.pyplot as plt

y_train_df, y_test_df = pd.DataFrame(y_train), pd.DataFrame(y_test)
y_train_df



y_train_df.value_counts()
y_test_df.value_counts()

from sklearn.dummy import DummyClassifier
from sklearn.svm import SVC
clf = SVC(kernel='linear', C=1).fit(X_train, y_train)
clf.score(X_test, y_test)

clf = DummyClassifier(strategy='most_frequent', random_state=0)
clf.fit(X_train, y_train)

clf.score(X_test, y_test)

clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)
clf.score(X_test, y_test)



"""# Gradio demo"""

!pip install gradio wandb

!wandb login

import gradio as gr
input = gr.inputs.Image(type='pil', label="Original Image")
output = gr.outputs.Image(type='pil')
gr.Interface(denoising_model, input, output).launch()

import torch, requests
from torchvision import transforms
from PIL import Image
torch.hub._validate_not_a_forked_repo=lambda a,b,c: True

model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)
model = model.eval()

# Download human-readable labels for ImageNet.
response = requests.get("https://git.io/JJkYN")
labels = response.text.split("\n")

def predict(inp):
  inp = Image.fromarray(inp.astype('uint8'), 'RGB')
  inp = transforms.ToTensor()(inp).unsqueeze(0)
  with torch.no_grad():
    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)
  return {labels[i]: float(prediction[i]) for i in range(1000)}

import gradio as gr

inputs = gr.inputs.Image()
outputs = gr.outputs.Label(num_top_classes=3)
io = gr.Interface(fn=predict, inputs=inputs, outputs=outputs)
io.launch(share=True)

import wandb
wandb.init(project="your-test-project")

io.integrate(wandb=wandb)

