# @package _global_

# Created Saturday, July 10th, 2021
# Author: Jacob A Rose
# 

# to execute this experiment run:
# python run.py +experiment=multi-gpu_experiment.yaml

#Based on original experiment: "/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/experiment/4_acc=ddp-16bit-precision-2-gpu.yaml"
defaults:
#     - override /model: 
    - trainer: ../../trainer/ddp_trainer.yaml
    - model: ../../model/transfer_learning_model.yaml
    - datamodule: ../../datamodule/fossil_datamodule.yaml

#     - override /logger: many_loggers.yaml
    - override /hydra/hydra_logging: colorlog
    - override /hydra/job_logging: colorlog


# hydra:
#     run:
#         dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

job_type: "${hparams.dataset}_train-test"
root_dir: ${oc.env:WANDB_CACHE_DIR}
work_dir: "/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021"

job_dir: "${work_dir}/${job_type}"
log_dir: "${job_dir}/${now:%Y-%m-%d}/${now:%H-%M}"
# results_dir: '${model.model_dir}/results'
seed: 12345
print_config: true


hparams_log_path: '${job_dir}/hparams/best_hparams.yaml'
hparams:
    batch_size: 128 # 256 # 8 #'default' #null
    lr: 2e-03 #'default' #null # base learning rate
    num_classes: null
    classes: null
    class_type: "family"
    image_size: 512 #1024
    dataset: '${datamodule.name}'  # "Extant"
    backbone: "resnet50"
##################################
##################################

# datamodule:
#     _target_: contrastive_learning.data.pytorch.pnas.PNASLightningDataModule
#     _target_: contrastive_learning.data.pytorch.datamodules.build_datamodule
#     basename: "${hparams.dataset}"
#     name: '${.basename}_family_10_${.image_size}'
#     batch_size: '${hparams.batch_size}'
#     val_split: 0.2
#     test_split: 0.0
#     classes: '${hparams.classes}'
#     num_classes: '${hparams.num_classes}'
#     image_size: '${hparams.image_size}'
#     channels: 3
#     class_type: '${hparams.class_type}'
#     normalize: true
#     seed: '${seed}'
#     predict_on_split: "val"
#     num_workers: 0
#     root_dir: ${root_dir}
#     dataset_dir: ${.root_dir}/datasets/${.name}

##################################
##################################

model:
    model_dir: '${log_dir}/model'

#     _target_: lightning_hydra_classifiers.models.transfer.TransferLearningModel
    
#     classifier:
#         _target_: lightning_hydra_classifiers.models.transfer.Classifier
#         backbone_name: '${hparams.backbone}'
#         num_classes: '${hparams.num_classes}'
#         finetune: true

#     train_bn: '${callbacks.finetuning.train_bn}'
#     milestones: '${callbacks.finetuning.milestones}'
#     batch_size: '${hparams.batch_size}'
#     optimizer: "Adam"
#     lr: '${hparams.lr}'
#     lr_scheduler_gamma: 0.1
#     num_workers: 6


##################################
##################################

# trainer:
#     _target_: pytorch_lightning.Trainer
#     gpus: [5,6]
#     auto_select_gpus: true
# #     precision: 16
#     accelerator: 'ddp'
#     min_epochs: 1
#     max_epochs: 15

#     weights_summary: "top"
# #     progress_bar_refresh_rate: 10

#     profiler: "simple"
#     log_every_n_steps: 50 

#     amp_backend: "native"
#     amp_level: "02"
#     precision: 16
    # resume_from_checkpoint: ${work_dir}/last.ckpt


callbacks:
    finetuning:
        _target_: lightning_hydra_classifiers.models.transfer.MilestonesFinetuning
        milestones: [3, 5, 10]
        train_bn: false

    checkpoint:
        _target_: pytorch_lightning.callbacks.ModelCheckpoint
        monitor: 'val_loss'
        dirpath: '${model.model_dir}/checkpoints'
        filename: 'best_model-epoch-{epoch:02d}--{val_loss:.2f}'
        save_top_k: 2
        mode: 'min'


#     uploadcodeasartifact:
#         _target_: lightning_hydra_classifiers.utils.logging_utils.UploadCodeAsArtifact
#         code_dir: "/media/data/jacob/GitHub/lightning-hydra-classifiers"

    uploadaheckpointsasartifact:
        _target_: lightning_hydra_classifiers.utils.logging_utils.UploadCheckpointsAsArtifact
        ckpt_dir: "${model.model_dir}/checkpoints"
        upload_best_only: false

    logconfusionmatrix:
        _target_: lightning_hydra_classifiers.utils.logging_utils.LogConfusionMatrix
        class_names: "${hparams.classes}"



wandb:
    init:
        _target_: wandb.init
        entity: '${oc.env:WANDB_ENTITY}' # "jrose"
        project: "mnist_scaling_experiments"
        job_type: '${job_type}'
        group: "2-gpu"
#         magic: true
        mode: "online" # "offline" # "disabled"
        allow_val_change: true
        sync_tensorboard: true
        reinit: true
        id: "${now:%Y-%m-%d}-${now:%H-%M}"


# logger:
#     wandb:
#         _target_: pytorch_lightning.loggers.wandb.WandbLogger
# #         entity: '${oc.env:WANDB_ENTITY}' # "jrose"
# #         project: "mnist_scaling_experiments"
# #         job_type: '${job_type}'
# #         group: "2-gpu"
# #         mode: "online" # "offline" # "disabled"
# #         allow_val_change: true
# #         sync_tensorboard: true
# #         id: 
# #         resume: 

#     tensorboard:
#         _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
#         save_dir: "${log_dir}/tensorboard/"
#         name: '${hparams.backbone}'


hydra:
    # output paths for hydra logs
    run:
        dir: ${log_dir}/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
#     sweep:
#         dir: logs/multiruns/${now:%Y-%m-%d_%H-%M-%S}
#         subdir: ${hydra.job.num}

    job:
        env_set:
            # currently there are some issues with running sweeps alongside wandb
            # https://github.com/wandb/client/issues/1314
            # this env var fixes that
            WANDB_START_METHOD: thread

# tuner:
#     instantiate:
#         _target_: pytorch_lightning.tuner.tuning.Tuner
#         # pass in the trainer
        
#     options:
#         force_rerun: false
    
#     scale_batch_size:
#         log_name: 'tuned_batch_size'
# #         log_path: '${log_dir}/hparams/${.log_name}.yaml'
#         kwargs:
#             mode: 'power'
#             steps_per_trial: 3
#             init_val: 4
#             max_trials: 25
#             batch_arg_name: 'batch_size'
#         tuned: false
#     # `batch_arg_name` should be the name of an attribute in one of the following:
#     # model
#     # model.hparams
#     # model.datamodule
#     # trainer.datamodule (the datamodule passed to the tune method)
    
#     lr_find:
#         log_name: 'tuned_learning_rate'
# #         log_path: '${log_dir}/hparams/${.log_name}.yaml'
#         kwargs:
#             min_lr: 1e-08
#             max_lr: 1
#             num_training: 100
#             mode: 'exponential'
#             early_stop_threshold: 4.0
#         tuned: false