finetuning:
    _target_: lightning_hydra_classifiers.models.transfer.MilestonesFinetuning
    milestones: '${hparams.finetune_milestones}'
    train_bn: false

checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint        
    monitor: 'val_loss'
    dirpath: '${model.model_dir}/checkpoints'
    filename: '{epoch number}-{global_step}-{val_loss}'
#     filename: 'best_model-epoch-{epoch:02d}--{val_loss:.2f}'
    save_top_k: 2
    mode: 'min'

uploadaheckpointsasartifact:
    _target_: lightning_hydra_classifiers.utils.logging_utils.UploadCheckpointsAsArtifact
    ckpt_dir: '${..checkpoint.dirpath}'
    upload_best_only: false

logconfusionmatrix:
    _target_: lightning_hydra_classifiers.utils.logging_utils.LogConfusionMatrix
    class_names: "${hparams.classes}"



log_per_class_metrics_to_wandb:
    _target_: lightning_hydra_classifiers.callbacks.wandb_callbacks.LogPerClassMetricsToWandb
    class_names: '${hparams.classes}'


# early_stopping:
#     _target_: pytorch_lightning.callbacks.early_stopping.EarlyStopping
#     code_dir: ${work_dir}
#     monitor: 'val_loss'
#     patience: 3
#     verbose: false
#     mode: "min"

module_data_monitor:
    _target_: pl_bolts.callbacks.ModuleDataMonitor

