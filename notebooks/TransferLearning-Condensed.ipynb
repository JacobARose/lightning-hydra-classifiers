{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Transfer Learning Experiments\n",
    "\n",
    "Created by: Jacob A Rose  \n",
    "Created On: Wednesday Oct 6th, 2021  \n",
    "\n",
    "Based on Notebook located at: https://jarvislabs.ai/blogs/transfer-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pp\n",
    "\n",
    "# import lux\n",
    "import pandas as pd\n",
    "# lux.config.default_display = \"pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms as t\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.Logger(__name__)\n",
    "logger.setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model, verbose: bool=True):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        param = parameter.numel()\n",
    "        \n",
    "        total_params += param\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        table.add_row([name, param])\n",
    "        trainable_params+=param\n",
    "    if verbose:\n",
    "        print(table)\n",
    "    print(f\"Total Trainable Params: {trainable_params:,}\")\n",
    "    print(f\"Total non-Trainable Params: {total_params-trainable_params:,}\")\n",
    "    return table\n",
    "\n",
    "\n",
    "\n",
    "from more_itertools import unzip\n",
    "from toolz.itertoolz import concat\n",
    "\n",
    "def collect_results(results):\n",
    "    \"\"\"\n",
    "    Converts a list of flat records/rows to a list of tall, row-wise concatenated columns\n",
    "    \n",
    "    Useful for collecting a set of batches or individual samples of 0- and 1-dimensional tensors returned by each model step, collected in a list in model.on_epoch_end.\n",
    "    \n",
    "    in:\n",
    "        [(item_00, item_01,... item_0C),\n",
    "         (item_10, item_11,... item_1C),\n",
    "         ...\n",
    "         (item_N0, item_N1,... item_NC)]\n",
    "     out:\n",
    "         [[item_00, item_10,... item_N0],\n",
    "         [item_01, item_11,... item_N1],\n",
    "         ...\n",
    "         [item_0C, item_1C,... item_NC]]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    rows = [list(concat(r)) for r in unzip(results)]\n",
    "    cols = []*len(rows)\n",
    "    print(cols)\n",
    "    for i, row in enumerate(rows):\n",
    "        if isinstance(row[0], torch.Tensor):\n",
    "            if len(row[0].shape) <= 1:\n",
    "                cols.append(torch.stack(row, dim=0).cpu().numpy())\n",
    "            else:\n",
    "                cols.append(torch.cat(row).cpu().numpy())\n",
    "        elif isinstance(row[0], list):\n",
    "            cols.extend(list(concat(row)))\n",
    "        elif isinstance(row[0], (str, int)):\n",
    "            cols.append(list(row))\n",
    "\n",
    "    np.all([len(c)==len(cols[0]) for c in cols])\n",
    "    \n",
    "    return cols\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# table = count_parameters(model)\n",
    "\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/hirune924/lightning-hydra/blob/master/layer/layer.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "# from typing import Optional\n",
    "import timm\n",
    "import glob\n",
    "import hydra\n",
    "from collections import OrderedDict\n",
    "from typing import *\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return torch.flatten(x, start_dim=1)\n",
    "    \n",
    "\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`.\"\n",
    "\n",
    "    def __init__(self, sz: Optional[int] = None):\n",
    "        super(AdaptiveConcatPool2d, self).__init__()\n",
    "        \"Output will be 2*sz or 2 if sz is None\"\n",
    "        self.output_size = sz or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.output_size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "    \n",
    "\n",
    "def build_global_pool(pool_type: str=\"avg\",\n",
    "                      pool_size: int=1, \n",
    "                      feature_size: Optional[int]=0):\n",
    "\n",
    "#     head = OrderedDict()\n",
    "    if pool_type == 'avg':\n",
    "        global_pool = nn.AdaptiveAvgPool2d(pool_size)\n",
    "        return global_pool, feature_size\n",
    "        \n",
    "    elif pool_type == 'avgdrop':\n",
    "        global_pool = nn.Sequential(nn.AdaptiveAvgPool2d(pool_size),\n",
    "                                    nn.Dropout2d(p=0.3, inplace=False))\n",
    "        return global_pool, feature_size\n",
    "    elif pool_type == 'avgmax':\n",
    "        feature_size = feature_size * 2\n",
    "        global_pool = AdaptiveConcatPool2d(pool_size)\n",
    "        return global_pool, feature_size\n",
    "\n",
    "    elif pool_type == 'max':\n",
    "        global_pool = nn.AdaptiveMaxPool2d(pool_size)\n",
    "        return global_pool, feature_size\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(f\"pool_type={pool_type} invalid.\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, ckpt_path: str):\n",
    "    ckpt_path = glob.glob(hydra.utils.to_absolute_path(ckpt_path))\n",
    "    model = model.load_state_dict(torch.load(ckpt_path[0]))\n",
    "    return model\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def build_timm_backbone(backbone_name='gluon_seresnext50_32x4d',\n",
    "                         pretrained: Union[bool, str]=True,\n",
    "                         num_classes: int=1000,\n",
    "                         feature_layer: int=-2):\n",
    "    if pretrained == \"imagenet\":\n",
    "        num_classes = 1000\n",
    "\n",
    "    model = timm.create_model(model_name=backbone_name, num_classes=num_classes, pretrained=pretrained)\n",
    "    if isinstance(pretrained, str) and pretrained != \"imagenet\":\n",
    "        model = load_model_checkpoint(model, ckpt_path=pretrained)\n",
    "        \n",
    "#     body = nn.Sequential(*list(model.children())[:feature_layer])\n",
    "\n",
    "    body = nn.Sequential(OrderedDict(list(model.named_children())[:feature_layer]))\n",
    "    \n",
    "    return body\n",
    "\n",
    "\n",
    "def build_model_backbone(backbone_name='gluon_seresnext50_32x4d',\n",
    "                         pretrained: Union[bool, str]=True,\n",
    "                         num_classes: int=1000,\n",
    "                         feature_layer: int=-2,\n",
    "                         model_repo: str= \"timm\"):\n",
    "\n",
    "    if model_repo == \"timm\":\n",
    "        return build_timm_backbone(backbone_name=backbone_name,\n",
    "                                   pretrained=pretrained,\n",
    "                                   num_classes=num_classes,\n",
    "                                   feature_layer=feature_layer)\n",
    "    # TBD Add other pretrained model backends\n",
    "    raise NotImplementedError(f\"Invalid model_repo={model_repo}\")\n",
    "    \n",
    "\n",
    "def build_model_head(num_classes: int=1000,\n",
    "                     pool_size: int=1,\n",
    "                     pool_type: str='avg',\n",
    "                     head_type: str='linear',\n",
    "                     feature_size: int=512,\n",
    "                     hidden_size: Optional[int]=512):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns a nn.Sequential model containing 3 children:\n",
    "        global_pool -> flatten -> classifier\n",
    "        \n",
    "    Available pool_types:\n",
    "        - \"avg\"\n",
    "            global_avg_pool\n",
    "        - \"avgdrop\"\n",
    "            global_avg_pool -> dropout\n",
    "        - \"avgmax\"\n",
    "            [global_avg_pool | global_max_pool]\n",
    "        \"max\"\n",
    "            global_max_pool\n",
    "            \n",
    "            \n",
    "    pool_types to be explored:\n",
    "        - \"maxdrop\"\n",
    "            global_max_pool -> dropout\n",
    "        - \"avgmaxdrop\"\n",
    "            [global_avg_pool | global_max_pool] -> dropout\n",
    "\n",
    "        \n",
    "    Available head_types:\n",
    "        - linear\n",
    "        \n",
    "        - custom\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    head = OrderedDict()\n",
    "    global_pool, feature_size = build_global_pool(pool_type=pool_type,\n",
    "                                                  pool_size=pool_size,\n",
    "                                                  feature_size=feature_size)\n",
    "    head[\"global_pool\"] = global_pool\n",
    "    head[\"flatten\"] = Flatten()\n",
    "    \n",
    "    classifier_input_feature_size = feature_size*(pool_size**2)\n",
    "    if head_type=='linear':\n",
    "        hidden_size = 0\n",
    "        head[\"classifier\"] = nn.Linear(classifier_input_feature_size, num_classes)\n",
    "    elif head_type=='custom':\n",
    "        head[\"classifier\"] = nn.Sequential(nn.Linear(classifier_input_feature_size, hidden_size),\n",
    "                                nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False),\n",
    "                                nn.BatchNorm1d(hidden_size),\n",
    "                                nn.Linear(hidden_size, num_classes))\n",
    "    head = nn.Sequential(head)\n",
    "    return head\n",
    "\n",
    "\n",
    "\n",
    "def build_model(backbone_name='gluon_seresnext50_32x4d',\n",
    "                pretrained: Union[bool, str]=True,\n",
    "                num_classes: int=1000,\n",
    "                pool_size: int=1,\n",
    "                pool_type: str='avg',\n",
    "                head_type: str='linear',\n",
    "                hidden_size: Optional[int]=512):\n",
    "    \n",
    "    backbone = build_model_backbone(backbone_name=backbone_name,\n",
    "                                    pretrained=pretrained,\n",
    "                                    num_classes=num_classes,\n",
    "                                    feature_layer=-2)\n",
    "    \n",
    "    feature_size = list(backbone.parameters())[-1].shape[0]\n",
    "    \n",
    "\n",
    "    head = build_model_head(num_classes=num_classes,\n",
    "                            pool_size=pool_size,\n",
    "                            pool_type=pool_type,\n",
    "                            head_type=head_type,\n",
    "                            feature_size=feature_size,\n",
    "                            hidden_size=hidden_size)\n",
    "    \n",
    "    model = nn.Sequential(OrderedDict({\n",
    "        \"backbone\":backbone,\n",
    "        \"head\":head\n",
    "    }))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.utils.metric_utils import get_per_class_metrics, get_scalar_metrics\n",
    "                \n",
    "class LayerSelectPlugin:\n",
    "    \"\"\"\n",
    "    LayerSelectPlugin\n",
    "\n",
    "    To be subclassed by main Pytorch Lightning Module\n",
    "    \n",
    "    Available methods:\n",
    "    * classmethods:\n",
    "        - count_parameters\n",
    "    * instance methods:\n",
    "        - get_batchnorm_modules\n",
    "        - get_conv_modules\n",
    "        - get_linear_modules\n",
    "        - get_named_parameters\n",
    "        - get_named_modules\n",
    "        - get_trainable_parameters\n",
    "        - get_nontrainable_parameters\n",
    "        - count_trainable_batchnorm_layers\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def get_batchnorm_modules(self):\n",
    "        return self.get_named_modules(\"bn\")\n",
    "    \n",
    "    def get_conv_modules(self):\n",
    "        return self.get_named_modules(\"conv\")\n",
    "    \n",
    "    def get_linear_modules(self):\n",
    "        return ((n,m) for n,m in self.named_modules() if isinstance(m, nn.modules.Linear))\n",
    "    \n",
    "    \n",
    "    def get_named_parameters(self,\n",
    "                             filter_pattern: Optional[str]=None,\n",
    "                             trainable: bool=False,\n",
    "                             nontrainable: bool=False):\n",
    "        \n",
    "        named_params = self.named_parameters()\n",
    "        if isinstance(filter_pattern, str):\n",
    "            named_params = ((n,p) for n,p in named_params if filter_pattern in n)\n",
    "        if trainable and nontrainable:\n",
    "            logger.warning('Returning all parameters regardless of the values of requires_grad.')\n",
    "            return named_params\n",
    "        if trainable:\n",
    "            return ((n, p) for n, p in named_params if p.requires_grad)\n",
    "        if nontrainable:\n",
    "            return ((n, p) for n, p in named_params if not p.requires_grad)\n",
    "        return named_params\n",
    "\n",
    "    def get_named_modules(self,\n",
    "                          filter_pattern: Optional[str]=None):\n",
    "        if isinstance(filter_pattern, str):\n",
    "            return ((n,l) for n,l in self.named_modules() if filter_pattern in n)\n",
    "        return self.named_modules()\n",
    "            \n",
    "    def get_trainable_parameters(self):\n",
    "        return (p for _, p in self.get_named_parameters(trainable=True))\n",
    "#         return (p for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def get_nontrainable_parameters(self):\n",
    "        return (p for _, p in self.get_named_parameters(nontrainable=True))\n",
    "#         return (p for p in self.parameters() if not p.requires_grad)\n",
    "\n",
    "    def count_trainable_batchnorm_layers(self) -> None:\n",
    "        is_training = np.array([m.training for _, m in self.get_batchnorm_modules()])\n",
    "        print(f\"trainable batchnorm modules:{np.sum(is_training)}\")\n",
    "        print(f\"nontrainable batchnorm modules:{np.sum(~is_training)}\")\n",
    "\n",
    "        is_training = np.array([p.requires_grad for _, p in self.get_named_parameters(\"bn\")])\n",
    "        print(f\"batchnorm params with requires_grad=True: :{np.sum(is_training)}\")\n",
    "        print(f\"batchnorm params with requires_grad=False:{np.sum(~is_training)}\")\n",
    "        \n",
    "    @classmethod\n",
    "    def count_parameters(cls, model, verbose: bool=True) -> PrettyTable:\n",
    "        return count_parameters(model, verbose=verbose)\n",
    "\n",
    "\n",
    "    \n",
    "class FinetuningLightningPlugin:\n",
    "    \"\"\"\n",
    "    FinetuningLightningPlugin\n",
    "    \n",
    "    To be subclassed by main Pytorch Lightning Module\n",
    "    \n",
    "    Available methods:\n",
    "    * classmethods:\n",
    "        - freeze_up_to\n",
    "        - freeze\n",
    "        - unfreeze\n",
    "        - freeze_bn\n",
    "        - set_bn_eval\n",
    "    * instance methods:\n",
    "        - freeze_backbone\n",
    "        - unfreeze_backbone_top_layers\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def freeze_up_to(cls, \n",
    "                     module, \n",
    "                     stop_layer: Union[int, str]=None,\n",
    "                     freeze_bn: bool=True):\n",
    "\n",
    "        cls.unfreeze(module, freeze_bn=freeze_bn)\n",
    "\n",
    "        if isinstance(stop_layer, str):\n",
    "            modules = list(module.named_modules())\n",
    "        elif isinstance(stop_layer, int) or (stop_layer is None):\n",
    "            modules = list(enumerate(module.modules()))\n",
    "\n",
    "        for module_id, m in modules:\n",
    "            if stop_layer == module_id:\n",
    "                logger.warning(f\"Stopping at layer: {n}\")\n",
    "                break\n",
    "            cls.freeze(m, freeze_bn=freeze_bn)\n",
    "#             for param_id, param in m.named_parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             m.eval()\n",
    "#             cls.freeze_bn(m, freeze_bn)\n",
    "            logger.warning(f\"Layer {module_id}: type={type(m)}|training={m.training}\")\n",
    "            logger.warning(f\"requires_grad={np.all([p.requires_grad for p in m.parameters()])}\")\n",
    "\n",
    "    @classmethod\n",
    "    def freeze(cls,\n",
    "               module,\n",
    "               freeze_bn: bool=True,\n",
    "               filter_pattern: Optional[str]=None):\n",
    "        modules = list(module.named_modules())\n",
    "        for n, m in modules:\n",
    "#             if filter_pattern not in n:\n",
    "            if isinstance(filter_pattern, str) and (filter_pattern not in n):\n",
    "                continue\n",
    "#             m.eval()\n",
    "            for p_name, p in m.named_parameters():\n",
    "#                 if filter_pattern not in n:\n",
    "                if isinstance(filter_pattern, str) and (filter_pattern not in n):\n",
    "                    continue\n",
    "                if freeze_bn or not isinstance(m, nn.BatchNorm2d):\n",
    "                    p.requires_grad=False\n",
    "            cls.freeze_bn(m, freeze_bn)        \n",
    "\n",
    "            \n",
    "    @classmethod\n",
    "    def unfreeze(cls,\n",
    "                 module,\n",
    "                 filter_pattern: Optional[str]=None):\n",
    "        modules = list(module.named_modules())\n",
    "        for n, m in modules:\n",
    "            if isinstance(filter_pattern, str) and (filter_pattern not in n):\n",
    "                continue\n",
    "            for p_name, p in m.named_parameters():\n",
    "                p.requires_grad=True\n",
    "            m.train()\n",
    "\n",
    "                \n",
    "    @classmethod\n",
    "    def freeze_bn(cls, module: nn.Module, freeze_bn: bool):\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d): \n",
    "                if freeze_bn:\n",
    "                    for p in m.parameters():\n",
    "                        p.requires_grad = False\n",
    "#                     m.eval() \n",
    "#                 else:\n",
    "#                     m.train()\n",
    "\n",
    "    @classmethod\n",
    "    def set_bn_eval(cls, module: nn.Module)->None:\n",
    "        \"Set bn layers in eval mode for all recursive children of `m`.\"\n",
    "        for l in module.children():\n",
    "            if isinstance(l, nn.BatchNorm2d) and not next(l.parameters()).requires_grad:\n",
    "                l.eval()\n",
    "            cls.set_bn_eval(l)\n",
    "\n",
    "\n",
    "    def freeze_backbone(self, freeze_bn: bool=True):\n",
    "        \n",
    "        self.freeze(self.model.backbone,\n",
    "                    freeze_bn=freeze_bn)\n",
    "        self.unfreeze(self.model.head)\n",
    "\n",
    "        \n",
    "    def unfreeze_backbone_top_layers(self,\n",
    "                                     unfreeze_down_to: Union[str,int]=-1):\n",
    "        if isinstance(unfreeze_down_to, str):\n",
    "            layers = list(reversed(list(self.model.backbone.named_children())))\n",
    "        if isinstance(unfreeze_down_to, int):\n",
    "            if unfreeze_down_to == 0:\n",
    "                print(f\"Pass non-zero integer or str label name to unfreeze layers. Returning without change.\")\n",
    "                return\n",
    "            layers = list(reversed(list(enumerate(self.model.backbone.children()))))\n",
    "            if unfreeze_down_to < 0:\n",
    "                unfreeze_down_to = len(layers) + unfreeze_down_to\n",
    "        \n",
    "        for layer_id, l in layers:\n",
    "            if layer_id == unfreeze_down_to:\n",
    "                break\n",
    "            self.unfreeze(l)\n",
    "\n",
    "\n",
    "\n",
    "class LightningMetricsPlugin:\n",
    "    \"\"\"\n",
    "    LightningMetricsPlugin\n",
    "    \n",
    "    To be subclassed by main Pytorch Lightning Module\n",
    "    \n",
    "    Available methods:\n",
    "    * instance methods:\n",
    "        - log_metric_step\n",
    "        - init_metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def log_metric_step(self,\n",
    "                        stage: str='train',\n",
    "                        omit_metric_types: Optional[List[str]]=None,\n",
    "                        omit_metric_keys: Optional[List[str]]=None):\n",
    "        omit_metric_types = omit_metric_types or []\n",
    "        omit_metric_keys = omit_metric_keys or []\n",
    "        \n",
    "        for metric_type, metric_collection in self.all_metrics[stage].items():\n",
    "            if metric_type in omit_metric_types:\n",
    "                continue\n",
    "            if metric_type == \"scalar\":\n",
    "                self.log_dict({k:v for k,v in metric_collection.items() if k not in omit_metric_keys},\n",
    "                               on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "            elif metric_type == \"per_class\":\n",
    "                for k,v in metric_collection.items():\n",
    "                    if k in omit_metric_keys:\n",
    "                        continue\n",
    "                    results = v.compute()\n",
    "                    for class_idx, result in enumerate(results): #range(len(results)):\n",
    "                        self.log(f\"{k}_class_{class_idx}\", result,\n",
    "                                 on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "            else:\n",
    "                logger.warning(f\"[Warning] {metric_type} requires specialized handling in lightningmodule.log_metric_step().\")\n",
    "\n",
    "    def init_metrics(self,\n",
    "                     stage: str='train',\n",
    "                     tag: Optional[str]=None):\n",
    "        tag = tag or \"\"\n",
    "        if not hasattr(self, \"all_metrics\"):\n",
    "            self.all_metrics = {}\n",
    "        \n",
    "        if not hasattr(self,\"num_classes\") and hasattr(self.hparams, \"num_classes\"):\n",
    "            self.num_classes = self.hparams.num_classes\n",
    "        \n",
    "        print(f\"self.num_classes={self.num_classes}\")\n",
    "        if stage in ['train', 'all']:\n",
    "            prefix=f'{tag}_train'.strip(\"_\")\n",
    "            self.metrics_train = get_scalar_metrics(num_classes=self.num_classes, average='macro', prefix=prefix)\n",
    "            self.metrics_train_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix='train')\n",
    "            self.all_metrics['train'] = {\"scalar\":self.metrics_train,\n",
    "                                         \"per_class\":self.metrics_train_per_class}\n",
    "            \n",
    "        if stage in ['val', 'all']:\n",
    "            prefix=f'{tag}_val'.strip(\"_\")\n",
    "            self.metrics_val = get_scalar_metrics(num_classes=self.num_classes, average='macro', prefix=prefix)\n",
    "            self.metrics_val_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix='val')\n",
    "            self.all_metrics['val'] = {\"scalar\":self.metrics_val,\n",
    "                                       \"per_class\":self.metrics_val_per_class}\n",
    "            \n",
    "        if stage in ['test', 'all']:\n",
    "            prefix=f'{tag}_test'.strip(\"_\")\n",
    "            self.metrics_test = get_scalar_metrics(num_classes=self.num_classes, average='macro', prefix=prefix)\n",
    "            self.metrics_test_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix='test')\n",
    "            self.all_metrics['test'] = {\"scalar\":self.metrics_test,\n",
    "                                        \"per_class\":self.metrics_test_per_class}\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "class BaseLightningModule(LightningMetricsPlugin,\n",
    "                          FinetuningLightningPlugin,\n",
    "                          LayerSelectPlugin,\n",
    "                          pl.LightningModule):\n",
    "    \"\"\"\n",
    "    BaseLightningModule\n",
    "    \n",
    "    Additional methods made available through subclassed plugins.\n",
    "    \n",
    "        Available methods:\n",
    "    * instance methods:\n",
    "        - step\n",
    "        - update_metric_step\n",
    "        - training_step\n",
    "        - validation_step\n",
    "        - test_step\n",
    "    \n",
    "    Plugins:\n",
    "    \n",
    "        -- LightningMetricsPlugin\n",
    "            * instance methods:\n",
    "                - log_metric_step\n",
    "                - init_metrics\n",
    "        -- FinetuningLightningPlugin\n",
    "            * classmethods:\n",
    "                - freeze_up_to\n",
    "                - freeze\n",
    "                - unfreeze\n",
    "                - freeze_bn\n",
    "                - set_bn_eval\n",
    "            * instance methods:\n",
    "                - freeze_backbone\n",
    "                - unfreeze_backbone_top_layers        \n",
    "        -- LayerSelectPlugin\n",
    "            * classmethods:\n",
    "                - count_parameters\n",
    "            * instance methods:\n",
    "                - get_batchnorm_modules\n",
    "                - get_conv_modules\n",
    "                - get_linear_modules\n",
    "                - get_named_parameters\n",
    "                - get_named_modules\n",
    "                - get_trainable_parameters\n",
    "                - get_nontrainable_parameters\n",
    "                - count_trainable_batchnorm_layers\n",
    "        \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed: Optional[int]=None):\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "        pl.seed_everything(seed)\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        image, y_true = batch[0], batch[1]\n",
    "        y_logit = self(image)\n",
    "        y_pred = torch.argmax(y_logit, dim=-1)\n",
    "        return y_logit, y_true, y_pred    \n",
    "    \n",
    "    def update_metric_step(self,\n",
    "                           y_logit,\n",
    "                           y_true,\n",
    "                           stage: str='train'):\n",
    "        out = {}\n",
    "        for metric_type, metric_collection in self.all_metrics[stage].items():\n",
    "            out[metric_type] = metric_collection(y_logit, y_true)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_logit, y_true, y_pred = self.step(batch, batch_idx)\n",
    "        loss = self.criterion(y_logit, y_true)\n",
    "        self.update_metric_step(y_logit,\n",
    "                                y_true,\n",
    "                                stage=\"train\")\n",
    "        self.log_dict({\"train_acc\": self.metrics_train[\"train/acc_top1\"],\n",
    "                       \"train_loss\": loss},\n",
    "                      on_step=True, on_epoch=True,\n",
    "                      prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_logit, y_true, y_pred = self.step(batch, batch_idx)\n",
    "        loss = self.criterion(y_logit, y_true)\n",
    "        self.update_metric_step(y_logit,\n",
    "                                y_true,\n",
    "                                stage=\"val\")\n",
    "        self.log_dict({\"val_acc\": self.metrics_val[\"val/acc_top1\"],\n",
    "                       \"val_loss\": loss},\n",
    "                      on_step=True, on_epoch=True,\n",
    "                      prog_bar=True, logger=True)\n",
    "        \n",
    "\n",
    "        return {\"loss\":loss,\n",
    "                \"y_logit\":y_logit,\n",
    "                \"y_pred\":y_pred,\n",
    "                \"y_true\":y_true}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_logit, y_true, y_pred = self.step(batch, batch_idx)\n",
    "        loss = self.criterion(y_logit, y_true)\n",
    "        self.update_metric_step(y_logit,\n",
    "                                y_true,\n",
    "                                stage=\"test\")\n",
    "        self.log_dict({\"test_acc\": self.metrics_test[\"test/acc_top1\"],\n",
    "                       \"test_loss\": loss},\n",
    "                      on_step=False, on_epoch=True,\n",
    "                      prog_bar=True, logger=True)\n",
    "        return {\"loss\":loss,\n",
    "                \"y_logit\":y_logit,\n",
    "                \"y_pred\":y_pred,\n",
    "                \"y_true\":y_true}\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx=None):\n",
    "        out = self.step(batch, batch_idx)\n",
    "        if hasattr(batch, \"metadata\"):\n",
    "            if \"path\" in batch.metadata:\n",
    "                out = [*out, batch.metadata[\"path\"]]\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def on_predict_epoch_end(self, results: List[Any]) -> None:\n",
    "        \"\"\"\n",
    "        Called at the end of predicting.\n",
    "        \"\"\"\n",
    "        \n",
    "#         y_logit, y_true, y_pred, paths = collect_results(results)\n",
    "        return collect_results(results)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# labels = model.label_encoder.classes\n",
    "# columns = [\"family_true\", \"family_pred\", \"paths\", *[f\"{l}_logit\" for l in labels]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningClassifier(BaseLightningModule):\n",
    "    def __init__(self,\n",
    "                 backbone_name='gluon_seresnext50_32x4d',\n",
    "                 pretrained: Union[bool, str]=True,\n",
    "                 num_classes: int=1000,\n",
    "                 pool_size: int=1,\n",
    "                 pool_type: str='avg',\n",
    "                 head_type: str='linear',\n",
    "                 hidden_size: Optional[int]=512,\n",
    "                 lr: float=2e-03,\n",
    "                 weight_decay: float=0.01,\n",
    "                 seed: int=None):\n",
    "        super().__init__(seed=seed)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = build_model(backbone_name=backbone_name,\n",
    "                                      pretrained=pretrained,\n",
    "                                      num_classes=num_classes,\n",
    "                                      pool_size=pool_size,\n",
    "                                      pool_type=pool_type,\n",
    "                                      head_type=head_type,\n",
    "                                      hidden_size=hidden_size)\n",
    "    \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.metrics = self.init_metrics(stage='all')\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "    def get_lr(self, group: str=None):\n",
    "        if group is None:\n",
    "            return self.hparams.lr\n",
    "        if group == \"backbone\":\n",
    "            return self.hparams.lr * 0.1\n",
    "        if group == \"head\":\n",
    "            return self.hparams.lr\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        print(f\"self.hparams={self.hparams}\")\n",
    "        self.optimizer = torch.optim.AdamW([{\"params\":self.model.backbone.parameters(), \"lr\":self.get_lr(\"backbone\"), \"weight_decay\": self.hparams.weight_decay},\n",
    "                                            {\"params\":self.model.head.parameters(), \"lr\":self.get_lr(\"head\"), \"weight_decay\": self.hparams.weight_decay}])\n",
    "#         self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.config.t_max, eta_min=self.config.min_lr)\n",
    "\n",
    "        return {'optimizer': self.optimizer}\n",
    "\n",
    "# self = model\n",
    "# layers = list(reversed(list(self.model.backbone.named_children())))\n",
    "# print([l[0] for l in layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BnFreeze(Callback):\n",
    "# source: https://github.com/fastai/fastai/blob/master/fastai/callback/training.py#L55\n",
    "#     run_after=TrainEvalCallback\n",
    "#     \"Freeze moving average statistics in all non-trainable batchnorm layers.\"\n",
    "#     def before_train(self):\n",
    "#         set_bn_eval(self.model)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiments\n",
    "\n",
    "For each architecture, 3 different experimental training methods will be evaluated\n",
    "\n",
    "Training methods:  \n",
    "1. Feature Extractor  \n",
    "2. Feature Extractor + set batchnorm to eval()  \n",
    "3. Freeze backbone except for all batchnorm layers  \n",
    "\n",
    "\n",
    "\n",
    "Baseline:  \n",
    "   * Architecture 1. simple pretrained backbone -> `avg_pool` -> linear_classifier  \n",
    "    \n",
    "Next Comparisons:  \n",
    "* Architecture 2. simple pretrained backbone -> `max_pool` -> linear_classifier\n",
    "* Architecture 3. simple pretrained backbone -> `avgmax_pool` -> linear_classifier  \n",
    "\n",
    "Later:  \n",
    "* Architecture 4. simple pretrained backbone -> `avgdrop_pool` -> linear_classifier  \n",
    "* Architecture 5. simple pretrained backbone -> `maxdrop_pool` -> linear_classifier  \n",
    "* Architecture 6. simple pretrained backbone -> `avgmaxdrop_pool` -> linear_classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = build_model(backbone_name='gluon_seresnext50_32x4d',\n",
    "#                     pretrained=True,\n",
    "#                     num_classes=19,\n",
    "#                     pool_size=1,\n",
    "#                     pool_type='avgmax',\n",
    "#                     head_type='linear',\n",
    "#                     hidden_size=None)\n",
    "# from torchinfo import summary\n",
    "\n",
    "\n",
    "#left off here 11 pm\n",
    "\n",
    "# model = LightningClassifier(backbone_name='gluon_seresnext50_32x4d',\n",
    "#                             pretrained=True,\n",
    "#                             num_classes=19,\n",
    "#                             pool_size=1,\n",
    "#                             pool_type='avgmax',\n",
    "#                             head_type='linear',\n",
    "#                             hidden_size=None,\n",
    "#                             lr=2e-03,\n",
    "#                             weight_decay=0.01,\n",
    "#                             seed=98)\n",
    "\n",
    "# pp(list(model.get_batchnorm_modules()))\n",
    "# # pp(list(model.get_conv_modules()))\n",
    "# pp(list(model.get_linear_modules()))\n",
    "# # pp(list(model.get_conv_modules()))\n",
    "# pp(list(model.get_named_modules()))\n",
    "\n",
    "# model.freeze_backbone(freeze_bn=False)\n",
    "# model.freeze_backbone(freeze_bn=True)\n",
    "# summary(model.model)\n",
    "\n",
    "# print(f\"trainable: {len(list(model.get_trainable_parameters()))}\")\n",
    "# print(f\"non-trainable: {len(list(model.get_nontrainable_parameters()))}\")\n",
    "\n",
    "# bn = {n:p.requires_grad for n, p in model.get_named_parameters(\"bn\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini test: Wrap all model hooks & display + verify for each parameter group the proper status of module.training & weight_tensors.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable batchnorm modules:49\n",
      "nontrainable batchnorm modules:0\n",
      "batchnorm params with requires_grad=True: :0\n",
      "batchnorm params with requires_grad=False:98\n",
      "Total Trainable Params: 77,843\n",
      "Total non-Trainable Params: 25,510,896\n"
     ]
    }
   ],
   "source": [
    "model.freeze_backbone(freeze_bn=True) #False)\n",
    "# model.set_bn_eval()\n",
    "# summary(model.model)\n",
    "model.count_trainable_batchnorm_layers()\n",
    "count_parameters(model.model, verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature extractor + BN set to Eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable batchnorm modules:0\n",
      "nontrainable batchnorm modules:49\n",
      "batchnorm params with requires_grad=True: :0\n",
      "batchnorm params with requires_grad=False:98\n",
      "Total Trainable Params: 77,843\n",
      "Total non-Trainable Params: 25,510,896\n"
     ]
    }
   ],
   "source": [
    "model.freeze_backbone(freeze_bn=True)\n",
    "model.set_bn_eval(model)\n",
    "# summary(model.model)\n",
    "model.count_trainable_batchnorm_layers()\n",
    "count_parameters(model.model, verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Freeze backbone except for BN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable batchnorm modules:49\n",
      "nontrainable batchnorm modules:0\n",
      "batchnorm params with requires_grad=True: :98\n",
      "batchnorm params with requires_grad=False:0\n",
      "Total Trainable Params: 138,387\n",
      "Total non-Trainable Params: 25,450,352\n"
     ]
    }
   ],
   "source": [
    "# model.unfreeze(model.model)\n",
    "model.freeze_backbone(freeze_bn=False)\n",
    "\n",
    "model.unfreeze(model.model,\n",
    "               filter_pattern=\"bn\")\n",
    "# summary(model.model)\n",
    "model.count_trainable_batchnorm_layers()\n",
    "count_parameters(model.model, verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data\n",
    "\n",
    "#### Working with task_1 -> PNAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.scripts.multitask.train import MultiTaskDataModule, LitMultiTaskModule, ImagePredictionLogger, train_task,  CIFAR10DataModule, run_multitask_test, load_data_and_model, load_data, resolve_config, configure_callbacks, configure_loggers, configure_trainer\n",
    "from lightning_hydra_classifiers.data.datasets.common import toPIL\n",
    "from lightning_hydra_classifiers.utils.etl_utils import ETL\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "# overrides = ['model/backbone=efficientnet_b3',\"data=extant_to_fossil\", \"trainer.max_epochs=2\", \"data.batch_size=16\", \"trainer.precision=16\"]\n",
    "overrides = ['model/backbone=resnet50',\"data=extant_to_pnas\", \"trainer.max_epochs=20\", \"data.batch_size=32\", \"trainer.precision=16\", \"trainer.gpus=[7]\"]\n",
    "config = ETL.load_hydra_config(config_name = \"config\",\n",
    "                              config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\",\n",
    "                              overrides=overrides)\n",
    "\n",
    "task_id = 1\n",
    "pp(config.data)\n",
    "datamodule = load_data(config,\n",
    "                       task_id=task_id)\n",
    "\n",
    "## Config model, trainer, et. al.\n",
    "\n",
    "# model_config = OmegaConf.create(dict(\n",
    "#                                 backbone_name=config.model.backbone.backbone_name, #'gluon_seresnext50_32x4d',\n",
    "#                                 pretrained=True,\n",
    "#                                 num_classes=datamodule.num_classes,\n",
    "#                                 pool_type='avg',\n",
    "#                                 head_type='linear',\n",
    "#                                 hidden_size=None,\n",
    "#                                 lr=2e-03,\n",
    "#                                 weight_decay=0.01,\n",
    "#                                 seed=98))\n",
    "\n",
    "# model_config = OmegaConf.create(dict(\n",
    "#                                 backbone_name=config.model.backbone.backbone_name, #'gluon_seresnext50_32x4d',\n",
    "#                                 pretrained=True,\n",
    "#                                 num_classes=datamodule.num_classes,\n",
    "#                                 pool_type='max',\n",
    "#                                 head_type='linear',\n",
    "#                                 hidden_size=None,\n",
    "#                                 lr=2e-03,\n",
    "#                                 weight_decay=0.01,\n",
    "#                                 seed=98))\n",
    "\n",
    "model_config = OmegaConf.create(dict(\n",
    "                                backbone_name=config.model.backbone.backbone_name, #'gluon_seresnext50_32x4d',\n",
    "                                pretrained=True,\n",
    "                                num_classes=datamodule.num_classes,\n",
    "                                pool_type='avgdrop',\n",
    "                                head_type='linear',\n",
    "                                hidden_size=None,\n",
    "                                lr=2e-03,\n",
    "                                weight_decay=0.01,\n",
    "                                seed=98))\n",
    "\n",
    "\n",
    "\n",
    "config.model = model_config\n",
    "\n",
    "algorithm_name = \"feature_extractor\"\n",
    "config.experiment_name = f\"{algorithm_name}-PNAS-{datamodule.num_classes}_classes-res_{config.data.image_size}-bsz_{config.data.batch_size}-{config.model.backbone_name}-pretrained_{config.model.pretrained}-pool_{config.model.pool_type}\"\n",
    "\n",
    "\n",
    "experiment_dir = config.experiment_dir\n",
    "results_dir = config.results_dir\n",
    "results_dir\n",
    "\n",
    "model = LightningClassifier(**config.model)\n",
    "model.label_encoder = datamodule.label_encoder\n",
    "\n",
    "\n",
    "group = f'{config.model.backbone_name}__PNAS__experiment_0__feature_extractor'\n",
    "config.logger.wandb.group = group\n",
    "config.callbacks.log_per_class_metrics_to_wandb.class_names = datamodule.classes\n",
    "\n",
    "\n",
    "callbacks = configure_callbacks(config)\n",
    "logger = configure_loggers(config)\n",
    "\n",
    "trainer: pl.Trainer = configure_trainer(config, callbacks=callbacks, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 21:51:02,963 lightning_hydra_classifiers.scripts.pretrain.lr_tuner INFO     Proceeding with overrides merged with default parameters\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrose\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">balmy-sponge-13</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/default\" target=\"_blank\">https://wandb.ai/jrose/default</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/default/runs/1rbvztpc\" target=\"_blank\">https://wandb.ai/jrose/default/runs/1rbvztpc</a><br/>\n",
       "                Run data is saved locally in <code>/media/data/jacob/GitHub/lightning-hydra-classifiers/notebooks/wandb/run-20211013_215103-1rbvztpc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 21:51:04,980 lightning_hydra_classifiers.scripts.pretrain.lr_tuner INFO     [Initiating Stage] lr_tuner\n",
      "2021-10-13 21:51:05,437 lightning_hydra_classifiers.experiments.multitask.datamodules INFO     Task_1 (None): datamodule.setup(stage=fit)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name                    | Type             | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model                   | Sequential       | 23.5 M\n",
      "1 | criterion               | CrossEntropyLoss | 0     \n",
      "2 | metrics_train           | MetricCollection | 0     \n",
      "3 | metrics_train_per_class | MetricCollection | 0     \n",
      "4 | metrics_val             | MetricCollection | 0     \n",
      "5 | metrics_val_per_class   | MetricCollection | 0     \n",
      "6 | metrics_test            | MetricCollection | 0     \n",
      "7 | metrics_test_per_class  | MetricCollection | 0     \n",
      "-------------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.188    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.hparams=\"backbone_name\": resnet50\n",
      "\"head_type\":     linear\n",
      "\"hidden_size\":   None\n",
      "\"lr\":            0.002\n",
      "\"num_classes\":   19\n",
      "\"pool_size\":     1\n",
      "\"pool_type\":     avgdrop\n",
      "\"pretrained\":    True\n",
      "\"seed\":          98\n",
      "\"weight_decay\":  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  11.489         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "evaluation_step_and_end            \t|  0.39565        \t|2              \t|  0.7913         \t|  6.8873         \t|\n",
      "validation_step                    \t|  0.3955         \t|2              \t|  0.791          \t|  6.8847         \t|\n",
      "evaluation_batch_to_device         \t|  0.029918       \t|2              \t|  0.059837       \t|  0.5208         \t|\n",
      "on_validation_batch_start          \t|  0.00012405     \t|2              \t|  0.00024811     \t|  0.0021595      \t|\n",
      "validation_step_end                \t|  3.6476e-05     \t|2              \t|  7.2951e-05     \t|  0.00063495     \t|\n",
      "on_validation_batch_end            \t|  3.3155e-05     \t|2              \t|  6.631e-05      \t|  0.00057715     \t|\n",
      "on_validation_start                \t|  5.0948e-05     \t|1              \t|  5.0948e-05     \t|  0.00044344     \t|\n",
      "on_before_accelerator_backend_setup\t|  4.7959e-05     \t|1              \t|  4.7959e-05     \t|  0.00041742     \t|\n",
      "on_validation_end                  \t|  4.6345e-05     \t|1              \t|  4.6345e-05     \t|  0.00040338     \t|\n",
      "on_val_dataloader                  \t|  4.6107e-05     \t|1              \t|  4.6107e-05     \t|  0.00040131     \t|\n",
      "on_validation_epoch_end            \t|  4.5127e-05     \t|1              \t|  4.5127e-05     \t|  0.00039278     \t|\n",
      "on_epoch_end                       \t|  2.4894e-05     \t|1              \t|  2.4894e-05     \t|  0.00021667     \t|\n",
      "on_epoch_start                     \t|  2.328e-05      \t|1              \t|  2.328e-05      \t|  0.00020262     \t|\n",
      "on_validation_epoch_start          \t|  2.0311e-05     \t|1              \t|  2.0311e-05     \t|  0.00017678     \t|\n",
      "\n",
      "Global seed set to 98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e477bd061a494900a9b16412ca2887e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  77.384         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_batch                 \t|  0.90106        \t|62             \t|  55.866         \t|  72.193         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.88873        \t|62             \t|  55.101         \t|  71.205         \t|\n",
      "training_step_and_backward         \t|  0.36505        \t|62             \t|  22.633         \t|  29.248         \t|\n",
      "model_forward                      \t|  0.33044        \t|62             \t|  20.487         \t|  26.475         \t|\n",
      "training_step                      \t|  0.33003        \t|62             \t|  20.462         \t|  26.442         \t|\n",
      "evaluation_step_and_end            \t|  0.32372        \t|18             \t|  5.827          \t|  7.53           \t|\n",
      "validation_step                    \t|  0.32359        \t|18             \t|  5.8246         \t|  7.5269         \t|\n",
      "get_train_batch                    \t|  0.041302       \t|62             \t|  2.5607         \t|  3.3091         \t|\n",
      "backward                           \t|  0.026618       \t|62             \t|  1.6503         \t|  2.1326         \t|\n",
      "on_train_batch_end                 \t|  0.0027655      \t|62             \t|  0.17146        \t|  0.22157        \t|\n",
      "on_batch_start                     \t|  0.0012028      \t|62             \t|  0.074572       \t|  0.096366       \t|\n",
      "evaluation_batch_to_device         \t|  0.0040127      \t|18             \t|  0.072229       \t|  0.093339       \t|\n",
      "training_batch_to_device           \t|  0.0010648      \t|62             \t|  0.066015       \t|  0.085309       \t|\n",
      "on_train_start                     \t|  0.046905       \t|1              \t|  0.046905       \t|  0.060613       \t|\n",
      "on_before_optimizer_step           \t|  3.4547e-05     \t|62             \t|  0.0021419      \t|  0.0027679      \t|\n",
      "on_before_zero_grad                \t|  3.4422e-05     \t|62             \t|  0.0021342      \t|  0.0027579      \t|\n",
      "training_step_end                  \t|  3.4373e-05     \t|62             \t|  0.0021311      \t|  0.002754       \t|\n",
      "on_after_backward                  \t|  3.1618e-05     \t|62             \t|  0.0019603      \t|  0.0025332      \t|\n",
      "on_batch_end                       \t|  2.9106e-05     \t|62             \t|  0.0018046      \t|  0.002332       \t|\n",
      "on_before_backward                 \t|  2.7884e-05     \t|62             \t|  0.0017288      \t|  0.002234       \t|\n",
      "on_train_batch_start               \t|  2.7017e-05     \t|62             \t|  0.001675       \t|  0.0021646      \t|\n",
      "on_validation_batch_start          \t|  5.274e-05      \t|18             \t|  0.00094932     \t|  0.0012268      \t|\n",
      "validation_step_end                \t|  3.109e-05      \t|18             \t|  0.00055962     \t|  0.00072317     \t|\n",
      "on_validation_batch_end            \t|  2.4094e-05     \t|18             \t|  0.0004337      \t|  0.00056045     \t|\n",
      "on_validation_epoch_end            \t|  5.2619e-05     \t|2              \t|  0.00010524     \t|  0.00013599     \t|\n",
      "on_validation_end                  \t|  5.042e-05      \t|2              \t|  0.00010084     \t|  0.00013031     \t|\n",
      "on_validation_start                \t|  4.9781e-05     \t|2              \t|  9.9562e-05     \t|  0.00012866     \t|\n",
      "on_epoch_start                     \t|  3.2993e-05     \t|3              \t|  9.8979e-05     \t|  0.00012791     \t|\n",
      "on_epoch_end                       \t|  2.4387e-05     \t|2              \t|  4.8774e-05     \t|  6.3028e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  4.7959e-05     \t|1              \t|  4.7959e-05     \t|  6.1975e-05     \t|\n",
      "on_val_dataloader                  \t|  4.6107e-05     \t|1              \t|  4.6107e-05     \t|  5.9582e-05     \t|\n",
      "on_validation_epoch_start          \t|  1.9616e-05     \t|2              \t|  3.9233e-05     \t|  5.0699e-05     \t|\n",
      "on_train_epoch_start               \t|  3.5191e-05     \t|1              \t|  3.5191e-05     \t|  4.5476e-05     \t|\n",
      "on_train_dataloader                \t|  2.6901e-05     \t|1              \t|  2.6901e-05     \t|  3.4763e-05     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_acc': 0.14941301941871643,\n",
      " 'val_acc_epoch': 0.14941301941871643,\n",
      " 'val_loss': 2.4381706714630127,\n",
      " 'val_loss_epoch': 2.4381706714630127}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  114.68         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  51.556         \t|2              \t|  103.11         \t|  89.911         \t|\n",
      "run_training_batch                 \t|  0.90444        \t|100            \t|  90.444         \t|  78.865         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.89241        \t|100            \t|  89.241         \t|  77.816         \t|\n",
      "training_step_and_backward         \t|  0.36694        \t|100            \t|  36.694         \t|  31.996         \t|\n",
      "model_forward                      \t|  0.33258        \t|100            \t|  33.258         \t|  29.0           \t|\n",
      "training_step                      \t|  0.33216        \t|100            \t|  33.216         \t|  28.963         \t|\n",
      "evaluation_step_and_end            \t|  0.32372        \t|18             \t|  5.827          \t|  5.081          \t|\n",
      "validation_step                    \t|  0.32359        \t|18             \t|  5.8246         \t|  5.079          \t|\n",
      "get_train_batch                    \t|  0.050074       \t|100            \t|  5.0074         \t|  4.3664         \t|\n",
      "backward                           \t|  0.026231       \t|100            \t|  2.6231         \t|  2.2873         \t|\n",
      "on_train_batch_end                 \t|  0.003058       \t|100            \t|  0.3058         \t|  0.26665        \t|\n",
      "training_batch_to_device           \t|  0.0010768      \t|100            \t|  0.10768        \t|  0.093892       \t|\n",
      "on_batch_start                     \t|  0.00076364     \t|100            \t|  0.076364       \t|  0.066588       \t|\n",
      "evaluation_batch_to_device         \t|  0.0040127      \t|18             \t|  0.072229       \t|  0.062982       \t|\n",
      "on_train_start                     \t|  0.046905       \t|1              \t|  0.046905       \t|  0.0409         \t|\n",
      "training_step_end                  \t|  3.6347e-05     \t|100            \t|  0.0036347      \t|  0.0031693      \t|\n",
      "on_before_zero_grad                \t|  3.5796e-05     \t|100            \t|  0.0035796      \t|  0.0031214      \t|\n",
      "on_before_optimizer_step           \t|  3.4551e-05     \t|100            \t|  0.0034551      \t|  0.0030128      \t|\n",
      "on_after_backward                  \t|  3.2609e-05     \t|100            \t|  0.0032609      \t|  0.0028434      \t|\n",
      "on_batch_end                       \t|  3.0031e-05     \t|100            \t|  0.0030031      \t|  0.0026186      \t|\n",
      "on_train_batch_start               \t|  2.8311e-05     \t|100            \t|  0.0028311      \t|  0.0024686      \t|\n",
      "on_before_backward                 \t|  2.7431e-05     \t|100            \t|  0.0027431      \t|  0.0023919      \t|\n",
      "on_validation_batch_start          \t|  5.274e-05      \t|18             \t|  0.00094932     \t|  0.00082779     \t|\n",
      "validation_step_end                \t|  3.109e-05      \t|18             \t|  0.00055962     \t|  0.00048798     \t|\n",
      "on_validation_batch_end            \t|  2.4094e-05     \t|18             \t|  0.0004337      \t|  0.00037817     \t|\n",
      "on_train_epoch_end                 \t|  0.00015279     \t|2              \t|  0.00030557     \t|  0.00026645     \t|\n",
      "on_epoch_start                     \t|  3.0793e-05     \t|4              \t|  0.00012317     \t|  0.0001074      \t|\n",
      "on_epoch_end                       \t|  2.8276e-05     \t|4              \t|  0.00011311     \t|  9.8625e-05     \t|\n",
      "on_validation_epoch_end            \t|  5.2619e-05     \t|2              \t|  0.00010524     \t|  9.1764e-05     \t|\n",
      "on_validation_end                  \t|  5.042e-05      \t|2              \t|  0.00010084     \t|  8.7931e-05     \t|\n",
      "on_validation_start                \t|  4.9781e-05     \t|2              \t|  9.9562e-05     \t|  8.6816e-05     \t|\n",
      "on_train_epoch_start               \t|  2.6529e-05     \t|2              \t|  5.3057e-05     \t|  4.6264e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  4.7959e-05     \t|1              \t|  4.7959e-05     \t|  4.1819e-05     \t|\n",
      "on_val_dataloader                  \t|  4.6107e-05     \t|1              \t|  4.6107e-05     \t|  4.0204e-05     \t|\n",
      "on_train_end                       \t|  3.9474e-05     \t|1              \t|  3.9474e-05     \t|  3.442e-05      \t|\n",
      "on_validation_epoch_start          \t|  1.9616e-05     \t|2              \t|  3.9233e-05     \t|  3.421e-05      \t|\n",
      "on_train_dataloader                \t|  2.6901e-05     \t|1              \t|  2.6901e-05     \t|  2.3457e-05     \t|\n",
      "\n",
      "Restoring states from the checkpoint file at /media/data/jacob/GitHub/lightning-hydra-classifiers/notebooks/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /media/data/jacob/GitHub/lightning-hydra-classifiers/notebooks/lr_find_temp_model.ckpt\n",
      "2021-10-13 21:52:58,676 lightning_hydra_classifiers.scripts.pretrain.lr_tuner INFO     Saved best lr value (along w/ batch_size, image_size) to file located at: /media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/feature_extractor-PNAS-19_classes-res_512-bsz_32-resnet50-pretrained_True-pool_avgdrop/replicate_1/results/task_1/lr_tuner/hparams.yaml\n",
      "2021-10-13 21:52:58,681 lightning_hydra_classifiers.scripts.pretrain.lr_tuner INFO     File contents expected to contain: \n",
      "{'optimized_hparam_key': 'lr', 'lr': 0.0009120108393559097, 'batch_size': 32, 'image_size': 512, 'lr_tuner_config': {'min_lr': 1e-08, 'max_lr': 1, 'num_training': 100, 'mode': 'exponential', 'early_stop_threshold': 4.0}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14400<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.25MB of 0.25MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/media/data/jacob/GitHub/lightning-hydra-classifiers/notebooks/wandb/run-20211013_215103-1rbvztpc/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/media/data/jacob/GitHub/lightning-hydra-classifiers/notebooks/wandb/run-20211013_215103-1rbvztpc/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>116</td></tr><tr><td>_timestamp</td><td>1634176379</td></tr><tr><td>_step</td><td>6</td></tr><tr><td>lr_finder/best/loss</td><td>2.73622</td></tr><tr><td>lr_finder/best/lr</td><td>0.00091</td></tr><tr><td>lr_finder/batch_size</td><td>32</td></tr><tr><td>image_size</td><td>512</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▂▃▅▆▇█</td></tr><tr><td>lr_finder/best/loss</td><td>▁</td></tr><tr><td>lr_finder/best/lr</td><td>▁</td></tr><tr><td>lr_finder/batch_size</td><td>▁</td></tr><tr><td>image_size</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">balmy-sponge-13</strong>: <a href=\"https://wandb.ai/jrose/default/runs/1rbvztpc\" target=\"_blank\">https://wandb.ai/jrose/default/runs/1rbvztpc</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 21:53:04,690 lightning_hydra_classifiers.scripts.pretrain.lr_tuner INFO     FINISHED: `run_lr_tuner(config)`\n",
      "2021-10-13 21:53:04,690 lightning_hydra_classifiers.scripts.pretrain.lr_tuner INFO     Proceeding with:\n",
      "\n",
      "2021-10-13 21:53:04,691 lightning_hydra_classifiers.scripts.pretrain.lr_tuner INFO     Learning rate = 9.120e-04\n",
      "2021-10-13 21:53:04,692 lightning_hydra_classifiers.scripts.pretrain.lr_tuner INFO     Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'backbone_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pretrained'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pool_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'avgdrop'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'head_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'linear'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hidden_size'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0009120108393559097</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'weight_decay'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'backbone_name'\u001b[0m: \u001b[32m'resnet50'\u001b[0m,\n",
       "    \u001b[32m'pretrained'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m: \u001b[1;36m19\u001b[0m,\n",
       "    \u001b[32m'pool_type'\u001b[0m: \u001b[32m'avgdrop'\u001b[0m,\n",
       "    \u001b[32m'head_type'\u001b[0m: \u001b[32m'linear'\u001b[0m,\n",
       "    \u001b[32m'hidden_size'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'lr'\u001b[0m: \u001b[1;36m0.0009120108393559097\u001b[0m,\n",
       "    \u001b[32m'weight_decay'\u001b[0m: \u001b[1;36m0.01\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m: \u001b[1;36m98\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"backbone_name\": resnet50\n",
      "\"head_type\":     linear\n",
      "\"hidden_size\":   None\n",
      "\"lr\":            0.0009120108393559097\n",
      "\"num_classes\":   19\n",
      "\"pool_size\":     1\n",
      "\"pool_type\":     avgdrop\n",
      "\"pretrained\":    True\n",
      "\"seed\":          98\n",
      "\"weight_decay\":  0.01\n",
      "\"backbone_name\": resnet50\n",
      "\"head_type\":     linear\n",
      "\"hidden_size\":   None\n",
      "\"lr\":            0.002\n",
      "\"num_classes\":   19\n",
      "\"pool_size\":     1\n",
      "\"pool_type\":     avgdrop\n",
      "\"pretrained\":    True\n",
      "\"seed\":          98\n",
      "\"weight_decay\":  0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightning_hydra_classifiers.scripts.pretrain import lr_tuner\n",
    "\n",
    "\n",
    "lr_tuner_results_dir = os.path.join(results_dir, f\"task_{task_id}\", \"lr_tuner\")\n",
    "\n",
    "# logger.info(f\"[Initiating Stage] lr_tuner\")\n",
    "lr_tune_output = lr_tuner.run_lr_tuner(trainer=trainer,\n",
    "                                       model=model,\n",
    "                                       datamodule=datamodule,\n",
    "                                       config=config,\n",
    "                                       results_dir=lr_tuner_results_dir,\n",
    "                                       group=group)\n",
    "\n",
    "\n",
    "# if len(lr_tune_output)==3:\n",
    "#     suggestion, lr_tuner_results, config = lr_tune_output\n",
    "# else:\n",
    "#     suggestion, lr_tuner_results, config, lr_tuner = lr_tune_output\n",
    "\n",
    "pp(OmegaConf.to_container(config.model, resolve=True))\n",
    "print(model.hparams)\n",
    "\n",
    "print(model.hparams_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "2021-10-13 21:53:05,226 lightning_hydra_classifiers.experiments.multitask.datamodules INFO     Task_1 (None): datamodule.setup(stage=fit)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.hparams=\"backbone_name\": resnet50\n",
      "\"head_type\":     linear\n",
      "\"hidden_size\":   None\n",
      "\"lr\":            0.0009120108393559097\n",
      "\"num_classes\":   19\n",
      "\"pool_size\":     1\n",
      "\"pool_type\":     avgdrop\n",
      "\"pretrained\":    True\n",
      "\"seed\":          98\n",
      "\"weight_decay\":  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">crimson-elevator-255</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jrose/image_classification\" target=\"_blank\">https://wandb.ai/jrose/image_classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jrose/image_classification/runs/2u8frfxq\" target=\"_blank\">https://wandb.ai/jrose/image_classification/runs/2u8frfxq</a><br/>\n",
       "                Run data is saved locally in <code>/media/data/jacob/GitHub/lightning-hydra-classifiers/notebooks/wandb/run-20211013_215305-2u8frfxq</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                    | Type             | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model                   | Sequential       | 23.5 M\n",
      "1 | criterion               | CrossEntropyLoss | 0     \n",
      "2 | metrics_train           | MetricCollection | 0     \n",
      "3 | metrics_train_per_class | MetricCollection | 0     \n",
      "4 | metrics_val             | MetricCollection | 0     \n",
      "5 | metrics_val_per_class   | MetricCollection | 0     \n",
      "6 | metrics_test            | MetricCollection | 0     \n",
      "7 | metrics_test_per_class  | MetricCollection | 0     \n",
      "-------------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.188    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fffd635c9a4c3bab2a8373c944f06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 98\n",
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pl_bolts/callbacks/data_monitor.py:104: UserWarning: ModuleDataMonitor does not support logging with LoggerCollection. Supported loggers are: TensorBoardLogger, WandbLogger\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af60f701533b475e8ae4e4efe39d18ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 37it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved. New best score: 0.366\n",
      "Epoch 0, global step 62: val_acc reached 0.36573 (best 0.36573), saving model to \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/feature_extractor-PNAS-19_classes-res_512-bsz_32-resnet50-pretrained_True-pool_avgdrop/replicate_1/results/checkpoints/epoch=00-val_loss=1.230-val_acc=0.366.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.125 >= min_delta = 0.05. New best score: 0.490\n",
      "Epoch 1, global step 124: val_acc reached 0.49036 (best 0.49036), saving model to \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/feature_extractor-PNAS-19_classes-res_512-bsz_32-resnet50-pretrained_True-pool_avgdrop/replicate_1/results/checkpoints/epoch=01-val_loss=0.897-val_acc=0.490.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.077 >= min_delta = 0.05. New best score: 0.567\n",
      "Epoch 2, global step 186: val_acc reached 0.56739 (best 0.56739), saving model to \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/feature_extractor-PNAS-19_classes-res_512-bsz_32-resnet50-pretrained_True-pool_avgdrop/replicate_1/results/checkpoints/epoch=02-val_loss=0.788-val_acc=0.567.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 248: val_acc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.061 >= min_delta = 0.05. New best score: 0.629\n",
      "Epoch 4, global step 310: val_acc reached 0.62877 (best 0.62877), saving model to \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/feature_extractor-PNAS-19_classes-res_512-bsz_32-resnet50-pretrained_True-pool_avgdrop/replicate_1/results/checkpoints/epoch=04-val_loss=0.877-val_acc=0.629.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 372: val_acc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 434: val_acc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 496: val_acc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 558: val_acc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) (19,) (19,) (19,)\n",
      "val/per_class/F1_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Precision_distributions: metric_data[0].shape=(1,)\n",
      "val/per_class/Recall_distributions: metric_data[0].shape=(1,)\n",
      "len(self.class_names)=19\n",
      "val/confusion_matrix/crimson-elevator-255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_acc did not improve in the last 5 records. Best score: 0.629. Signaling Trainer to stop.\n",
      "Epoch 9, global step 620: val_acc reached 0.65465 (best 0.65465), saving model to \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/feature_extractor-PNAS-19_classes-res_512-bsz_32-resnet50-pretrained_True-pool_avgdrop/replicate_1/results/checkpoints/epoch=09-val_loss=0.812-val_acc=0.655.ckpt\" as top 1\n",
      "Saving latest checkpoint...\n",
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:32: LightningDeprecationWarning: `Trainer.train_loop` has been renamed to `Trainer.fit_loop` and will be removed in v1.6.\n",
      "  rank_zero_deprecation(\n",
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  1373.4         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  112.02         \t|12             \t|  1344.2         \t|  97.873         \t|\n",
      "run_training_batch                 \t|  0.91536        \t|720            \t|  659.06         \t|  47.986         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.90443        \t|720            \t|  651.19         \t|  47.413         \t|\n",
      "training_step_and_backward         \t|  0.3779         \t|720            \t|  272.09         \t|  19.81          \t|\n",
      "on_validation_batch_end            \t|  1.3841         \t|180            \t|  249.15         \t|  18.14          \t|\n",
      "on_validation_epoch_end            \t|  18.587         \t|13             \t|  241.63         \t|  17.593         \t|\n",
      "model_forward                      \t|  0.33244        \t|720            \t|  239.35         \t|  17.427         \t|\n",
      "training_step                      \t|  0.33197        \t|720            \t|  239.02         \t|  17.403         \t|\n",
      "on_train_epoch_end                 \t|  5.8711         \t|12             \t|  70.454         \t|  5.1297         \t|\n",
      "evaluation_step_and_end            \t|  0.31768        \t|180            \t|  57.182         \t|  4.1634         \t|\n",
      "validation_step                    \t|  0.31754        \t|180            \t|  57.157         \t|  4.1616         \t|\n",
      "get_train_batch                    \t|  0.043775       \t|720            \t|  31.518         \t|  2.2948         \t|\n",
      "backward                           \t|  0.036041       \t|720            \t|  25.95          \t|  1.8894         \t|\n",
      "on_train_end                       \t|  2.322          \t|2              \t|  4.6441         \t|  0.33813        \t|\n",
      "on_train_batch_end                 \t|  0.0011183      \t|720            \t|  0.80515        \t|  0.058623       \t|\n",
      "training_batch_to_device           \t|  0.0010714      \t|720            \t|  0.7714         \t|  0.056165       \t|\n",
      "on_validation_start                \t|  0.035102       \t|13             \t|  0.45632        \t|  0.033225       \t|\n",
      "evaluation_batch_to_device         \t|  0.001923       \t|180            \t|  0.34614        \t|  0.025202       \t|\n",
      "on_train_start                     \t|  0.070372       \t|2              \t|  0.14074        \t|  0.010248       \t|\n",
      "on_batch_start                     \t|  0.00015116     \t|720            \t|  0.10884        \t|  0.0079242      \t|\n",
      "on_train_epoch_start               \t|  0.0045843      \t|12             \t|  0.055011       \t|  0.0040053      \t|\n",
      "on_validation_end                  \t|  0.0038035      \t|13             \t|  0.049446       \t|  0.0036001      \t|\n",
      "on_before_optimizer_step           \t|  5.2382e-05     \t|720            \t|  0.037715       \t|  0.002746       \t|\n",
      "on_before_zero_grad                \t|  5.018e-05      \t|720            \t|  0.03613        \t|  0.0026306      \t|\n",
      "on_train_batch_start               \t|  4.654e-05      \t|720            \t|  0.033509       \t|  0.0024398      \t|\n",
      "on_after_backward                  \t|  4.6024e-05     \t|720            \t|  0.033137       \t|  0.0024127      \t|\n",
      "on_before_backward                 \t|  4.2185e-05     \t|720            \t|  0.030373       \t|  0.0022114      \t|\n",
      "on_batch_end                       \t|  4.071e-05      \t|720            \t|  0.029312       \t|  0.0021342      \t|\n",
      "training_step_end                  \t|  3.3604e-05     \t|720            \t|  0.024195       \t|  0.0017616      \t|\n",
      "on_validation_batch_start          \t|  9.4559e-05     \t|180            \t|  0.017021       \t|  0.0012393      \t|\n",
      "validation_step_end                \t|  2.9312e-05     \t|180            \t|  0.0052762      \t|  0.00038415     \t|\n",
      "on_epoch_end                       \t|  8.3809e-05     \t|25             \t|  0.0020952      \t|  0.00015255     \t|\n",
      "on_epoch_start                     \t|  5.7419e-05     \t|25             \t|  0.0014355      \t|  0.00010452     \t|\n",
      "on_validation_epoch_start          \t|  3.783e-05      \t|13             \t|  0.00049179     \t|  3.5807e-05     \t|\n",
      "on_fit_start                       \t|  0.00019467     \t|1              \t|  0.00019467     \t|  1.4174e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  4.4239e-05     \t|2              \t|  8.8478e-05     \t|  6.442e-06      \t|\n",
      "on_val_dataloader                  \t|  4.6107e-05     \t|1              \t|  4.6107e-05     \t|  3.357e-06      \t|\n",
      "on_train_dataloader                \t|  2.6901e-05     \t|1              \t|  2.6901e-05     \t|  1.9586e-06     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = trainer.fit(model, datamodule=datamodule)\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "2021-10-13 22:14:01,190 lightning_hydra_classifiers.experiments.multitask.datamodules INFO     Task_1: datamodule.setup(stage=test)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b4bb3fec6848bc9fd5d05ee7f5ad7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TEST Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  1410.2         \t|  100 %          \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  112.02         \t|12             \t|  1344.2         \t|  95.32          \t|\n",
      "run_training_batch                 \t|  0.91536        \t|720            \t|  659.06         \t|  46.734         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.90443        \t|720            \t|  651.19         \t|  46.176         \t|\n",
      "training_step_and_backward         \t|  0.3779         \t|720            \t|  272.09         \t|  19.294         \t|\n",
      "on_validation_batch_end            \t|  1.3841         \t|180            \t|  249.15         \t|  17.667         \t|\n",
      "on_validation_epoch_end            \t|  18.587         \t|13             \t|  241.63         \t|  17.134         \t|\n",
      "model_forward                      \t|  0.33244        \t|720            \t|  239.35         \t|  16.973         \t|\n",
      "training_step                      \t|  0.33197        \t|720            \t|  239.02         \t|  16.949         \t|\n",
      "evaluation_step_and_end            \t|  0.31776        \t|268            \t|  85.161         \t|  6.0388         \t|\n",
      "on_train_epoch_end                 \t|  5.8711         \t|12             \t|  70.454         \t|  4.9959         \t|\n",
      "validation_step                    \t|  0.31754        \t|180            \t|  57.157         \t|  4.053          \t|\n",
      "get_train_batch                    \t|  0.043775       \t|720            \t|  31.518         \t|  2.2349         \t|\n",
      "test_step                          \t|  0.31781        \t|88             \t|  27.967         \t|  1.9832         \t|\n",
      "backward                           \t|  0.036041       \t|720            \t|  25.95          \t|  1.8401         \t|\n",
      "on_train_end                       \t|  2.322          \t|2              \t|  4.6441         \t|  0.32931        \t|\n",
      "on_train_batch_end                 \t|  0.0011183      \t|720            \t|  0.80515        \t|  0.057093       \t|\n",
      "training_batch_to_device           \t|  0.0010714      \t|720            \t|  0.7714         \t|  0.0547         \t|\n",
      "on_validation_start                \t|  0.035102       \t|13             \t|  0.45632        \t|  0.032358       \t|\n",
      "evaluation_batch_to_device         \t|  0.0016541      \t|268            \t|  0.4433         \t|  0.031434       \t|\n",
      "on_train_start                     \t|  0.070372       \t|2              \t|  0.14074        \t|  0.0099802      \t|\n",
      "on_batch_start                     \t|  0.00015116     \t|720            \t|  0.10884        \t|  0.0077175      \t|\n",
      "on_train_epoch_start               \t|  0.0045843      \t|12             \t|  0.055011       \t|  0.0039008      \t|\n",
      "on_validation_end                  \t|  0.0038035      \t|13             \t|  0.049446       \t|  0.0035062      \t|\n",
      "on_test_start                      \t|  0.045446       \t|1              \t|  0.045446       \t|  0.0032225      \t|\n",
      "on_before_optimizer_step           \t|  5.2382e-05     \t|720            \t|  0.037715       \t|  0.0026744      \t|\n",
      "on_before_zero_grad                \t|  5.018e-05      \t|720            \t|  0.03613        \t|  0.002562       \t|\n",
      "on_train_batch_start               \t|  4.654e-05      \t|720            \t|  0.033509       \t|  0.0023761      \t|\n",
      "on_after_backward                  \t|  4.6024e-05     \t|720            \t|  0.033137       \t|  0.0023498      \t|\n",
      "on_test_batch_end                  \t|  0.00037239     \t|88             \t|  0.03277        \t|  0.0023237      \t|\n",
      "on_before_backward                 \t|  4.2185e-05     \t|720            \t|  0.030373       \t|  0.0021537      \t|\n",
      "on_batch_end                       \t|  4.071e-05      \t|720            \t|  0.029312       \t|  0.0020785      \t|\n",
      "training_step_end                  \t|  3.3604e-05     \t|720            \t|  0.024195       \t|  0.0017156      \t|\n",
      "on_validation_batch_start          \t|  9.4559e-05     \t|180            \t|  0.017021       \t|  0.0012069      \t|\n",
      "validation_step_end                \t|  2.9312e-05     \t|180            \t|  0.0052762      \t|  0.00037413     \t|\n",
      "on_test_batch_start                \t|  5.4881e-05     \t|88             \t|  0.0048295      \t|  0.00034246     \t|\n",
      "test_step_end                      \t|  2.9209e-05     \t|88             \t|  0.0025704      \t|  0.00018227     \t|\n",
      "on_epoch_end                       \t|  8.2899e-05     \t|26             \t|  0.0021554      \t|  0.00015284     \t|\n",
      "on_test_end                        \t|  0.0017573      \t|1              \t|  0.0017573      \t|  0.00012461     \t|\n",
      "on_epoch_start                     \t|  5.7393e-05     \t|26             \t|  0.0014922      \t|  0.00010581     \t|\n",
      "on_validation_epoch_start          \t|  3.783e-05      \t|13             \t|  0.00049179     \t|  3.4873e-05     \t|\n",
      "on_fit_start                       \t|  0.00019467     \t|1              \t|  0.00019467     \t|  1.3804e-05     \t|\n",
      "on_test_epoch_end                  \t|  0.00016592     \t|1              \t|  0.00016592     \t|  1.1765e-05     \t|\n",
      "on_fit_end                         \t|  0.00015196     \t|1              \t|  0.00015196     \t|  1.0775e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  4.5365e-05     \t|3              \t|  0.0001361      \t|  9.6506e-06     \t|\n",
      "on_test_epoch_start                \t|  5.4913e-05     \t|1              \t|  5.4913e-05     \t|  3.8939e-06     \t|\n",
      "on_val_dataloader                  \t|  4.6107e-05     \t|1              \t|  4.6107e-05     \t|  3.2694e-06     \t|\n",
      "on_train_dataloader                \t|  2.6901e-05     \t|1              \t|  2.6901e-05     \t|  1.9075e-06     \t|\n",
      "on_test_dataloader                 \t|  2.5262e-05     \t|1              \t|  2.5262e-05     \t|  1.7913e-06     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.3532838225364685, 'test_loss': 2.026762008666992}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report\n",
    "\n",
    "\n",
    "# def predict_step(batch, batch_idx=None):\n",
    "#     out = self.step(batch, batch_idx)\n",
    "#     if hasattr(batch, \"metadata\"):\n",
    "#         if \"path\" in batch.metadata:\n",
    "#             out = [*out, batch.metadata[\"path\"]]\n",
    "#     return out\n",
    "\n",
    "# self=model\n",
    "# model.predict_step = predict_step\n",
    "# test_results = trainer.predict(dataloaders=datamodule.test_dataloader(), return_predictions=True)\n",
    "# results = collect_results(prediction_results)\n",
    "prediction_results = test_results\n",
    "results = collect_results(prediction_results)\n",
    "len(prediction_results[0])\n",
    "# len(results)\n",
    "\n",
    "def tensors2np(t: Union[torch.Tensor, list]) -> np.ndarray:\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        t = t.cpu().numpy()\n",
    "    elif isinstance(t, list):\n",
    "        t = list(map(tensors2np, t))\n",
    "    if isinstance(t, np.ndarray):\n",
    "        return t\n",
    "    else:\n",
    "        raise TypeError(f\"type(t)={type(t)} is invalid for function tensors2np\" + '\\n' + 'tensors2np(t: Union[torch.Tensor, list]) -> np.ndarray:')\n",
    "        \n",
    "rows = []\n",
    "for result in list(prediction_results):\n",
    "    \n",
    "    y_logit.append(result[0])\n",
    "    y_true.append(result[1])\n",
    "    y_pred.append(result[2])\n",
    "    paths.extend(result[3])\n",
    "    \n",
    "y_logit = torch.cat(y_logit).cpu().numpy()\n",
    "y_true = torch.cat(y_true).cpu().numpy()\n",
    "y_pred = torch.cat(y_pred).cpu().numpy()\n",
    "# paths = torch.cat(paths).cpu().numpy()\n",
    "\n",
    "# [(r[0].device, r[0].shape, r[1].shape, r[2].shape) for r in test_results]\n",
    "\n",
    "print(y_logit.shape, y_true.shape, y_pred.shape, len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = model.label_encoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F\n",
    "\n",
    "\n",
    "\n",
    "# https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwji_bKG6cPzAhXJT98KHU0eCsg4HhAWegQIAhAB&url=https%3A%2F%2Fclear.ml%2Fdocs%2Flatest%2Fdocs%2Fguides%2Freporting%2Fexplicit_reporting%2F&usg=AOvVaw3tvUYT7fU3QHIwunDpE800\n",
    "labels = model.label_encoder.classes\n",
    "\n",
    "test_predictions_filepath = os.path.join(results_dir, f\"task_{task_id}\", \"test_predictions.csv\")\n",
    "\n",
    "\n",
    "class ImageInterpretation:\n",
    "    \n",
    "    def __init__(self, model, datamodule, trainer, y_col: str='family'):\n",
    "        self.model = model\n",
    "        self.dm = datamodule\n",
    "        self.trainer = trainer\n",
    "        self.y_col = y_col\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def decoder(self):\n",
    "        return self.dm.label_encoder.idx2class\n",
    "        \n",
    "    def decode_label(y: int): #, labels: Union[Dict[int, str], List[str]]=None):\n",
    "        try:\n",
    "            return self.decoder[y]\n",
    "        except:\n",
    "            return y\n",
    "        \n",
    "    def log_image_predictions(self,\n",
    "                              results_path: str=None,\n",
    "                              sort_by_losses: bool=True,\n",
    "                              ascending: bool=True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Save table of model predictions as csv\n",
    "        \n",
    "        |losses\t|y_true\t|y_pred\t|paths \t|per-class logits|\n",
    "        |---\t|---\t|---\t|---\t| ---\t |---\t |\n",
    "        |   \t|   \t|   \t|   \t|   \t |   \t |\n",
    "        |   \t|   \t|   \t|   \t|   \t |   \t |\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        pred_results = trainer.predict(dataloaders=datamodule.test_dataloader(), return_predictions=True)\n",
    "\n",
    "        results = collect_results(pred_results)\n",
    "\n",
    "        labels = list(self.decoder.values())\n",
    "        columns = [\"xEnt_loss\", f\"{self.y_col}_true\", f\"{self.y_col}_pred\", \"paths\", *[f\"{l}_logit\" for l in labels]]\n",
    "\n",
    "        y_logits = torch.from_numpy(results[0].astype(\"float32\"))\n",
    "        y_true = torch.from_numpy(results[1])\n",
    "        xEnt_loss = F.cross_entropy(y_logits, y_true, reduction=\"none\")\n",
    "\n",
    "        losses = xEnt_loss\n",
    "        y_true = results[1]\n",
    "        y_pred = results[2]\n",
    "        paths = results[3]\n",
    "        per_class_y_logits = np.hsplit(results[0], results[0].shape[1])\n",
    "\n",
    "        num_results = len(results[0])\n",
    "        rows = []\n",
    "        for i in range(num_results):\n",
    "            rows.append({k:v for k, v in zip(columns,\n",
    "                                             [losses,\n",
    "                                              self.decode_label(y=y_true[i]),\n",
    "                                              self.decode_label(y=y_pred[i]),\n",
    "                                              paths[i],\n",
    "                                              *(y[i].item() for y in per_class_y_logits)]\n",
    "                                            )\n",
    "                        })\n",
    "\n",
    "        data_df = pd.DataFrame.from_records(rows)\n",
    "        \n",
    "        if sort_by_losses:\n",
    "            data_df = data_df.sort_values(\"xEnt_loss\", ascending=ascending)\n",
    "        \n",
    "        ETL.df2csv(data_df, results_spath)\n",
    "        \n",
    "        return data_df\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#         data = {\"xEnt_loss\":xEnt_loss,\n",
    "#                 f\"{self.y_col}_true\":results[1],\n",
    "#                 f\"{self.y_col}_pred\":results[2],\n",
    "#                 \"path\":results[3],\n",
    "#                 \"y_logits\":np.hsplit(results[0], results[0].shape[1])}    \n",
    "    \n",
    "    \n",
    "    \n",
    "#         rows = []\n",
    "#         for i in range(num_results):\n",
    "#             rows.append({k:v for k, v in zip(columns,\n",
    "#                                              [data[\"xEnt_loss\"][i], \n",
    "#                                               self.decode_label(y=data[f\"{self.y_col}_true\"][i]),\n",
    "#                                               self.decode_label(y=data[f\"{self.y_col}_pred\"][i]),\n",
    "#                                               data[\"path\"][i],\n",
    "#                                               *(y[i].item() for y in data[\"y_logits\"])]\n",
    "#                                             )\n",
    "#                         })\n",
    "\n",
    "#         data_df = pd.DataFrame.from_records(rows)\n",
    "#         ETL.df2csv(data_df, test_predictions_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_true</th>\n",
       "      <th>family_pred</th>\n",
       "      <th>paths</th>\n",
       "      <th>Anacardiaceae_logit</th>\n",
       "      <th>Annonaceae_logit</th>\n",
       "      <th>Apocynaceae_logit</th>\n",
       "      <th>Betulaceae_logit</th>\n",
       "      <th>Celastraceae_logit</th>\n",
       "      <th>Combretaceae_logit</th>\n",
       "      <th>Ericaceae_logit</th>\n",
       "      <th>Fabaceae_logit</th>\n",
       "      <th>Fagaceae_logit</th>\n",
       "      <th>Lauraceae_logit</th>\n",
       "      <th>Malvaceae_logit</th>\n",
       "      <th>Melastomataceae_logit</th>\n",
       "      <th>Myrtaceae_logit</th>\n",
       "      <th>Passifloraceae_logit</th>\n",
       "      <th>Phyllanthaceae_logit</th>\n",
       "      <th>Rosaceae_logit</th>\n",
       "      <th>Rubiaceae_logit</th>\n",
       "      <th>Salicaceae_logit</th>\n",
       "      <th>Sapindaceae_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Betulaceae/Betulaceae_Corylus_sieboldiana_Wolfe_8505.jpg</td>\n",
       "      <td>-3.701172</td>\n",
       "      <td>-1.307617</td>\n",
       "      <td>-3.009766</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-3.759766</td>\n",
       "      <td>-5.464844</td>\n",
       "      <td>-3.427734</td>\n",
       "      <td>-3.501953</td>\n",
       "      <td>2.558594</td>\n",
       "      <td>-2.662109</td>\n",
       "      <td>-0.223267</td>\n",
       "      <td>-5.761719</td>\n",
       "      <td>-5.398438</td>\n",
       "      <td>-5.808594</td>\n",
       "      <td>-5.355469</td>\n",
       "      <td>4.937500</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>-2.923828</td>\n",
       "      <td>2.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Betulaceae/Betulaceae_Carpinus_minutiserrata_Wolfe_8499.jpg</td>\n",
       "      <td>-3.636719</td>\n",
       "      <td>-0.404541</td>\n",
       "      <td>-3.140625</td>\n",
       "      <td>6.570312</td>\n",
       "      <td>-3.857422</td>\n",
       "      <td>-5.554688</td>\n",
       "      <td>-2.789062</td>\n",
       "      <td>-3.111328</td>\n",
       "      <td>2.324219</td>\n",
       "      <td>-1.705078</td>\n",
       "      <td>-0.510254</td>\n",
       "      <td>-4.355469</td>\n",
       "      <td>-4.910156</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>-5.363281</td>\n",
       "      <td>4.960938</td>\n",
       "      <td>-1.021484</td>\n",
       "      <td>-3.556641</td>\n",
       "      <td>1.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Salicaceae/Salicaceae_Salix_paradoxa_Wolfe_18143.jpg</td>\n",
       "      <td>-3.542969</td>\n",
       "      <td>-0.666504</td>\n",
       "      <td>-0.868652</td>\n",
       "      <td>-4.914062</td>\n",
       "      <td>-1.795898</td>\n",
       "      <td>-0.714355</td>\n",
       "      <td>-3.437500</td>\n",
       "      <td>-1.461914</td>\n",
       "      <td>-1.703125</td>\n",
       "      <td>-0.938477</td>\n",
       "      <td>-1.060547</td>\n",
       "      <td>-4.484375</td>\n",
       "      <td>-1.119141</td>\n",
       "      <td>-2.339844</td>\n",
       "      <td>-0.384521</td>\n",
       "      <td>-2.453125</td>\n",
       "      <td>-0.583984</td>\n",
       "      <td>0.764648</td>\n",
       "      <td>-2.248047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Melastomataceae/Melastomataceae_Miconia_candolleana_Wolfe_7579.jpg</td>\n",
       "      <td>-3.531250</td>\n",
       "      <td>-1.310547</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-3.238281</td>\n",
       "      <td>-1.906250</td>\n",
       "      <td>-0.319824</td>\n",
       "      <td>-2.828125</td>\n",
       "      <td>-0.099548</td>\n",
       "      <td>-1.003906</td>\n",
       "      <td>-1.366211</td>\n",
       "      <td>-2.339844</td>\n",
       "      <td>-2.994141</td>\n",
       "      <td>-0.514160</td>\n",
       "      <td>-2.351562</td>\n",
       "      <td>-1.755859</td>\n",
       "      <td>-1.009766</td>\n",
       "      <td>0.995605</td>\n",
       "      <td>-0.788086</td>\n",
       "      <td>-1.653320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Betulaceae/Betulaceae_Betula_delavayi_Wolfe_8515.jpg</td>\n",
       "      <td>-3.523438</td>\n",
       "      <td>-1.132812</td>\n",
       "      <td>-2.919922</td>\n",
       "      <td>5.539062</td>\n",
       "      <td>-3.652344</td>\n",
       "      <td>-5.593750</td>\n",
       "      <td>-3.361328</td>\n",
       "      <td>-3.464844</td>\n",
       "      <td>2.539062</td>\n",
       "      <td>-2.875000</td>\n",
       "      <td>-0.327637</td>\n",
       "      <td>-5.246094</td>\n",
       "      <td>-4.781250</td>\n",
       "      <td>-5.527344</td>\n",
       "      <td>-5.289062</td>\n",
       "      <td>4.582031</td>\n",
       "      <td>0.183960</td>\n",
       "      <td>-2.988281</td>\n",
       "      <td>2.130859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Anacardiaceae/Anacardiaceae_Anacardium_microsepalum_Wolfe_4202.jpg</td>\n",
       "      <td>2.031250</td>\n",
       "      <td>2.220703</td>\n",
       "      <td>0.378418</td>\n",
       "      <td>-1.996094</td>\n",
       "      <td>-2.458984</td>\n",
       "      <td>-5.179688</td>\n",
       "      <td>-1.397461</td>\n",
       "      <td>-1.719727</td>\n",
       "      <td>-1.301758</td>\n",
       "      <td>-0.013664</td>\n",
       "      <td>-3.351562</td>\n",
       "      <td>-2.388672</td>\n",
       "      <td>-0.104553</td>\n",
       "      <td>-4.355469</td>\n",
       "      <td>-3.957031</td>\n",
       "      <td>-1.269531</td>\n",
       "      <td>-0.753418</td>\n",
       "      <td>-3.931641</td>\n",
       "      <td>0.276123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Apocynaceae/Apocynaceae_Condylocarpon_amazonicum_Wolfe_9166.jpg</td>\n",
       "      <td>2.072266</td>\n",
       "      <td>1.169922</td>\n",
       "      <td>0.980957</td>\n",
       "      <td>-1.467773</td>\n",
       "      <td>-2.337891</td>\n",
       "      <td>-5.261719</td>\n",
       "      <td>-1.712891</td>\n",
       "      <td>-1.980469</td>\n",
       "      <td>-1.117188</td>\n",
       "      <td>-1.140625</td>\n",
       "      <td>-3.160156</td>\n",
       "      <td>-1.918945</td>\n",
       "      <td>0.444092</td>\n",
       "      <td>-3.927734</td>\n",
       "      <td>-4.417969</td>\n",
       "      <td>-1.185547</td>\n",
       "      <td>-0.756348</td>\n",
       "      <td>-3.855469</td>\n",
       "      <td>0.513184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Anacardiaceae/Anacardiaceae_Anacardium_humile_Wolfe_12854.jpg</td>\n",
       "      <td>2.318359</td>\n",
       "      <td>2.531250</td>\n",
       "      <td>0.029694</td>\n",
       "      <td>-2.062500</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>-4.988281</td>\n",
       "      <td>-1.211914</td>\n",
       "      <td>-1.269531</td>\n",
       "      <td>-1.435547</td>\n",
       "      <td>-0.288818</td>\n",
       "      <td>-3.263672</td>\n",
       "      <td>-1.830078</td>\n",
       "      <td>-0.763672</td>\n",
       "      <td>-4.531250</td>\n",
       "      <td>-3.882812</td>\n",
       "      <td>-1.215820</td>\n",
       "      <td>-0.182251</td>\n",
       "      <td>-4.171875</td>\n",
       "      <td>-0.325928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Annonaceae/Annonaceae_Cyathocalyx_pahangensis_Wolfe_7860.jpg</td>\n",
       "      <td>2.400391</td>\n",
       "      <td>1.263672</td>\n",
       "      <td>0.446533</td>\n",
       "      <td>-1.714844</td>\n",
       "      <td>-2.597656</td>\n",
       "      <td>-5.339844</td>\n",
       "      <td>-1.846680</td>\n",
       "      <td>-1.754883</td>\n",
       "      <td>-1.322266</td>\n",
       "      <td>-0.888672</td>\n",
       "      <td>-2.570312</td>\n",
       "      <td>-2.365234</td>\n",
       "      <td>-0.152954</td>\n",
       "      <td>-3.791016</td>\n",
       "      <td>-4.105469</td>\n",
       "      <td>-1.630859</td>\n",
       "      <td>-0.832520</td>\n",
       "      <td>-3.966797</td>\n",
       "      <td>0.582031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Apocynaceae/Apocynaceae_Dyera_lowii_Wolfe_8430.jpg</td>\n",
       "      <td>2.603516</td>\n",
       "      <td>1.859375</td>\n",
       "      <td>0.836426</td>\n",
       "      <td>-2.263672</td>\n",
       "      <td>-2.771484</td>\n",
       "      <td>-4.992188</td>\n",
       "      <td>-1.857422</td>\n",
       "      <td>-1.131836</td>\n",
       "      <td>-1.219727</td>\n",
       "      <td>-0.742188</td>\n",
       "      <td>-3.986328</td>\n",
       "      <td>-2.173828</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>-4.257812</td>\n",
       "      <td>-4.199219</td>\n",
       "      <td>-2.175781</td>\n",
       "      <td>-0.663086</td>\n",
       "      <td>-4.609375</td>\n",
       "      <td>-0.463867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2797 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      family_true  family_pred  \\\n",
       "2382            3            3   \n",
       "2379            3            3   \n",
       "1310           17           17   \n",
       "2220           11           16   \n",
       "2388            3            3   \n",
       "...           ...          ...   \n",
       "1578            0            1   \n",
       "2503            2            0   \n",
       "703             0            1   \n",
       "2281            1            0   \n",
       "2362            2            0   \n",
       "\n",
       "                                                                                                                                                                   paths  \\\n",
       "2382            /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Betulaceae/Betulaceae_Corylus_sieboldiana_Wolfe_8505.jpg   \n",
       "2379         /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Betulaceae/Betulaceae_Carpinus_minutiserrata_Wolfe_8499.jpg   \n",
       "1310                /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Salicaceae/Salicaceae_Salix_paradoxa_Wolfe_18143.jpg   \n",
       "2220   /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Melastomataceae/Melastomataceae_Miconia_candolleana_Wolfe_7579.jpg   \n",
       "2388                /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Betulaceae/Betulaceae_Betula_delavayi_Wolfe_8515.jpg   \n",
       "...                                                                                                                                                                  ...   \n",
       "1578  /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Anacardiaceae/Anacardiaceae_Anacardium_microsepalum_Wolfe_4202.jpg   \n",
       "2503      /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Apocynaceae/Apocynaceae_Condylocarpon_amazonicum_Wolfe_9166.jpg   \n",
       "703        /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Anacardiaceae/Anacardiaceae_Anacardium_humile_Wolfe_12854.jpg   \n",
       "2281        /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Annonaceae/Annonaceae_Cyathocalyx_pahangensis_Wolfe_7860.jpg   \n",
       "2362                  /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Apocynaceae/Apocynaceae_Dyera_lowii_Wolfe_8430.jpg   \n",
       "\n",
       "      Anacardiaceae_logit  Annonaceae_logit  Apocynaceae_logit  \\\n",
       "2382            -3.701172         -1.307617          -3.009766   \n",
       "2379            -3.636719         -0.404541          -3.140625   \n",
       "1310            -3.542969         -0.666504          -0.868652   \n",
       "2220            -3.531250         -1.310547           0.375000   \n",
       "2388            -3.523438         -1.132812          -2.919922   \n",
       "...                   ...               ...                ...   \n",
       "1578             2.031250          2.220703           0.378418   \n",
       "2503             2.072266          1.169922           0.980957   \n",
       "703              2.318359          2.531250           0.029694   \n",
       "2281             2.400391          1.263672           0.446533   \n",
       "2362             2.603516          1.859375           0.836426   \n",
       "\n",
       "      Betulaceae_logit  Celastraceae_logit  Combretaceae_logit  \\\n",
       "2382          6.000000           -3.759766           -5.464844   \n",
       "2379          6.570312           -3.857422           -5.554688   \n",
       "1310         -4.914062           -1.795898           -0.714355   \n",
       "2220         -3.238281           -1.906250           -0.319824   \n",
       "2388          5.539062           -3.652344           -5.593750   \n",
       "...                ...                 ...                 ...   \n",
       "1578         -1.996094           -2.458984           -5.179688   \n",
       "2503         -1.467773           -2.337891           -5.261719   \n",
       "703          -2.062500           -2.750000           -4.988281   \n",
       "2281         -1.714844           -2.597656           -5.339844   \n",
       "2362         -2.263672           -2.771484           -4.992188   \n",
       "\n",
       "      Ericaceae_logit  Fabaceae_logit  Fagaceae_logit  Lauraceae_logit  \\\n",
       "2382        -3.427734       -3.501953        2.558594        -2.662109   \n",
       "2379        -2.789062       -3.111328        2.324219        -1.705078   \n",
       "1310        -3.437500       -1.461914       -1.703125        -0.938477   \n",
       "2220        -2.828125       -0.099548       -1.003906        -1.366211   \n",
       "2388        -3.361328       -3.464844        2.539062        -2.875000   \n",
       "...               ...             ...             ...              ...   \n",
       "1578        -1.397461       -1.719727       -1.301758        -0.013664   \n",
       "2503        -1.712891       -1.980469       -1.117188        -1.140625   \n",
       "703         -1.211914       -1.269531       -1.435547        -0.288818   \n",
       "2281        -1.846680       -1.754883       -1.322266        -0.888672   \n",
       "2362        -1.857422       -1.131836       -1.219727        -0.742188   \n",
       "\n",
       "      Malvaceae_logit  Melastomataceae_logit  Myrtaceae_logit  \\\n",
       "2382        -0.223267              -5.761719        -5.398438   \n",
       "2379        -0.510254              -4.355469        -4.910156   \n",
       "1310        -1.060547              -4.484375        -1.119141   \n",
       "2220        -2.339844              -2.994141        -0.514160   \n",
       "2388        -0.327637              -5.246094        -4.781250   \n",
       "...               ...                    ...              ...   \n",
       "1578        -3.351562              -2.388672        -0.104553   \n",
       "2503        -3.160156              -1.918945         0.444092   \n",
       "703         -3.263672              -1.830078        -0.763672   \n",
       "2281        -2.570312              -2.365234        -0.152954   \n",
       "2362        -3.986328              -2.173828         0.757812   \n",
       "\n",
       "      Passifloraceae_logit  Phyllanthaceae_logit  Rosaceae_logit  \\\n",
       "2382             -5.808594             -5.355469        4.937500   \n",
       "2379             -6.250000             -5.363281        4.960938   \n",
       "1310             -2.339844             -0.384521       -2.453125   \n",
       "2220             -2.351562             -1.755859       -1.009766   \n",
       "2388             -5.527344             -5.289062        4.582031   \n",
       "...                    ...                   ...             ...   \n",
       "1578             -4.355469             -3.957031       -1.269531   \n",
       "2503             -3.927734             -4.417969       -1.185547   \n",
       "703              -4.531250             -3.882812       -1.215820   \n",
       "2281             -3.791016             -4.105469       -1.630859   \n",
       "2362             -4.257812             -4.199219       -2.175781   \n",
       "\n",
       "      Rubiaceae_logit  Salicaceae_logit  Sapindaceae_logit  \n",
       "2382         0.086670         -2.923828           2.140625  \n",
       "2379        -1.021484         -3.556641           1.875000  \n",
       "1310        -0.583984          0.764648          -2.248047  \n",
       "2220         0.995605         -0.788086          -1.653320  \n",
       "2388         0.183960         -2.988281           2.130859  \n",
       "...               ...               ...                ...  \n",
       "1578        -0.753418         -3.931641           0.276123  \n",
       "2503        -0.756348         -3.855469           0.513184  \n",
       "703         -0.182251         -4.171875          -0.325928  \n",
       "2281        -0.832520         -3.966797           0.582031  \n",
       "2362        -0.663086         -4.609375          -0.463867  \n",
       "\n",
       "[2797 rows x 22 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.sort_values(\"Anacardiaceae_logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_true</th>\n",
       "      <th>family_pred</th>\n",
       "      <th>paths</th>\n",
       "      <th>Anacardiaceae_logit</th>\n",
       "      <th>Annonaceae_logit</th>\n",
       "      <th>Apocynaceae_logit</th>\n",
       "      <th>Betulaceae_logit</th>\n",
       "      <th>Celastraceae_logit</th>\n",
       "      <th>Combretaceae_logit</th>\n",
       "      <th>Ericaceae_logit</th>\n",
       "      <th>Fabaceae_logit</th>\n",
       "      <th>Fagaceae_logit</th>\n",
       "      <th>Lauraceae_logit</th>\n",
       "      <th>Malvaceae_logit</th>\n",
       "      <th>Melastomataceae_logit</th>\n",
       "      <th>Myrtaceae_logit</th>\n",
       "      <th>Passifloraceae_logit</th>\n",
       "      <th>Phyllanthaceae_logit</th>\n",
       "      <th>Rosaceae_logit</th>\n",
       "      <th>Rubiaceae_logit</th>\n",
       "      <th>Salicaceae_logit</th>\n",
       "      <th>Sapindaceae_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Fagaceae/Fagaceae_Lithocarpus_densiflora_Axelrod_105.jpg</td>\n",
       "      <td>-2.101562</td>\n",
       "      <td>0.946777</td>\n",
       "      <td>-2.197266</td>\n",
       "      <td>-5.257812</td>\n",
       "      <td>-1.637695</td>\n",
       "      <td>-4.351562</td>\n",
       "      <td>-0.747070</td>\n",
       "      <td>2.486328</td>\n",
       "      <td>-2.169922</td>\n",
       "      <td>1.900391</td>\n",
       "      <td>-2.886719</td>\n",
       "      <td>-4.980469</td>\n",
       "      <td>-2.761719</td>\n",
       "      <td>-3.220703</td>\n",
       "      <td>-2.039062</td>\n",
       "      <td>-2.855469</td>\n",
       "      <td>-1.358398</td>\n",
       "      <td>-2.119141</td>\n",
       "      <td>-1.598633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Ericaceae/Ericaceae_Arctostaphylos_bicolor_Axelrod_1064.jpg</td>\n",
       "      <td>-1.948242</td>\n",
       "      <td>-0.208008</td>\n",
       "      <td>-1.406250</td>\n",
       "      <td>-2.642578</td>\n",
       "      <td>-0.841797</td>\n",
       "      <td>-3.482422</td>\n",
       "      <td>-1.020508</td>\n",
       "      <td>-0.872070</td>\n",
       "      <td>-1.964844</td>\n",
       "      <td>-1.522461</td>\n",
       "      <td>-0.915039</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>-3.214844</td>\n",
       "      <td>-2.896484</td>\n",
       "      <td>-1.683594</td>\n",
       "      <td>0.409424</td>\n",
       "      <td>-1.733398</td>\n",
       "      <td>1.350586</td>\n",
       "      <td>-0.226807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Fabaceae/Fabaceae_Amorpha_californica_Axelrod_107.jpg</td>\n",
       "      <td>-1.899414</td>\n",
       "      <td>0.320557</td>\n",
       "      <td>-1.014648</td>\n",
       "      <td>-2.306641</td>\n",
       "      <td>-0.415283</td>\n",
       "      <td>-2.873047</td>\n",
       "      <td>0.164185</td>\n",
       "      <td>0.775391</td>\n",
       "      <td>-1.668945</td>\n",
       "      <td>-0.221802</td>\n",
       "      <td>-1.568359</td>\n",
       "      <td>-3.824219</td>\n",
       "      <td>-2.728516</td>\n",
       "      <td>-2.455078</td>\n",
       "      <td>-2.048828</td>\n",
       "      <td>0.622559</td>\n",
       "      <td>-1.046875</td>\n",
       "      <td>0.237793</td>\n",
       "      <td>-0.510742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Anacardiaceae/Anacardiaceae_Rhus_scheidana_Axelrod_1083.jpg</td>\n",
       "      <td>-2.119141</td>\n",
       "      <td>0.247192</td>\n",
       "      <td>-0.884277</td>\n",
       "      <td>-3.435547</td>\n",
       "      <td>-0.860840</td>\n",
       "      <td>-2.496094</td>\n",
       "      <td>-0.708984</td>\n",
       "      <td>0.709961</td>\n",
       "      <td>-1.564453</td>\n",
       "      <td>-0.077454</td>\n",
       "      <td>-1.122070</td>\n",
       "      <td>-3.429688</td>\n",
       "      <td>-2.224609</td>\n",
       "      <td>-2.107422</td>\n",
       "      <td>-1.483398</td>\n",
       "      <td>-0.923340</td>\n",
       "      <td>-1.385742</td>\n",
       "      <td>0.063416</td>\n",
       "      <td>-0.836426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Rosaceae/Rosaceae_Holodiscus_bousierii_Axelrod_11.jpg</td>\n",
       "      <td>-1.210938</td>\n",
       "      <td>-0.155151</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>-0.239014</td>\n",
       "      <td>-1.089844</td>\n",
       "      <td>-3.675781</td>\n",
       "      <td>-0.708984</td>\n",
       "      <td>-0.143677</td>\n",
       "      <td>-0.320557</td>\n",
       "      <td>-1.643555</td>\n",
       "      <td>-1.702148</td>\n",
       "      <td>-4.644531</td>\n",
       "      <td>-3.546875</td>\n",
       "      <td>-4.359375</td>\n",
       "      <td>-3.392578</td>\n",
       "      <td>1.975586</td>\n",
       "      <td>0.340088</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.621094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2792</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Fabaceae/Fabaceae_Spatholobus_sanguineus_Wolfe_9986.jpg</td>\n",
       "      <td>-1.414062</td>\n",
       "      <td>-0.408203</td>\n",
       "      <td>-1.287109</td>\n",
       "      <td>-1.636719</td>\n",
       "      <td>-1.176758</td>\n",
       "      <td>-3.201172</td>\n",
       "      <td>-1.117188</td>\n",
       "      <td>0.658691</td>\n",
       "      <td>-0.553223</td>\n",
       "      <td>-0.359619</td>\n",
       "      <td>-0.608398</td>\n",
       "      <td>-4.261719</td>\n",
       "      <td>-3.195312</td>\n",
       "      <td>-2.927734</td>\n",
       "      <td>-2.158203</td>\n",
       "      <td>-0.062805</td>\n",
       "      <td>-0.152954</td>\n",
       "      <td>-1.143555</td>\n",
       "      <td>0.043213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Fabaceae/Fabaceae_Strongylodon_lucidus_Wolfe_9991.jpg</td>\n",
       "      <td>-1.657227</td>\n",
       "      <td>-0.341553</td>\n",
       "      <td>-1.412109</td>\n",
       "      <td>-2.822266</td>\n",
       "      <td>-1.095703</td>\n",
       "      <td>-2.773438</td>\n",
       "      <td>-0.649414</td>\n",
       "      <td>1.941406</td>\n",
       "      <td>-1.231445</td>\n",
       "      <td>-0.438477</td>\n",
       "      <td>-1.717773</td>\n",
       "      <td>-4.644531</td>\n",
       "      <td>-3.406250</td>\n",
       "      <td>-3.111328</td>\n",
       "      <td>-2.568359</td>\n",
       "      <td>-0.196289</td>\n",
       "      <td>-0.147827</td>\n",
       "      <td>-1.016602</td>\n",
       "      <td>-0.682617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Fabaceae/Fabaceae_Sweetia_nitens_Wolfe_9994.jpg</td>\n",
       "      <td>-1.159180</td>\n",
       "      <td>0.910645</td>\n",
       "      <td>-1.306641</td>\n",
       "      <td>-3.710938</td>\n",
       "      <td>-1.150391</td>\n",
       "      <td>-4.042969</td>\n",
       "      <td>-0.448486</td>\n",
       "      <td>0.985352</td>\n",
       "      <td>-1.852539</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>-2.884766</td>\n",
       "      <td>-3.646484</td>\n",
       "      <td>-1.609375</td>\n",
       "      <td>-2.974609</td>\n",
       "      <td>-2.533203</td>\n",
       "      <td>-1.801758</td>\n",
       "      <td>-1.070312</td>\n",
       "      <td>-1.525391</td>\n",
       "      <td>-0.565918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Fabaceae/Fabaceae_Templetonia_retusa_Wolfe_9995.jpg</td>\n",
       "      <td>-1.577148</td>\n",
       "      <td>-1.782227</td>\n",
       "      <td>-1.040039</td>\n",
       "      <td>-3.068359</td>\n",
       "      <td>-2.150391</td>\n",
       "      <td>-1.496094</td>\n",
       "      <td>-1.236328</td>\n",
       "      <td>3.931641</td>\n",
       "      <td>-1.172852</td>\n",
       "      <td>-2.310547</td>\n",
       "      <td>-3.220703</td>\n",
       "      <td>-5.195312</td>\n",
       "      <td>-2.341797</td>\n",
       "      <td>-3.683594</td>\n",
       "      <td>-3.480469</td>\n",
       "      <td>-0.072754</td>\n",
       "      <td>0.024765</td>\n",
       "      <td>-1.898438</td>\n",
       "      <td>-1.231445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Fabaceae/Fabaceae_Tessmannia_anomala_Wolfe_9998.jpg</td>\n",
       "      <td>-0.927734</td>\n",
       "      <td>-0.419189</td>\n",
       "      <td>-0.990234</td>\n",
       "      <td>-4.324219</td>\n",
       "      <td>-1.419922</td>\n",
       "      <td>-2.306641</td>\n",
       "      <td>-0.715332</td>\n",
       "      <td>4.031250</td>\n",
       "      <td>-1.281250</td>\n",
       "      <td>-1.117188</td>\n",
       "      <td>-3.248047</td>\n",
       "      <td>-5.554688</td>\n",
       "      <td>-1.846680</td>\n",
       "      <td>-3.681641</td>\n",
       "      <td>-2.121094</td>\n",
       "      <td>-1.324219</td>\n",
       "      <td>-0.002041</td>\n",
       "      <td>-2.394531</td>\n",
       "      <td>-2.003906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2797 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      family_true  family_pred  \\\n",
       "0               8            7   \n",
       "1               6           17   \n",
       "2               7            7   \n",
       "3               0            7   \n",
       "4              15           18   \n",
       "...           ...          ...   \n",
       "2792            7            7   \n",
       "2793            7            7   \n",
       "2794            7            7   \n",
       "2795            7            7   \n",
       "2796            7            7   \n",
       "\n",
       "                                                                                                                                                           paths  \\\n",
       "0       /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Fagaceae/Fagaceae_Lithocarpus_densiflora_Axelrod_105.jpg   \n",
       "1     /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Ericaceae/Ericaceae_Arctostaphylos_bicolor_Axelrod_1064.jpg   \n",
       "2           /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Fabaceae/Fabaceae_Amorpha_californica_Axelrod_107.jpg   \n",
       "3     /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Anacardiaceae/Anacardiaceae_Rhus_scheidana_Axelrod_1083.jpg   \n",
       "4          /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Rosaceae/Rosaceae_Holodiscus_bousierii_Axelrod_11.jpg   \n",
       "...                                                                                                                                                          ...   \n",
       "2792     /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Fabaceae/Fabaceae_Spatholobus_sanguineus_Wolfe_9986.jpg   \n",
       "2793        /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Fabaceae/Fabaceae_Strongylodon_lucidus_Wolfe_9991.jpg   \n",
       "2794              /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/test/Fabaceae/Fabaceae_Sweetia_nitens_Wolfe_9994.jpg   \n",
       "2795         /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Fabaceae/Fabaceae_Templetonia_retusa_Wolfe_9995.jpg   \n",
       "2796         /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_512/train/Fabaceae/Fabaceae_Tessmannia_anomala_Wolfe_9998.jpg   \n",
       "\n",
       "      Anacardiaceae_logit  Annonaceae_logit  Apocynaceae_logit  \\\n",
       "0               -2.101562          0.946777          -2.197266   \n",
       "1               -1.948242         -0.208008          -1.406250   \n",
       "2               -1.899414          0.320557          -1.014648   \n",
       "3               -2.119141          0.247192          -0.884277   \n",
       "4               -1.210938         -0.155151          -0.820312   \n",
       "...                   ...               ...                ...   \n",
       "2792            -1.414062         -0.408203          -1.287109   \n",
       "2793            -1.657227         -0.341553          -1.412109   \n",
       "2794            -1.159180          0.910645          -1.306641   \n",
       "2795            -1.577148         -1.782227          -1.040039   \n",
       "2796            -0.927734         -0.419189          -0.990234   \n",
       "\n",
       "      Betulaceae_logit  Celastraceae_logit  Combretaceae_logit  \\\n",
       "0            -5.257812           -1.637695           -4.351562   \n",
       "1            -2.642578           -0.841797           -3.482422   \n",
       "2            -2.306641           -0.415283           -2.873047   \n",
       "3            -3.435547           -0.860840           -2.496094   \n",
       "4            -0.239014           -1.089844           -3.675781   \n",
       "...                ...                 ...                 ...   \n",
       "2792         -1.636719           -1.176758           -3.201172   \n",
       "2793         -2.822266           -1.095703           -2.773438   \n",
       "2794         -3.710938           -1.150391           -4.042969   \n",
       "2795         -3.068359           -2.150391           -1.496094   \n",
       "2796         -4.324219           -1.419922           -2.306641   \n",
       "\n",
       "      Ericaceae_logit  Fabaceae_logit  Fagaceae_logit  Lauraceae_logit  \\\n",
       "0           -0.747070        2.486328       -2.169922         1.900391   \n",
       "1           -1.020508       -0.872070       -1.964844        -1.522461   \n",
       "2            0.164185        0.775391       -1.668945        -0.221802   \n",
       "3           -0.708984        0.709961       -1.564453        -0.077454   \n",
       "4           -0.708984       -0.143677       -0.320557        -1.643555   \n",
       "...               ...             ...             ...              ...   \n",
       "2792        -1.117188        0.658691       -0.553223        -0.359619   \n",
       "2793        -0.649414        1.941406       -1.231445        -0.438477   \n",
       "2794        -0.448486        0.985352       -1.852539         0.742188   \n",
       "2795        -1.236328        3.931641       -1.172852        -2.310547   \n",
       "2796        -0.715332        4.031250       -1.281250        -1.117188   \n",
       "\n",
       "      Malvaceae_logit  Melastomataceae_logit  Myrtaceae_logit  \\\n",
       "0           -2.886719              -4.980469        -2.761719   \n",
       "1           -0.915039              -5.437500        -3.214844   \n",
       "2           -1.568359              -3.824219        -2.728516   \n",
       "3           -1.122070              -3.429688        -2.224609   \n",
       "4           -1.702148              -4.644531        -3.546875   \n",
       "...               ...                    ...              ...   \n",
       "2792        -0.608398              -4.261719        -3.195312   \n",
       "2793        -1.717773              -4.644531        -3.406250   \n",
       "2794        -2.884766              -3.646484        -1.609375   \n",
       "2795        -3.220703              -5.195312        -2.341797   \n",
       "2796        -3.248047              -5.554688        -1.846680   \n",
       "\n",
       "      Passifloraceae_logit  Phyllanthaceae_logit  Rosaceae_logit  \\\n",
       "0                -3.220703             -2.039062       -2.855469   \n",
       "1                -2.896484             -1.683594        0.409424   \n",
       "2                -2.455078             -2.048828        0.622559   \n",
       "3                -2.107422             -1.483398       -0.923340   \n",
       "4                -4.359375             -3.392578        1.975586   \n",
       "...                    ...                   ...             ...   \n",
       "2792             -2.927734             -2.158203       -0.062805   \n",
       "2793             -3.111328             -2.568359       -0.196289   \n",
       "2794             -2.974609             -2.533203       -1.801758   \n",
       "2795             -3.683594             -3.480469       -0.072754   \n",
       "2796             -3.681641             -2.121094       -1.324219   \n",
       "\n",
       "      Rubiaceae_logit  Salicaceae_logit  Sapindaceae_logit  \n",
       "0           -1.358398         -2.119141          -1.598633  \n",
       "1           -1.733398          1.350586          -0.226807  \n",
       "2           -1.046875          0.237793          -0.510742  \n",
       "3           -1.385742          0.063416          -0.836426  \n",
       "4            0.340088         -1.000000           2.621094  \n",
       "...               ...               ...                ...  \n",
       "2792        -0.152954         -1.143555           0.043213  \n",
       "2793        -0.147827         -1.016602          -0.682617  \n",
       "2794        -1.070312         -1.525391          -0.565918  \n",
       "2795         0.024765         -1.898438          -1.231445  \n",
       "2796        -0.002041         -2.394531          -2.003906  \n",
       "\n",
       "[2797 rows x 22 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/feature_extractor-PNAS-19_classes-res_512-bsz_32-resnet50-pretrained_True-pool_avg/replicate_1/results/task_1/test_predictions.csv'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df\n",
    "test_predictions_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastprogress                   1.0.0\n"
     ]
    }
   ],
   "source": [
    "from lightning_hydra_classifiers.utils.report_utils.pandas_embed_images import df_embed_paths2imgs\n",
    "\n",
    "\n",
    "\n",
    "df_embed_paths2imgs(df: pd.DataFrame,\n",
    "                        file_path: str, \n",
    "                        path_col: str=\"path\",\n",
    "                        display: bool=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2797"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.102e+00,  9.468e-01, -2.197e+00, ..., -1.358e+00, -2.119e+00,\n",
       "        -1.599e+00],\n",
       "       [-1.948e+00, -2.080e-01, -1.406e+00, ..., -1.733e+00,  1.351e+00,\n",
       "        -2.268e-01],\n",
       "       [-1.899e+00,  3.206e-01, -1.015e+00, ..., -1.047e+00,  2.378e-01,\n",
       "        -5.107e-01],\n",
       "       ...,\n",
       "       [-1.159e+00,  9.106e-01, -1.307e+00, ..., -1.070e+00, -1.525e+00,\n",
       "        -5.659e-01],\n",
       "       [-1.577e+00, -1.782e+00, -1.040e+00, ...,  2.477e-02, -1.898e+00,\n",
       "        -1.231e+00],\n",
       "       [-9.277e-01, -4.192e-01, -9.902e-01, ..., -2.041e-03, -2.395e+00,\n",
       "        -2.004e+00]], dtype=float16)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)\n",
    "\n",
    "y_logit#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdaec2ebbe0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEMCAYAAABtKgnyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9Z2BU9bo9vKZPMpNJLySkkEIJRHqXLmoQ9IDKwYsiR5BX4Oi5KEhT32M7qIgUC4gEryDKNR6IEgkqgjQLR0JRSiAJkBDS20wymT7/DzFjQghkr9kQvJn1RZnZv8ye2Xv/nrae9UicTqcTHnjggQceeNAGkLb1CXjggQceeNB+4TFCHnjggQcetBk8RsgDDzzwwIM2g8cIeeCBBx540GbwGCEPPPDAAw/aDB4j5IEHHnjgQZvhljVCFy5cwIwZM9C7d28MGjQIL7/8Murq6tr6tDzwwAMPPBAR8rY+gatBr9dj2rRpCA8Px+rVq1FRUYFly5ahoqICK1eubOvT88ADDzzwQCTckkZo69at0Ov1SEtLQ0BAAABAJpNh/vz5mDNnDhISEtr4DD3wwAMPPBADt2Q6bv/+/Rg0aJDLAAHAXXfdBaVSif3797fhmXnggQceeCAmbkkjlJOTg/j4+CavKZVKREVFITc3t43OygMPPPDAA7FxSxohvV4PnU7X7HWdTofq6uo2OCMPPPDAAw9uBG7JmtDNwMfhDwteM8ivlPqsrPKA6x/UAmwSCbXOx2mj1u3yklHr/stZS63z8TFT6/JLmzsprUGYL3eeWXo/at1/1NQydLTz/mGJlNMk7mDn7rUx/iXUutJyLbWuzs7do+EBBmodAHQ+vYteCwDWstZncBRBsW591p8Nt6QR0ul00Ov1zV7X6/WIjRXnAoXYhW/SUhn3cJfIuYcGAI4orNS6kWYFta4UJmqd2crdSgkPcZv7T+86qHUd5c3vq9bAQjoDR+wV1DovKe+4sHdbJblwX0UIta6zg7vXyqRKap21wpdaBwCd6ZW/w2F39y/8n8UtaYTi4uKQk5PT5DWLxYK8vDxMmjRJlM9QQvhNofTmooux3peRdSGYWnt3J87L/CU7jFpX7OB6sXy8uE163xpuQ7nNm4toTldwm7vGyW0icTIuYrsEzvkAgBoyCg6QcNdizkPcPZOdyjl1OhO3bQ15knPMRIGTc5raA25JIzR8+HCsXbsWlZWV8Pf3BwB8++23sFgsGDFihCifYST8xdpqFf15Whm3qRw9F0qtC5Fxaa77HKwHXkmtSuxQTq0rKOK82kCJhVpX5eQ2sAA3HrFIG2fYy2ScMYnifhrU/sLVaR0O7hqGqI3UuvxNnHEGgK7P0Uvr4fAYoZZwSxqhKVOm4OOPP8acOXMwZ84clJeX47XXXsO4ceOaseZY5CmFf/W82kDc4cPVhUocnAFL9+IenIl13EZEOpl0Oi6iJ5eS+bmYdwgGRxQJXtMRwJ7CDoLXxVqBowruGnZxkBeDTKsFEilqANh9tiO1LszOOWZWcMa5nLxHAaArvbIeTk8k1CJuSSOk0+nw0Ucf4ZVXXsGTTz4JlUqFe+65BwsWLBDtM7REdmVU9GXqs76/GE7zEI1OLqKpIfPmuVJuIxpk4XY+qRcXYRil3EZ0b598al3qsUjqGv4o41JVI61esHBfEbUSbsOrlHHXsJo0eh3sXDquTMbdM4q2HCJNGvj2gFvSCAFAp06dkJKScsP+PpPpqKnkvO9gN27Au+BNrYuWcjWTbKeGWqeUc5tt+U9crSXaxuWO9HmccQ6xcht7KJkaM7vRPBFKMuuspNEbqeRSseB+GujM3LW3kKw6UeAhJrSIW9YI3Wgwj6nam0sfKCv5G7BUzl2iUDP3hOvYiM3KeaeBcm5zV5DePstwdJDsOH8n94P6uLFnFZFPdZiNc5Yu1fhQ6zqQ5JJKK+cMasgoXxR40nEtot0aoRDigTMZuY221o2fmXWI2XUFMu5h6U96eiofbmMwOTmv1kamDVXkJpIn4X4Xq4Jncvk6OYOZr+Du0+4Ojihgc6MXikGQH2f0RIGHmNAi2q0RMkiFb0ZGIxddhKuM9AP3lZTbjLqRZqgHWYhwkMViTT+OjWc9xX1ebQ13Ddl6iQzc5hPH5sYA5JP2q4eVi/QvSb2odR1tXArXSd5rP+iDqHWA+31CHmJCyxDNCGVkZGDHjh04efIkqqurERkZiYceeghTpkyBVPrHhrhv3z6sWrUK2dnZCA0NxaOPPopHHnmk2d9LSUnBli1bUFZWhvj4eCxYsACDBw8W63ThS3juOh3H5AKAYrI7fDTZdKoEybclqVU2B2f0Ln3BedEhCi6tplJzkZe3mfs8lqKdpXSiE2mI7ODOtULKnWv/UK6XTa3jjN6pLK459jYpr5jgNjyRUIsQzQh9+OGHCA8Px7PPPovAwED8/PPPePXVV5Gfn4+FCxcCAI4ePYo5c+bgvvvuw8KFC5GZmYl//etfkMvleOihh1x/KyUlBStXrsS8efOQmJiI1NRUzJo1C6mpqeja1V2yZD00EuGbUVGZD3ReHFstNLCGWneimvMyjSQlPJvc3LUmzlhG6rjfM6+aawLVV3KRUACs+J5k8mnJ9NjoUOFUcgB4t4JrjP5VxTkSiiLu8/wKOSP0q4q7hvHmNiQmkHT09gCJ0ykOb7GioqLJ6AUAWLZsGT799FP88ssvUCqVmDlzJqqrq5Gamuo65vnnn8fevXuxf/9+SKVSWCwWDBkyBJMnT8azzz4LALDb7ZgwYQISEhKwevVqMU4XH3QUrh3Xj/SkLlk4hhsADBvBbUSHvucUE3724jai0XVc5MVKsPQN4bzvklKuiC6VcI/JSXBsQ5Mb5RIzn8mj8EhfjvZ+9AfuHtWQjd9hIXwkFHPsW3otAJhP7231sapuo9z6rD8bRIuErjRAANCtWzeYzWZUVVXBz88PP/30E5555pkmx4wfPx6fffYZTp48iaSkJGRmZsJgMOCee+5xHSOTyZCcnIyNGzfC6XRCQjKVGiPYJjw8tsm4naGLP0lhBfC/hyKodfGk7Es3C/cdWfJFvJLbGM4VBVLrJGSqqlDORUEX5NznJVv5Ufb7FJzTE04665ePk+w4LZcdMNRxUX5uoT+1DgBi6JW/w5OOaxE3lJhw5MgR+Pn5ITAwEOfPn4fVakVcXFyTYxqmpObm5iIpKcmlGXflcfHx8TAajSguLkZYGOdBNQajTq1Rc97+5QruIQWAO8OKqXX5xZww6EUFZ+DvkHKbpoGkknfUcsbLZOFueRXZJ1Qi4zbMY1I+eiZFtOk+ofAkMkNwnEupyqTctYgJrKLWiQIPMaFF3DAj9Ouvv2Lbtm2YO3cuZDKZaw7QlXOCGv7d8L5er4dSqYRa3VQD39e3XmeqqqpKFCOUrRT+xHUkPTBfFUsSAF6u5vS1kkkxyv4mzh0+ruCIFyODOBmko+Uc0ynAwX2/y2TTaZaUq3nNsPGNQv+RcAask4071y9+iaTW9VdxRkFOthF8U8URGgDgCXrl7/BEQi3ihhih0tJSPPXUU0hKSsLjjz9+Iz7CbQwzC2e6JfTnxDYB4PhhToh0KklHBTgW2F5yntBkcCKWpRVczSTIwRn2joHcKIdaUn07xskZr4NyPuXMrrwg55wslkBRVMRFQnaS6DGxK1e7EgNO0vlpDxDdCBkMBjz++ONQq9VYu3YtFL833TVEMlfOCWr4d8P7Op0OFosFZrMZKtUfD0VDpOTnx6WZrsQ5mfBpY0Hn+BSJiezbeU3ORQqPSoSLbQJAVzJo+1bORWx/CeC+34FKjpElreA2PjbF9Zn5PLcQwP+n4GZnXZBxUdQ4FedI7C7hMhNdyXlCLEM14ywXsQHAdHrl7/BEQi1CVCNkNpsxe/ZslJeXY+vWra4xDAAQFRUFhUKB3NxcDB8+3PV6dnY2ALiG1TXUgnJycpCYmOg6LicnBxqNBqGhXERxJVQEKTBTH4AeKs6TZn/osXLuAQ8ykx3+Cs5YDrJwG0rI7ZxXG76d7PchhwSekHBRwuOIRaaM3GxJh0Ap437Tn41ctNeJ1PELC+ZqScfLOFJKDCkGLAo8NaEWIZoRstls+Mc//oGsrCxs3rwZERFNWV1KpRKDBg1CRkYGpk+f7no9PT0dwcHB6N69OwCgT58+8PHxwc6dO11GyG63IyMjA8OGDROFGceit7YSF4j+FBmckBGsLAeAaYEcMeF0QRBqCFUImwTwIxxpq1MCg0T47VR7yoSii8J/02BVHXx9hZMhbDYpjlYJ38QinQ54E4Pttns54U00ACc4FLhI9GxpnRLIyXSVwlnfJCsUt0nNiBstnOlWdkQOm0240yNFfd+WUIQFGKD1F26IWOHiJvAImLYI0YzQSy+9hL1792LBggUwmUw4duyY6734+HhotVrMnTsXDz/8MJ577jlMmDABmZmZSE1NxQsvvOBSVVAqlZg9ezZWrlyJgIAAV7NqXl4eVqxYIdbpgunJVCjtSAgWTrfOKuWpodoO5MyVyxJK76xEKkEJEQzF253wIzaGn8+GC/8wAEnBZdS6/ApfBBH1MtaP9SUfsUqyL6lS4kQ1qVdXKJdD5xBuwBLudwCE2nudkVWh4J4J31AuImXXNYEnEmoRohmhgwcPAgCWL1/e7L1NmzZh4MCB6N27N9577z289dZbSEtLQ0hICBYvXtxELQEAZsyYAQDYvHkzysrKkJCQgPXr14umlgAAciIdp9JwN7+9jI/ePv6Vy2P3A5kecwivlQFAsI6T30lM4GpCu45yv0sfXQW17jsTl6oqAkddjwd3HQDAlxR31ZJ1r8qD3LUHODUQLxmXis06w9URAcDtIoCnJtQiRFNM+LPhVNw91z/oCvgEcBv7sXyeGnqA3Iv6kwNpjqu42+FOktrdexLXsHjg3xwRItaXq+kdqeGM0KdyrlF5oIQn4LAPNCuaqiU32GgVKWVl48glAzScAwIAXc/upNcCgOnQllYfqx461a3P+rOh3apo5xEzULr5coVNhgTRgAhyxLPGwXmLgeTnqaRcCsheyRl2P7KZk70U7BX0JwkNKppoDdSQqTwmOwAA/mR6zM+fixK9i7nmb0OtCLUdFp5IqEW0WyPEoI4cAxDlq0eWnvNs/xLE9WAcu8xFXydI5YO+Di4FdOEHbkOJ6MDRictLub6kcDvHAJMTTdEAUIabX8g+q+Si51HxnI5fwTlx2i1aiy7j2m6ekJMgtbQX3DAjVFtbi+TkZBQXF+Pzzz9HUlKS6720tDSsW7cOBQUFiIqKwty5czFu3Lgm661WK9asWYPt27fDYDAgKSkJS5cuRbdu3UQ5P4atZrXyKrzVpO7cT4VcNlpF+u5msoBqcnLaaj2HcxvDb99yZA8JGSXoFJwROm7mCBQA0F3JORJxZK1FQUZfl85yxqSWnMarIzd0w2/8ZFUuAdgInkioRdwwI/TOO+/Abm9+s+zatQsLFy7ErFmzMHToUOzevRtPP/00NBoNRowY4Tpu2bJlSEtLw6JFixAREYENGzZg+vTp+PLLL0XpFYoO4DzpDndxD07ep9yDc9CL2xi6WDmj10nCbWAaCbdJV5zgzjOHaDZuwF2duc75rTnCyRCPSrXIJcdK32YjZxHJuXstgZw6e87JSTZFSrhUbC45ATamistkiAIPO65F3BAjdPbsWWzduhWLFi3CCy+80OS91atX4+6773apaQ8aNAi5ubl4++23XUaouLgYW7duxdKlSzF58mQAQM+ePTFmzBh89NFHrhEP7mB3jXDtsdtl1TifJvyzqkxq+BDziwCgi5XLYyfYuQe8Rspv7gzspLGMInXO/NRmFOYKJzVUmNRIIgYFHlYr0cEp/DHzt7PCS0AXGzvCnIsSw0kmJiu/M1TOOZA1NZ6a0K2IG2KEXnrpJUydOhUxMTFNXs/Pz0dubi7mzZvX5PXx48dj8eLFrplEBw8ehN1ub5Ki02q1GDVqFPbv3y+KEWJuCYWC8zBP2/ibf9ond1Drlk3j5p98XHOSWveSsju1LoMcmb7gTq6Jd/seTs6on4YT29xl4RhZNfab393vQ0aXO98eQ637z+wj1Lqv7Rwzcu6yaGqdKLDzqUAG27Ztw+LFi5u9PnXq1GaBwZVoTblETIhuhNLS0nDx4kW8//77+O2335q8l5ubC+DqYxoa3g8ICEBOTg6CgoKayP40HJeeng6Hw9FkZDiDoRLhkiHht3MpJ+3X1DIAwMWZn1Hr/B0cpbiXhuu/0ZIyQV3J+UVnvuM2oh5SjhasUHIOSBcHV7sql/FGSEHqFPYEl1Yzbsyg1qmknPyODxlUmHf8wC0E4CV8BmZTtFE6bsOGDfDx+YP8ExR07QxQa8slYkJUI2QwGLB8+XIsXLgQGk1zFlJL4xwaxEsbj3No/MM1Ps5qtcJoNEKr5R6YBhSahdc+7N9y6YNIB7+hnCJZdb1JnawSL+53jVVzOmDhXbi+nexfuQ2szMp5+1l67nfRKzijN9jJl8IryXk7LA+96jz3m+o0XBov0cAZWVtVG6bE2igd171796sOHG0JrSmXiA1RjdCqVasQHR2Ne++9V8w/e0PAfPEaks0DAAqSldWLlKc5UcrN2ykm6h4AcNHEbdJnT3CbbUcJRyU/oeZu+b5kM65Oyd0zZyS84xIE7jNzyM8MHszd2z99yV17nYy7R73HcyljUfAnqAm1tlwiNkQzQufOncPWrVuxceNG13gGo9Ho+m9NTU2TcQ7BwX9IaDREQI3HORgMzT3r6upqKBQKeHvzIxUaUCIT/tXD7fyNVEpuDP9j4NJO95Ju7b0mrkajcXI57zH/5GjIX73IGefOZNrwHGlMjpgvUusA4G4Vlxr1c3CRQmc7V7v84UtuY4rw5uj5RjN3Lba8zkXdAPD4k/TSerRROm7ChAmoqKhAhw4dMGnSJDzxxBOQy6++97W2XCI2RDNCFy9ehM1mw7Rp05q9N23aNHTt2hXvvPMOgPov0/iLNoz0bjzOoby8HFVVVU3mB+Xk5CAmJsbtehAABBCFQhOktOyLpoZLWYy2cJTpAHBe7QWSHRdIenrFa09R68KdnHFmt4JAK3CeKNw/JY3GL+T4iO4WLv2bzajzAlCT6bguYdywR7mSuxrWIi7q7gsuZSwKBOw327Ztw/bt21t9/MSJEzFp0qQmrwUHB+PJJ5/EbbfdBplMhv379+O9997DpUuX8Nprr13177S2XCI2RDNCffr0waZNm5q8dvr0aSxbtgwvvvgiunfvjsjISMTGxmLnzp0YO3as67j09HQkJSW5rOztt98OqVSKjIwMl7hpbW0t9uzZg/vvv1+U82WaOQPVdTCZhf9kVSaeHTduHMkC+4abQ/SjgjNef63jHIOjxZyoZL+O3O/yZTHHjutksSOQGLm9QVVDWb4wqRf2KznjdcHObbYXlZyaxH1PcMLCOW9coNaVkI7Z7Y+14XRTAU5aQUEBDh8+3OrjBwwY0Oy1YcOGYdiwYa5/Dx06FD4+Pnj77bcxZ84cREVFtfrv32iIZoQCAgIwcODAq77XvXt3l2LCU089hXnz5iEqKgpDhgzBd999h0OHDuH99993HR8aGoopU6bgzTffhFwuR3h4ODZu3AgAePTRR0U5X51ceF45IJhLHziKeR2wwkNceizCRup5kY2AfkpOSVll5bzhqjIuJdvfzp1nvpTc+MBFbOEWXm/QR8mRWfxJ9W3jlyeodQoFZ/TMEs7hMRzgBUzdo0FBUDouIiLiqoblWse3BsnJyXj77bdx8uTJqxqh1pZLxMZN145LTk6GyWTCunXrkJKSgqioKKxYsaIZ82Lx4sXw9vbGqlWrXLI9H374oWiTVY9IhT8AQ8u5ukeujb+Fx83ltNX2LedYWRVkGo/1To3khpIQx42A2HeqI7VOQTZy/iblGGA5Sl4iKtvOpYw7yDijoJk2klp3fj5nvIxSzqlTBf452HGTHpjULL12M9BQDrleuURs3FAjNHDgQGRlZTV7feLEiZg4ceI11yoUCsyfPx/z58+/IefWzyl8k2Z1x2LlnEEAAOOuS9Q6Pzu52cq5B1wn5SIvBanarS/kalfl5PdTE8PeAEBG6rF1ILX4AKBQwq9l4Ph9kxIKrTfn8EhruChY0cltBTgetwA77quvvoJEIkGPHj2u+n5ryyVio92qaB8hAuzBDq4wJ5c5cNLCPQDdZvSn1uU+c4Zad56sJ5yXkH07ZLmsX18uKnV8z30em94sknFUcgMp8wQAp02c8no3NVdHlN42mFrnpeG6uEtNnGGXkKlmUXCTx7bNmDEDAwcOROfOnSGRSHDgwAF88skneOCBBxAZWc+6XLJkCdLS0nDq1B/koNaUS8RGuzVCXsQ94e3N9ScAgK6O84QKXjhArRts4oye3ovr8O8u5VJARgvntZ/fz6WOBhNKGQBQDa42N9iN4XTepLZab68u1DoN6aybNqdT6/KKOFLKIDKzUPoNP8pB23xgtDDYbq5sT2xsLP7973+juLgYNpsNMTExmD9/fpOausPhaCYy3dpyiZhot5NVP+jI6XD0JTexI06utvNfs7mayYfruB1ll4SbBPqAnTNeWgd3+w3ryqUpj57i2HHROr7H5C0rZzBrnCS5RMIZTBvZW7byr5yk0YlNnJHNIkkiDzzIU4x9Vu2g1wJA3cdLW32s18OvuvVZfzbcEO24TZs2ITs7G15eXkhMTMSKFStc+cR9+/Zh1apVyM7ORmhoKB599FE88sgjzf5OSkoKtmzZgrKyMsTHx2PBggUYPJgL+68GL2KP7qWugoOoDWRafRFCNLpWyqRwVpGeu9QbnYnAzaFwwofYxOwSoJtdeOrpO6WaOk+FH6A/L/w8C+VyyvDVGFWoswt/XD5TS8FkHA2wQUvUdixwwEQOxLM6nfCRCP+O0pgIQC88OpFJSDVsKdDVItxAOw0mSH2FGzBHNZdSbfpH2r4mdKtCVCO0du1arF+/HrNmzcLChQthMBjw888/w2qtv2GOHj2KOXPm4L777sPChQuRmZmJf/3rX5DL5a5+IKDeAK1cuRLz5s1DYmIiUlNTMWvWLKSmpqJrV64n4Up0kQgPzWtJNQEFgBqiwVbhBN78jCvCRtskqCGCqGESrvjoa3fiMoSTBf57Ihdh/OdTjiUZQm7QdokESmKE+WELN3V0jLJ1tNurgU3jdbJy6755kWtWjVFxKaoIswMGiXD24PGvWIqxEsOuf9C10T4TTq2CaOm43NxcTJgwAe+88w5GjRp11WNmzpyJ6upqpKamul57/vnnsXfvXuzfvx9SqRQWiwVDhgzB5MmTXSMb7HY7JkyYgISEBKxevVqM08X3oQ8KXuOn5tg8TnJTAIAzJL07ihRNLZRwTIEQJ1cvi4/l5HcKLnK1FoONq0GVS7l1/1FxHnAPN6b4FpGuZaSV2wr6+3HXkGTno7iceyZ81HxNN+m8m+m4D1s/fsbrb2+49Vl/NogWCW3btg3h4eEtGiCLxYKffvrJpc7agPHjx+Ozzz7DyZMnkZSUhMzMTBgMBtxzzz2uY2QyGZKTk7Fx40Y4nU5IJPym3oBa4quHe3OpMYedlxkqryMp03Zu07STP62JbHRkNyIrqY/mI+fqLJdIYkKknftdeiv5+sUBG+fxq8j6XHY5Vw9MCObqj/lExA0A/kaeup5Er/wdnnRcixDNCB0/fhxdunTBe++9hy1btqCqqgrdunXDs88+iwEDBiAvLw9Wq7WZOF5CQgKA+kgqKSnJ1Rh1NRE9o9GI4uJihIVxVNLGMBOGbJ+BU6buauMaFgFARzrElxXcwkEKbvM7Q7LxFCQlS0r2bP1ANmSyW0geOdrb38x3p+eSs48KZJwHMq9LIbWuIo8jGAQ7OEdCJWk7Q+C0c9ekPUA0I1RaWorffvsNZ86cwdKlS6HVarFx40bMnDkTO3fubFEcr+HfjWcJKZVKqNVNvZ0GyYiqqipRjJCaULXtJyUpnkrAQo5cHhHK9XxkFXAG85CE2/xiHNxme+IYV9vpHMOlgCoucBENO569SM15378q7ehIRlEKskE2kozYpaSovW8YV/AvruTScfFduXtGFHgioRYhmhFyOp0wGo345JNP0K1bNwBA//79MWbMGKSkpGD8+PFifZQoKG5Bzvxa0Fq4DQwAzE7uAe/Wn9uI8ku4S1tJpsdkZCMg+2h2I5WiLyi584whywnl4EUz5WRkku3knKVacry3egRHFqpKOU+t05NqEqoO7qfxabTRKIc/A0QzQjqdDn5+fi4DBABeXl7o2bMnzp0710QcrzEa/t14lpDFYoHZbIZK9UeRvCFSajzawR0EEeFxNeSIVnONcqxiwqMZnOGbaicHjRHCrgAwkGRWDXqaS49teodjj0kBDFcStQglkGEVXvvo5pBjm/2y8M8D0F8STq3zlnGRgoIk0Kx9izN68RYuWmd5Pgf28hmUCfTK30HW29oDRDNC8fHxyMvLu+p7ZrMZUVFRUCgUyM3NxfDhw13vZWdnA2g6SwioF81LTEx0HZeTkwONRiOagKmMIAWGKetgI1IWtVYFOpLCoLQKM7hURz8bl6fXyDh1avNhzhuOtXAbihJ2lFiE54/Oy1VgyOt75EbEyoU7TqNt3vS4bdbnDiab+vurqqh1NpKVYrZy21abmgFPOq5FiGaERo0ahW3btuHkyZPo3r1+jK7RaMSxY8dw1113QalUYtCgQcjIyMD06dNd69LT0xEcHOxa06dPH/j4+GDnzp0uI2S325GRkYFhw4aJwowDABPxAHipudSKlcztA4COvHftpLsYbOM+sIZMkRSd4AoKWlIw1V/LGecKA/f9ohVcisvHDc/ZjxRbDSWlZRwK8vMiyblHuRwbz+FGq4Tb8BATWoRoRuiOO+7Abbfd5hLA02g02LhxI0wmE/72t78BAObOnYuHH34Yzz33HCZMmIDMzEykpqbihRdecE1LVSqVmD17NlauXImAgABXs2peXh5WrFgh1umis7fwJkmtHxfNVNZw0QUARFq5jaGUlG4pUXDeqYqcf2MiRzV7kwPf9LWcUWChIWuBTKNxA87LuHtGS9RJASDei7sWMrKH6rSEc1xGBnODEI6NpJMAACAASURBVEWBJxJqEaIZIalUivfffx9vvPEGXnzxRZjNZvTs2RObNm1CdHQ0AKB3795477338NZbbyEtLQ0hISFYvHhxE7UEoF4BFgA2b96MsrIyJCQkYP369aKpJQBAtlF4jSaC3DD1Dr4/4bSKu0R9LZzBPCjjorYhXlxdoNPd3Aa2J41TdohUcOepJ1NHx8DVEH2k/AyqMCd3z5SSu0FlNWcUjEbOUepk4+5tQ/XNdUCawFMTahHtVsD08w5TBa/p6sU3EOYQRg8Ahg0soNbtOczNE/pCyaWrHjdzxotlDXaO4KRiLl7mUjlW8jw/Z+TaAWjAp3CrSEZeKNmQu/Au7lpkpXPG66KDW3fXfdx5AoDPul30WgAwLn+s1cd6L9jo1mf92dBuRzlICdvrF8wV3wHA9wKXknv5OKf6fJeDy0HfZeUecJWU+216JXNF7Z/SOWZVQhi3EX1dwRFibCQhpRo29LVzEkolhE4hAPQ1cSmjfV9wUWmPUK5v52IJd49+n8Y5IAAwYR29tB6eSKhFtFsjFCYRvjnk5fkjOoaTGmFkggCgAymHA3B1gTNKbiPqTho9exXntdeR6bHyco4S3g81+J5IkXVxqJDu4BqOQ+Uh1LpSMhIKUHJG724fbtR6XQ0XeWnJe61HDCcmKwacnppQixDVCO3evRvr1q1DTk4OvL290adPHzzzzDOIiYlpclxaWhrWrVuHgoICREVFYe7cuRg3blyTY6xWK9asWYPt27fDYDAgKSkJS5cubdKH5A6KnMIfuB7+FaitEv7gVOu9ECLnpHu+IwVFe5OE1F5mjkFU7eA2FHkoFyH6kxstK/dzyu6DMGIfOaA0o4tUuAc+1KoC2+caKOPumW7EeAQA+E8VF5X20VVQ66wkQ/bHPC6rAADC5Y6vgIcd1yJEM0I//vgj/v73v+Pee+/FvHnzoNfr8c477+Bvf/sbduzYAa223ovctWsXFi5ciFmzZmHo0KHYvXs3nn76aWg0mibT+5YtW4a0tDQsWrQIERER2LBhA6ZPn44vv/xSlF4hhgVmNnEEA4WcvwEjHdwlMkm4z5STJUIDMYcGAPRHuXSVihxsxtJ0FeTvwtZZfIn5Uw2olHFRogncuhA7F3WrvTmjp67kfhtVW5a/Pem4FiGaEUpPT0d4eDhef/11Vy9PREQEHnzwQRw5csRlYFavXo27777bpaY9aNAg5Obm4u2333YdU1xcjK1bt2Lp0qWYPHkyAKBnz54YM2YMPvroI9eIB3fQwSr8RpaQUjG1Zs4zBYAxao4MUUeOHvhGwW3uk724NGXpZY4FZnZwacoaO2cUgsmwxEwSDI6oeY62N7nfnVFx5zpzDJeOO7qTq9HEhXD3mkzRhikxTzquRYhmhGw2GzQaTZNmUh+fpiOt8/PzkZubi3nz5jV5ffz48Vi8eDEqKioQEBCAgwcPwm63N0nRabVajBo1Cvv37xfFCDEh/a8mTr0glBzTDADnLaQsP+n1JZm5dSfN3IZyxz3cBnY6g7t1f1BwDkGSiYssj9i5lFMfGVfsB4Bosjm6QMZd+4pMLrpk53NVVXGOkjfZzyQKPJFQixDNCE2cOBHp6enYvHkz7rvvPuj1erz++uuIi4tzjeXOzc0FcPUxDQ3vBwQEICcnB0FBQfD39292XHp6OhwOh6u5lUU5IQ451p8vbP5cHkytCyJTHazflc7MPQcwxcQ9ZFm7uEiojGSO3SnjuvRPy3yuf9BVMETG1UtUbuxZRaQxsZD1MquFM3omUn6HTalGxXFMTFHgETBtEaIZoUGDBuHtt9/G/Pnz8corrwAAOnfujA8//BBKZX0KpKVxDg3ipY3HOVwZRTUcZ7VaYTQaXTUmFibChhlreRXtDnZOGPSoittsB1q5fp9hFrahj/t+HTtxG4MlO5Bap9Fw52nhbBcqSJYiJMBQC1lnk3KbtIaU+yk2cc9i0giOLp/5PefQfX+M650DgAfolb/DEwm1CNGMUGZmJhYuXIgHHngAo0ePRlVVFd577z3Mnj0bn3zySbP5QG2NeLPw9MpFsw/CVVw/DJt66Eemx2rJrnmrnNuIZCQbr7qETK2Qsj0XK7n0ppqMLoZa5DhN+i4mkgWWJ+e87gByWm1yEpdSLTnBXXt2OF3fkDakaNtuLjsuIyMDO3bswMmTJ1FdXY3IyEg89NBDmDJlyjWzSI888ggOHz7c7PXPP/8cSUluz5e9KkQzQq+88goGDhyIJUuWuF7r1asXRo4ciS+++AJ//etfm4xzCA7+w5tpiIAaj3MwGJq7ntXV1VAoFPD2JqdoNUIRMXm0v7waCoXwm6nC4I0ygiUVBAsqSHZVnEaPWpPwtVlSBY7LhBvMARrOOEukTjiImeKXLN6wEsPbfGBHdIBwskdNZQB8nMKv/Q4vIIhwCHycEhQRT6faCcRbOWNSKwVMhN2T+clQcUL4yRZXalFEtCAM61yAqiLhe0BZmRYatfBIuNakRCfBq67ATY6EPvzwQ4SHh+PZZ59FYGAgfv75Z7z66qvIz8/HwoULr7m2T58+zY65soQiJkQzQjk5ORg9enST18LCwuDv7+8a8dAwriE3N7fJl2oY6d14nEN5eTmqqqqazA/KyclBTEyM2/UgAAizCt9Q7FIp7Gbhn61RWaAh01VlZs4I1RAGCAAUTqCfTfjGUEJOMPf34haGSLnIssyuwrkK4SQKNRyU0TPChjyCLj/UxmcOGOMFAAmkCK3hrAQKtfDvGNupArHE51UWck6ovy/nKClVZEq1MW5yTWjdunUICPiD3DJo0CAYjUZs2bIF8+bNc5VIrgadTodevXrdjNMEIKIRCg8Px8mTJ5u8VlBQgMrKSkRE1A8gi4yMRGxsLHbu3ImxY8e6jktPT0dSUpLrR7v99tshlUqRkZHhEjetra3Fnj17cP/994t1yoIhk5JjDkhDAgDhMq62w6KCFGjwJinMBtJYysgiuo1McfmTtZ0AcFT5S3Lec/YmC/clZCpW7cf9NvlnuAGVfn7cM+EXyzkuouAmR0KNDVADunXrBrPZjKqqKoSEcGocNwKiGaGpU6fi5Zdfxssvv4wxY8agqqoKa9euRWBgIJKTk13HNYx6iIqKwpAhQ/Ddd9/h0KFDeP/9913HhIaGYsqUKXjzzTchl8sRHh6OjRvrRf0effRRUc7XSERTLCuHne0DAHoiKgEAPxkXebFnKpNyD1mAN7ehlOg5+Z1gQq4JAOrIpmEF+Ytq3bhnvEmnmxyOC0sN57mUWcgBilbu3q7M4aNLnjBfD+ctQEw4cuQI/Pz8EBh4bVLP4cOH0bt3b9hsNvTo0QNPPfWUi+F8IyCqEVIoFPjkk0+wbds2aDQa9OzZE6tWrWpCtU5OTobJZMK6deuQkpKCqKgorFixoolaAgAsXrwY3t7eWLVqlUu258MPPxRtsqqS6KNhpqoCgIXWf+ONCWswz0q49Nhd5DNWXcNtDOwzXUUO39OSChSF4NbZyGZjANCRunoslFruO0pIMouEjIKDB7YhTVoAMWHbtm3Yvn17q4+fOHEiJk2adM1jfv31V2zbtg1z586F7BrjWvr37497770XMTExKCsrw0cffYTHHnsMGzduvGGGqN2OckglRjkkKEmeLoAKE7fZ/qjmNqMxJEX7kJzLt99u4/Ltsd05mu5/TnA6YL4SLnXEsg23uNEgOdjORQokkQ/+dm5hiJNzlNjxGB39hQ+kBIBycu4RAAwp/De9FgAMc5Kvf9Dv+J9u4/DOO++0+vi///3vePLJJ1t8v7S0FJMnT0ZoaCg2b94MhaL1e4rFYsG9996LwMBAbNmypdXrhKDdqmh7EzIaPj58Ttlq46KhIFJmxkrSbcNEqMEKgVnP/S5+5HjvID9uqN35aq7ptLMbUXCQjTMKF8lx22HkaHellIuEIoO5HrGsIq5HLFR1c+urTSAgdI+IiMCAAQMEHd8SDAYDHn/8cajVaqxdu1aQAQLqJ12PGTPmhhkgoB0boXK58M3hUGUwujg5j7+OlFIJJXsi2IRMIXlHBLIsvgtc46GC/F2KKn1QSZAFfOBAmFR4qjLBCmxUctdeJ+fqgeQcPZSTU3UT/TnDfpo0Jn26cKMx9KVt16soJOE0adKk66bXWgOz2YzZs2ejvLwcW7dubaZAc6ug3Rohpgg7Qsc15RVW+NBsrlI5Z050ZK9ICNlTpySFgqICOYHWsiqOmBDga0QYse5oVSAuOIWnx06qAKaKGc+yBACUk8FXOKncXVrBXYteccXUujNZnOOikd/kML8xbjIxwWaz4R//+AeysrKwefPma0ZL14LFYsHu3btvWKMq0EojdPHiRaSkpOD48eM4d+4cYmNjkZ6e3uy4ffv2YdWqVcjOzkZoaCgeffRRPPLII82OS0lJwZYtW1BWVob4+HgsWLCgWdGrpqYGb7zxBr7++mtYLBYMHDgQzz33HDp25KU3GsOXyH9X6bkcvTtl4kAyReIkWVlBNu5BtZDf0k6mDTUqLh1XSdYFWEFYI1l8hxvjvcmRUNCTIyCSyEhIHUpG+bnUMtTZ29DnvslG6KWXXsLevXuxYMECmEwmHDt2zPVefHw8tFotlixZgrS0NJw6dQoA8Msvv2DDhg0YO3YsIiIiUFZWhk2bNuHSpUt46aWXbti5tuqqnDt3Dvv27UPPnj3hcDiuGloePXoUc+bMwX333YeFCxciMzMT//rXvyCXy129PkC9AVq5ciXmzZuHxMREpKamYtasWUhNTUXXrl1dxz3zzDM4efIknn/+eWi1WqxZswbTp0/Hjh074OXFGYPGMBP9IlpvrgArI8U9AaCObFr0I4VPjVLuQfVmJ6uSEZud1Dnz03F1gcIq7joEkjUhLzc2LR352+jISKiojBN31XXgmJhmltCg44lF7sJJOpMsDh48CABYvnx5s/c2bdqEgQMHwuFwwN5o2F5wcDCsVitWrlyJqqoqqNVq9OzZE5s2bULfvn1v2Lm2ih3XWLV60aJF+O2335pFQjNnzkR1dTVSU1Ndrz3//PPYu3cv9u/fD6lUCovFgiFDhmDy5MmucQx2ux0TJkxAQkICVq9eDQA4fvw4Jk+ejPXr17uo25cvX8bYsWOxZMkSTJ0qnNl2JT4n2HGx8hrqsxSklhcAlBo5g8sOKGMjqEqynhDl4MgeHYO5NF52CdfxwTgtAHBUxV2HfuToCAA4quauRSxJ5BsQUEatC+rLOUo/7uLScTG+HKsOALqe3UmvBYDqR8a0+ljfzd+59Vl/NrTK7b2eTI7FYsFPP/3kGlTXgPHjx+Ozzz7DyZMnkZSUhMzMTBgMBtxzzz2uY2QyGZKTk7Fx40Y4nU5IJBLs27cPPj4+GDZsmOu48PBw9OnTB/v37xfFCIVDuBfmjjFh4afkNul8K5env6DkNs0IK+e5s16txcxFbJVSboP2JmVXbGRqrEzOp46KJeTEUjnZQxXE3aO158jmb7YB2J/UlhIBt0Kz6q0KUZKkeXl5sFqtzUTuEhISANRrxSUlJbk04q42T8hoNKK4uBhhYWHIyclBbGxsM+MXHx/vCjPdRa5EeIQxWMNFQgBwmUxZHFRxaaCh4DaGEgfHyApycKnKMF+unnChWnf9g66CXv7ckLlj1VwEZSQJGycUwEBCpxAAoh2cMUkyc5FJ3jmOdcWIiQKgleyzL3BsPABwuxLtMUItQhQj1NKcoIZ/N54TpFQqm411aFDPrqqqQlhYWIvzhHQ6netvuQvGl/qpIhg91VxvQwjZnxKp54yQiaxFxFg5LzqPpBPr6riNKFp3cxsWI2FCLSHdc1cdcFzFqh9wGxdL0a4jlRZY4m9oIufUZR5m+I1AkKrtIiF6ymQ7QLulaBuJ522wkjNADocEZhOpPUayspSkXIyRZGX528kaBkkdtFq589SqOKN30aSlPJezShnl8HSy8LuWgpRsutm+uoO0CT4yzlGqIye5igFPOq5liHJVGs8JaoyGfzeeE2SxWGA2m6FqNDG0IbppGNug0+lQWFjY7HP0er3rb7mLrjbhT4BfBJcGOHWBV6wtV3Ibii85cvkIWdS+k5QJCg7jvOFzeVxqJS6CS8cZC7n031ly5IRTyUWWAC9EqiPmOrmD8otc3bKUTBknhXEECjHgJNUv2gNEMUJRUVFQKBTIzc3F8OHDXa9nZ2cDaDonCKifC5SYmOg6LicnBxqNxiVOGhcXhx9++MFFVGj89xr+lru4JBV+IwdXcqkxLSkxAwDepHozO+m0MxcoQE/WIUJJijZL2DAZeWFQBqHkUEJ35JMuk1+RjbpZsVy/MM6pk1ZzjmhFOa8dF0Ov/B2edFyLEMUIKZVKDBo0CBkZGZg+fbrr9fT0dAQHB6N79+4A6if2+fj4YOfOnS4jZLfbkZGRgWHDhrkMzogRI/Duu+/iwIEDLqNWWFiIzMzMJpNb3UG57OZ6fSxY/4kdgczO22Ebcg3VnGHXW7jNXefD5YCs5O/iTV5AtRu6wirSKLDfkZ2z5SCpgyFy7hpWWtpQtsdjhFpEq4xQXV0d9u3bB6B+UF1NTQ127doFAEhKSkJERATmzp2Lhx9+GM899xwmTJiAzMxMpKam4oUXXnCx3JRKJWbPno2VK1ciICDA1ayal5eHFStWuD6vZ8+eGDlyJJYuXYpFixZBq9Vi9erV6NChgyiaSgDQ0Sr8rlAFcBFNBy8rzhdx5dsA8u5lI6FzCm5dkpLzatkNTHqTfQjWKLAxcBk5YM6dz9STjhlr2Msua6l1rDhvfASn2C4KPEaoRbSqWfXSpUsYM+bqzVbLli1zGYZ9+/bhrbfeQk5ODkJCQjB9+nRMmzat2ZqUlBR8/PHHKCsrQ0JCwjVle3bt2tVEticyMpL5ns3wS8e/CF7DdtsDQDmpdUZ/Hqm+nafkguMkK7cReSn4VCUDNoIqkHF1iDw3sn8BJNcjV84tDHNw9cBpPfOpdTUF3I9z4jLXrNorsoRaBwCR/3GvgbQsecT1D/odQRn73PqsPxva7TyhT8O5htduCk7646KZ8/qKSAHTWJJqfVjNGaFhZq6YlBDPFYuzszliQngQd/3OlfIKxDtJzjTb76Mhn2h2DlEXG1ef0yq4e6bGyjkSPkqy4Amgd94X9FoAKB3beiMU/G37MkLtmKItPPXQV6ZHtkV402lXr2p0DxCeCjCZFKg0+wleB9Sn46okwi9vX5MdYWrhqbXTUh/UEL+p9wVfBIcIZ8j5e5uoGU35pToUyoRvYlqJg+rUP62SIpzY3LuaHZAQORwngFpidD1QL9JaTaTkvOVWeBOCsuH9jCg5ITzCrK1UUKr03t4WXKoUznJkh+g1hqcm1DLarRHSEiraXlorkryEU3wLK7VALacBZybTOU5I4OsUnpbJVSiQbxf+oHZ1mKi8t87fBHOd8NuwgtTUiwqpRhSxbl8FR7M3k7U5VqsO4GdCRVsBHfFcyJUOWAiHoOKUAnJCCsvklHGMnWpveEmF0w7dmcjaAI8RahmijHKw2+3YuHEj9u3bh+zsbNjtdnTu3Bl///vfrzqX/FYY5RAtEV7f8QvjakLFVVwqDgDCBcymbwy1hFuXT2qW3e7FERPMJs7K2kgGWG0NKUtENuMeI4ke58nanDuwkIYvJIJLcVYUcXVSPzmpstGV6xETBeT92h4gyigHk8mE999/H3/5y18wY8YMyOVybN++HX/729+wdu1ajBo1ynXsrTLKodwufDPqWMelOZhJng3YqeZqO/cT0QUAyMm6QG0dl6fv8hduQznzb+43jQ/kGFKXi7jelAsOTmWjh4NLwwJACVncCSQN7d4L3MC07gouzVVg557/kt/CqXUAcM/1D7kmPJFQyxBllIPdbkdNTU0TNQOn04n7778fGo0GmzdvBoBbapTDjrCHrn/QFYj34XTrSg08My5Iw0UY52u4Dv/TKo4hdRspfqmRcOs6BHMbWEUll1ox2jijl6Hm1oWQNGQAMJBTfCNIxYRxMQXUOgf5eZnnmVm1QN9Ybiw4AIT/sJdeCwCFt4+6/kG/o8NB9z7rzwZRRjnIZLJmcjoSiQRdu3bFkSNHXK/dSqMcrESR2UkOCwvSGHG2lvOke43mIqGjX3HG5LiEM3o9SGUAg5OL2DqHcYyswlLOOLN9V5ecHHX9kgTwl3AGzEjqBurJUQ4Pdueu/eW93D1aTs6u0g3hrr0YYA1ue8ANSzw7HA4cPXq0ydiGW2mUA3MbVxn5jusAsoWw6CApa0OmVnrKuUjBW8IZhejwSmqdvpC7FpclXE3I28HlU+zE3KoGaEgxWXadF1m3sJVw31Gp4q5FTDV3r1nPi6PAz8CTjmsZN8wIbd68GefPn8fLL7/seu1WGuXAoNSpQjdyHk1+BRcJbTMGUev6k0bPh2xYZOHlz51n1kmuZhIGM6I7cobv5wLhIwQmWryRqeKiqFgb54DUkJm8SHIw4bEfuPRYsDdH9Kklt61fD3FNrgAw/PqHXBNsFqU94IYYocOHD2P58uV47LHH0K9fvxvxEW6jUib8SR3mX0p9VlGZD81Wc5KkBoaKCgBy9vNkbihuErCTXnt0WBXshGjqodJQapTDYSVnZJPIGpQ7kJJ961IyVanz54yQT93NnbElBtqnJEDrILoROnPmDObMmYM77rgDCxYsaPLerTTKgZknxBa13UFfE7e515DGJMxBUruJSbUAkH+SY1aFSjhW3akiLrIMAEmgcHLXIcLKG/VTKu6x1hOOGQAMTeCcM0MJl46LCOJIKQYDPx7DXXgioZYhqhHKy8vDzJkzkZiYiDfeeKPJGAbg1hrlEE4ImGp9uFx0toE3nBeUnPfWxcJtYt+REjN/IY1lfCwn2/NbDpcCSiRnyrDGq4ZUM88iDQnA1TsBPo138jTXyBug5mpJBSaObdpJ677yAQsPMaFliGaESktL8dhjjyEoKAjvvfcelMrmjJlbaZRDv2jhdE1NFDke4Rc+DWA2cYyeUhl3aevARRhs8+iRXG5Us4pMAdWS3nAu6Qz0N3H3TLAbhIadao6tpiavoYI0tCZy0ulF8lp09bq5KePGaItI6MKFC3j55ZeRmZkJlUqFe+65B/Pnz29Vn2VaWhrWrVuHgoICREVFYe7cuRg3btwNOU9RRjkEBgZi5syZKC8vx6JFi1zD7BrQq1cvALfWKIdPioQ3rvW+wCs+10q4B+cr0lv8ax2XBurq4DawOpIWPOoR7vt9vJXzhm21nHpFiJQzeu8rOBLEHRJOoBXgDV8xOT6iikz96smYbVxgMbXuUAkXPQNA3PUPuSacN1kxQa/XY9q0aQgPD8fq1atRUVGBZcuWoaKiAitXrrzm2l27dmHhwoWYNWsWhg4dit27d+Ppp5+GRqNx9W2KCVFGOQwYMKDF9wEgKyuryb9vhVEO/9tBeK/RkI58s1tmPvcAnFNyOZJEsnn0Jy9uYxhr4lKVUeGcosD5Ak7V+rZBnJz/t4c5uaifVeQ8BgAhZA8VScaDP3mqYTbuXuuo4Ua7Zxu57EBSED9PKO63r+m1AJCdeFerj40/5d5nAcD69evx3nvvYc+ePQgICAAA7NixA/Pnz0d6ejoSEhJaXJucnIzOnTu7xAMA4LHHHoNer8fnn3/u9rldiXY7yqF4FGnRSY+muphj9BRVClftBoBiCRfRfK3ijMk0E/e71JCMpUAZlza0uKFEcFxBjncne0QibVzk/bUX9x3ZVpZp4IwJq9G6x87VWKd25+YeAe7P+Dnb7e5WH9v59C63PgsAHn74YWi1Wqxbt871msViQd++ffHf//3fmDFjxlXX5efn44477sDbb7+NO++80/X6tm3bsHjxYvz4448uoyYW2q2Kdvo54RFVHykn1GgmFIYbUCDljEkHO7dJD7RxG20JKRXjTSh9A4CNNCZFZLNqmNOMnsTgvg1qJ8UUGGz3wjkll+ZSk/WyJM7/gG8HjmpdWsKlRqfEX6LWFZ7hFRM4WsofuNnpuJycHNx///1NXlMqlYiKikJubm6L6xreu5qgQMP7HiMkEpgUv1zG+YrVZs6QAEBfYg4RAFwkm2PN5LMS4uSMnlLGGaEAX3KceDV3DWvJAXP97dw6LZ/Fg44MMdhrr/TmTpZ9nnKyuHpZIHnPiAEh7Lht27Zh+/btrT5+4sSJzWrler0eOl1zo3u9hv+G965c29AacyPEAtqtEfJxCLdCKlLR2mjkBUx31HI+2GAp550WkrpcQ0hWXXgn7qY+eI7rLxoQwdX1GLUEAPhVxoUXncmIDeBrQqRIPAovchEGm45jowqjkXcG3YUQdlxBQQEOHz7c6uMHDBjAnNItA1HmCV2J3377DQ8++CDUajWOHj3a5D2r1Yo1a9Zg+/btMBgMSEpKwtKlS9GtW7cmx5WWluLVV1/FgQMHIJFIMHLkSCxZskS0ULCQYAJpKzipmDh/3ntINXENsuESLq02po4ztP9RcLWrBwM5YoL8LLfTHrjcgVrHUsKt5LpOFl5srI6crKojG5WzpFxarYuMqyXpvDjDnlvH3aMA0JteWQ+HAMMZEREhyLBERDR3yHQ6HfT65n1Rer3+mr2WDRGPXq9HcPAfMkcNEZBYYgGNIco8ocZwOBz45z//iYCAABiNzcPfZcuWIS0tDYsWLUJERAQ2bNiA6dOn48svv3Q1q9psNsycORNWqxWvv/46bDYbli9fjjlz5uDTTz9t1gTLINYi/IFLCObotgCQU8qxufzJ0UnsbJj95OiBOy1c5KXowBlZJakI2TuMY8ftKOUiISt5npVynkBRRM4TCrJzUXCojfuOxPR5AMBRC7cRRjnJopcIEBK9TZo0ye1WlLi4OJdgdAMsFgvy8vKu+bcbDFRubu5VxafFEgtojFbdBqNHj8Ydd9wB4I95Qi3hs88+g8FgwP333++aI9SA4uJibN26FUuXLsXkyZMB1PcEjRkzBh999JFr5M4JQwAAIABJREFUxtA333yDM2fONKEShoSE4KGHHsL+/ftF4aoz3qLFzBMMlGQfTbiDS8vIyfRYoIPbGcqdXKoj/2uuT4jdorMLuHpCZ1K251sVdx2+lFowDFzkzZaTOpFSQWFetdQ6O0kuGaTjVC8cdt6wu4ubzUEePnw41q5di8rKSvj71zvA3377LSwWyzX3z8jISMTGxmLnzp0YO3as6/X09HQkJSWJTkoARJon1ICGRqjly5fj+PHjzd4/ePAg7HZ7k85brVaLUaNGYf/+/S4jtG/fPnTu3LkJl71Pnz6IiIjAvn37RDFCQQ7hm0OxXoPoMC59ZNVzD8AFKZce60I2x5aTTZldSY+/Izkv6dKX1DJ0CuGi2R/KOWmaB21++FrBFcR9SWvikHGZgkopd88UWbnIpJ+avBZ6rk4aZ+dVKLpe/5BrQkg6TgxMmTIFH3/8MebMmYM5c+agvLwcr732GsaNG+diugHAkiVLkJaWhlOnTrlee+qppzBv3jxERUVhyJAh+O6773Do0CG8//77N+RcRSUmvPnmm+jTpw+GDx9+VSOUk5ODoKAgl2VuQHx8PNLT010TXHNycpr8UI2Puxa9UAgsBG92wEiuU/t/D0UgmJA2KZVLkUhuRCdUToTbhN/4PzrKESUTnuMvlKuopsXUjBBEEh54qUwOOeFehv7FD+YTwq9jfLkR2RCeOvxVaUc/h/B1RVIHFe1lyx1QMHLfAE4rgUBCXqa/3QgTkco7U+sLP6fwa1+pAjoSYyfiYsthNQk/T4XaDari73DcZNkenU6Hjz76CK+88gqefPJJl2zPlaLSDocD9itS98nJyTCZTFi3bh1SUlIQFRWFFStW3BC1BEBEI3T06FF89dVX1yQstDQnyNfXF1arFUajEVqt9przhK7Mc7I4rxD+1RPOcD+XzuGEWSr8JtQ5nPheyaVzJtZxXu1ALy7cTpTUgFFvGfIQ93Bu+0QOO1EbLE6rAiA8xXlcytWuTjkMOHX9w5phiNMPJcQl1DmlsLFkCCt3LS46vaj8aEenmRqP0Y1UA2F1CgFAuMhXU9zsSAgAOnXqhJSUlGse89prr+G1115r9vrEiRMxceLEG3VqTSCKEbLb7XjxxRcxffp00WR1bjR6OYSnSHzCOIPgX8R7UqPJHqMSUgesghyGp7dy5/njJu48A8nKhyaQu4a6cm5jHyrj6jo6NyZxXiaJCZcU3LUYbuHSXHpyzEUNmTYc1qWAWicGbnaz6p8Johihzz77DKWlpfiv//ovFy3QbK5nojSepqrT6WAwNFcdqK6uhkKhgLd3vbfZ0nFizhMKChBeTJX7cXUdf1JiBgCyyHHbYeSUTCWZypGSoi9qKWdMTCQRwko2wxSRRj2MFG6OsfFMrhJSRZvtLypxcuSZQHImVD6pEF9VRFJNAfDSp/Voi0jozwJRjFBubi7KyspcYxcao3///pg2bRqWLl2KuLg4lJeXo6qqyjXADqivFcXExLgIEHFxcTh9+nSzv5WdnY2RI0eKccrIIijTvfK5mlCRg+vZAYACcpNOdHBGQUryznRk2jAiniN6FJ3mmlXVAWR3/2VqGY4rOCtkkrrRrEqu8yajL5YuT/o7CCYp4ZV63gi5i3Yp0NlKiGKEHn74YReFuwHbt2/Hzp078cEHHyAsrD4Xe/vtt0MqlSIjIwMPPfQQAKC2thZ79uxponM0YsQIfPHFF8jJyXFx1Y8dO4aCggLRimPniKFhIXlcFBYhN6LEyhmiAFJJWUFSiruRY6UtDi5Fogonae/NfZRWIe83Lj0WZOe2kdOkNM0JmQW32bmIxoc2Jtw6Xwl3r/l6c2k8JTmqopz8PcUAS0dvDxBlnlB0dDSio6ObrDl8+DBkMhkGDhzoei00NBRTpkzBm2++CblcjvDwcGzcuBEA8Oijj7qOu/POO9GlSxc89dRTePrpp2G32/HGG2+gd+/eV422GMQQHekF8EZnH85z70BSnw9IOMmfKglnTBTkRlRFPuBSNWeEApxc7YpNi+gJYgkA9LQpkSXnNunedo7afUTGpXAD7Nzmfl7KOVjdSZXwLAn3/VTkMygG3Cjx/Z9Hq4xQeXk5/vGPfzR5reHfy5YtE9Tdu3jxYnh7e2PVqlUu2Z4PP/zQpZYAAHK5HBs2bMCrr76KBQsWuGR7li5dKopaAgB4kSmEGiOX7Ci1ceuWTuOUCFI/4YzXHjm38U21cUbo37u4bHtyF05JGQD+5yI3G2iSN9ck+UMddy1Wkx6BBdwY61g1d57//DtnhC6s5fqEQkh23NCJnAMpBpxs7rEdoN3OE2KG2nmThqt/50JqHQC8lMc1SSaSkYmGdNnKyazaQxFcsWXvRY40qyfPM59MqxlJH/j/783fM68d5X6bCDKlyg7Di3dwDtYJ+c2v7TyR/7Fb678PfbDVx44sTnXrs/5saLcq2ueVwj2TThYur1uazwsn+pKXiN0Yentx3um3Vk4bz0E01AKAP6mNZybpvTE27tpnyTkjdOYnfoJNN/I7sqJUnZ1c9ByTUEGt8yZVu7Ms/HPoLhyeSKhFtFsjFEjsYQEOLg1wzg313uf+zm1+W9dd/5irYQs5tTKRpCLvJlWtJ/TipmR+forrYxsi45TQz4BLcW1W8TqFElI3UE0yI++fydVozn3A3TSlJi4SGjuET+G6C086rmW0WyOkI9hOUf5crv1ENS/69/gHnDjkX23cg1pE6o4F27kNZchUzove/QlH0Z7QiUv//Xye67a/oOSm8faW8FNAQ0iJmEAbl5n/mHR44q3cPWonN/RDP/K6BxPolfVgz7k9QNR5QmazGevXr8cXX3yBoqIi+Pv7Y8SIEXjllVeaHJeSkoItW7agrKwM8fHxWLBgAQYPHtzkmJqaGrzxxhv4+uuvYbFYMHDgQDz33HPo2JErKl+JWKnwzc8/gtswR0QYcfw3bhNbLCWjL2oVkCPhmiTvIvthftzCbUTBZKOjycCxBtm45C4nF1kSMxdduEjWry6TDsjcTlxUmn06+PoHXQWV5MC/2xPaTjHBw45rGaLNE3I4HJgzZw4uXLiAJ554AtHR0SgqKmo29iElJQUrV67EvHnzkJiYiNTUVMyaNQupqano2vUPrdpnnnkGJ0+exPPPPw+tVos1a9Zg+vTp2LFjB7y83C9MltiEM3rC9HzgaCFTHf/r5FJ5t5Mkis5OjulkJDv8+w7hJp0e+YEz6go1ZywtJCvzsIxXbk4gr4WWpKHHk8IeJ05x16KDhovyK4ycI5Gfw9UtAYBXnauHxwi1DNHmCf373//GsWPHsHPnziZ063vvvdf1/xaLBWvXrsW0adMwY8YMAPWjaSdMmIC1a9di9erVAIDjx4/j+++/x/r1613NqZ07d8bYsWOxbds2TJ0qnNl2JRRED/PpvGAMuIMbilZZwPnSFlIjrUTOGczTEo6xNIQ0shd+4ZpHc5TcRjSwL/e73NW3DP/7lXDPfbBNjQJSy01NRkPVZOYnn9SOk1q537TWyKUcraRDoPPhHQJ34akJtQzR5gmlpqbi7rvvbmKArkRmZiYMBgPuuece12symQzJycnYuHEjnE4nJBIJ9u3bBx8fHwwbNsx1XHh4OPr06YP9+/eLYoR0hJ5bZHQVaolJEqdygxFKCoOOq+N8qGryQbWSbC4rqewgZecXmbjIKyeDiy5yrFr4E47L9you8vKDDOUStk+IW2cm75m/diTlrC5zRijfzmVCwke2YbOqxwa1CFGICVarFadOncLIkSOxcOFCfPPNN3A6nRgyZAiee+45hIfXFwQbxjA0HhsL1M8JMhqNKC4uRlhYGHJychAbG9vM+MXHx+PgwYNinDI0KuFGwU5K3aslvIq2PyltYq/lIhNvchier4IzCiUmjlmlJYf92cjpmloHdw19yGpSoBsyLwbSeHmTaTxNR87Q1uZx24+WTW4p246H5aFotwxRrkpVVRWsVis++OAD9OnTB2vWrIFer8eKFSswa9YspKWlQS6XN1HUbowGZeyqqiqEhYVdc55QdTVHlb0SEuJBtRi5n8vi5Om2541cTchMjC8HgO4OLs1VbeWMQkwA18WuN3ARjZTcoH1IfbRa0giZJLwRYusPIaSvVHqacyQ6RXA9aT8Vciob5iyOqQgA7nYYuT8W7/8uRDFCjt8VmzUaDd59912XkYmKisIDDzyAb7/9FsnJyWJ8lGhgNvdoRw31WRI3NHRZlQY/Mv13EWQxXMZ9ntafi6DMJs5YMs4HAFRauN9FTnrAvNsC+JARTR3prCvJlKOJJBgEkf161QW8mj3fOlwPh0hyY/8XIYoR0ul0kEgk6NOnT5MoJykpCVqtFufOnUNycjJ0Oh0sFgvMZjNUqj9olg3RTcN4B51Oh8LC5rIlYs4TCpYK3/wCQjk2TwBqkXsxkFpbTW5HdnIjKpOSIyDIzb28UPgocQAwmrkNLCSY84ZLTNx950/6H+4MtSsn62xe5EZZV8NJRLFisjVkylgb3JbEBA9agihGyMvLCxERV28elEgkrgF3DbWgnJwcJCYmuo7JycmBRqNxkRri4uLwww8/uIgKDcjOzkZsbKwYpwyDXfgmJiVZTgBQwcy+BnBZwaVlwq3cLsbWBWpt3K3UqQcn3XL2BOebqn1IaYdSbtlZcGzDs1IgAVwB3kw6BGHktS+p4dJxUSFcar2ElM+qLeNnNLkLD0W7ZYhWqRs1ahQyMjJgMplc0dCJEydgMBjQvXt3AECfPn3g4+ODnTt3uoyQ3W5HRkYGhg0b5jI4I0aMwLvvvosDBw64RjcUFhYiMzMTS5YsEeV8jYQ39Z/cDoj14jxpP3L0QCk5tTJMynl9+Q5uQ/GScVnvugruFmR/l7pznCBsCGwokwo/1/EWL3yu5NK4ajKVpyGpWCpyp4yN5ByJC5c4JZE7w7neMnNdGxITPNm4FiHKPKGIiAjMmDEDX375JZ544glMnz4der0eK1eudPX3AIBSqcTs2bOxcuVKBAQEuJpV8/LysGLFCtfn9ezZ0zW6YdGiRdBqtVi9ejU6dOggaGzEtVBIRBgj1BWw2YQbL99AI+zEXRgQXIujJZy22mWbGr5O4YbhotKO0XXCPelyKGEliunOAiDAV7gSRRCpjyaROuFFqDt8LfFBNDEy3SiVIAnCU47T/EtQUCQ8eo4Iq8Z3ZVzh/oTSjpHEtT+XH0TVPTUyG6LjhRuwz893RA8LkU73MuESMVajoxeXhm8Mj2xPy2jVKIdLly5hzJgxV32v8TyhM2fOYNmyZTh27BiUSiVGjBiBRYsWISioaeokJSUFH3/8McrKypCQkHBN2Z5du3Y1ke2JjOQEKK/Ed6F/FbzGi5TQ8VZyURAAlJNijWbypmd9xQKyOTbaxhmTjgGkjl8VV5sLcHDXcL+aS8PeZuZTv6dV3LXvTComDOrIRSbaGC56PnWAS8WqyOGCANDvUhq9FgA2RTzc6mOnFbg3NoLB1q1b8e233yIrKwu1tbWIjY3FjBkzMG7cuOuuHT16NAoKmksi/fjjjwgIuH60227nCX3QsfU3RQMGKbgcdi1ZRAeAHHDpsRgHl447K+MYRHF2juXmJSOpz2QNqoM/lxrLqeSICfkK7jw7WnlSbw1Jz1eQW0EUOQiRJbOoldw9w9auAPdn/PyPACM0vQ2M0IgRIzB06FCMGjUKGo0Ge/bswebNm/HCCy9cVxxg9OjR6NGjBx577LEmr/fo0QPyVjin7VZFW07c/ywV9YyJV0Q+TSbqg8n89wlSiDSO3DPj+nH1hMM/cWnKgEhuw2SV0H+Sc8SEaNIZAABfkmCgINepLFy0HuXNOQTna7jnqYOSu/Zi4Fb39Ldv394kahkyZAiKioqQkpLSKoWaoKAg9OrVi/rsdmuEGJKnLoiLLsx6Ph9sInk1bOLBSLbVqaTcOmMRR7ctk3G3rox0hkvl5HgEcPRl1pAAfGMkuxkYychLX8eRS2Tkli4n1cXFwK1OTLha2qxbt274/vvvb/hnizbKwWazYePGjdi2bRsKCwsRGBiIO+64A0899RS02j8Ks1arFWvWrMH27dthMBiQlJSEpUuXolu3bk3+XmlpKV599VUcOHAAEokEI0eOxJIlS1qVY2wNfO3Cb0ibmXvYRkQU4kg+Vyxm++a9ya0omKSS+2q54q1PIpk6yuM2otrLpPEiXdkLTu53uSABBoDz+E3kJh1Bei5hTi4VK2N70kgrW1Z388eCN+DPSNE+cuRIM4m1lrBjxw6kpqZCJpOhb9++ePrpp12s6OtBtFEO7777LtavX48nn3wSvXr1Qk5ODlauXImCggK8++67ruOWLVuGtLQ0LFq0CBEREdiwYQOmT5+OL7/80tUnZLPZMHPmTFitVrz++uuw2WxYvnw55syZg08//bRJ7xALFaFEcDIvGOEkU4ZVPkgkFYrt5G3vR2qWVdVw6aOAPO73ZLvmz+WRxASyAfSvdh98ruS+Yw1ZM1GRBrOSnCc0mOz3qasla6UkWc1fx6VGxYBdwE+7bds2bN++vdXHT5w4UTTWcAN2796NQ4cO4Y033rjusaNHj8Ztt92G8PBwFBQUYP369Zg6dSo+//xzxMfHX3e9aKMc0tPTMX78eDzxxBMAgEGDBsFoNOKtt96C0WiEt7c3iouLsXXrVixduhSTJ08GUE/HHjNmDD766CM8++yzAIBvvvkGZ86cQXr6/2vvywOiqvv1n5mBYYBhZAdBQAVRMQQp19xwSc3MNN/eLFNz6aa++V4r3OtWVr5pZmjGTZOuW3W1655UtGFeS1PAlNRgUEEQZR+22ef3B5fJERDOM4PYz/P8pYfzne+ZOed8P9/P9jyH0a1bNwCAv78/pk6diqNHj1rlHeyBjuTmYru8WeqenhYujl1NejRs2EBr4sJqRWput+/vxsf3S2uFG0w/sx61JHsFSwrLGpNa0nj5szkhJbchqCjnPBN2C+rh3X6MCUK2hAUFBTh58mSrz+/Xr1+jY1VVVbhxo2XZmaCgoEb6bDk5OVi2bBnGjh2LiRMntvgZK1eutP77gQcewNChQzFu3Dhs3ry5VUbMYVIORqOxEemoSqWCxWKxek7Hjh2DyWSyKftTKpWIj4/H0aNHrUYoLS0NkZGRVgME1De6BgcHIy0tzSFGKJ/oExoq43Z8OoMTPJ24+tdfpcL7GgCgh4mbr5eWi3WUSLnch9JIsmGTHlutifMsy6ScUT8nB8IIAtv6Em3OmBSQLBt+Rs57vnqJ04Tq1IUjry3K4zYuBVe46wQAXhi8HkJ+2eDg4CYNy+3OvxWpqalYtmxZi2O3b9+O/v37W/9fVFSEOXPmIDIyslUGpCl4eXlhwIAByMrKatX5DitMmDJlCv7rv/4LI0aMQExMDHJzc5GcnIxJkybB3b1+IVWr1fD19YWXl63CYUREBA4fPgyz2QypVAq1Wt2kGxcREYHcXELQpwkwu8wO3iQFSwEXAgIAHzIMpCAD52ecOWMyyMh5Ju6unLG8ruGMs787d52FWm4BKyfZt4vI0m4A0JDPTIWMM173Ec3GAGAWEqO6CawcR3sWJgi5I5MnT7Y7vMZ8Rnl5OWbNmgWlUomkpCQbfs+2hMOM0IIFC2AymTBr1iyr5zN27FisWrXKek5zEg0dOnSAwWBAbW0tlErlbaUcGjSJ7IXSLPxFZWk/Kgi6lwbcIKuywkzcQlREGi9XsiGXDZGk13AFKp1V3O7bq4b7XWrIOsVC0iAAQJiR9BLJKfU67vn+I5eT2+6s5KizarTcBssRuNur42pqajB37lzU1dXhs88+g0rFt5WUlZXh559/Rnx8fKvOd5gR2rlzJ7Zv346lS5eiV69eyM7ORmJiIt5880289tprjprGYQg2C9+B+0RwnpDkFDUMAEBq06HQwBUKVJASEFVabtek0nFGSMWKzAVznldxMZfXkZBGSG5HifZFJ+636Uzm9UpruNyOD6FuDADpdZzxGh5SSI1zBO726riFCxfi/PnzWLVqFYqKilBU9CcLRlRUFOTyegM+Y8YMFBYWIjU1FUB9LcAPP/yAoUOHIiAgAAUFBdiyZQv0ej3mzp3bqrkdYoTKy8vxzjvvICEhAdOnTwcA9O3bF0qlEgkJCZgxYwa6dOkClUqFqqrGu5jKyko4OzvDza2+iaO58xwp5cCwWh//lYsMu8CMTmSjnM7AyWmxvRtD9JwxSSWz6GNvcN/PR8ItYHvPcLRPIUbOmHR14n7PCD3f3ighC/uzSUdBQZLXKt240m4lKWgoc26/ltG7XdSuQbG6qTzSd999h06dOgGo144zmf78Np06dcKNGzfwr3/9CxqNBkqlEv369cOGDRtaXd7tECOUn58PvV7fqNengSk7Ly8PXbp0QXh4OEpLS1FRUWHVDgLqc0WdO3e2FkCEh4fj/PnzjebJycnB8OHDHXHJqCEW6fuVXHc/ABRruC5JPaluxtLoZJOL5ggdZxSCQ7hij6JCLlwwzKOEGvd7ORf+qyCl3U+5AD0M3M2vIJ+ZbqThM5EtE97B3MZMcZ6792wBBQBw/Bx/4m4Px128eLFV5+3YscPm/7GxsY2OCYVDjFBQUL2HkJWVhb59+1qPN5RyN1jRwYMHQyqVIiUlBVOnTgVQH4v8/vvv8fjjj1vHDRs2DAcOHIBarbZa08zMTBQUFDikMg4AXAierHNVXohy43IKSjJnojRwRkFLSooHkpxlJRKuesyrhDPOLiS9kLbOmQod+kGP6wT7QX+tBL+RhKK+Ju5eFDuxJeFc0EhJFpc4uXNGjw1tsaq6jsDdHo5rTzhMymHMmDFITEyEyWRCr169kJOTg40bN2LQoEFWQxIQEIAnn3wS7777LpycnBAUFITk5GQA9bHGBjz00EPo3r07Fi5ciBdffBEmkwlr1qxBnz59rPpC9oIha+zT6To1V/5VLoYNANVkA6EfmTO5KOeM3v1GLl/mF8IlmYuucLthlUoLpYdwL/FEqR8131Fy8xFpliNHzhkTHVnazRKfsgl/C8nQUCMllVXdSJpwB+Bu545rTzhMyqG6uhpJSUlITU1FUVER/Pz8EB8fjxdeeMEmj2MwGJCYmNiItudmpVXgT9qeo0ePWml7VqxY4TDanos9xgkeo/TiQlxX7ehPYFFg5uLmn7pwpJILdZzxKiebalk9ITmZv8g3ccn3NAW3Bw4121FRSVY4OpFtoAuCrlHjLqg5w15K9mw9PI8aBgBwf3UXPxjAW2Etk4A2YMUV++b6q+GelXLYH/iU4DE+5MLn68F39+druB2/tzNXdaY2cf03LI0Oq9FkIavHiklCUTkZqjql4Hbt9ugJsZLwrmTMaFz4VWrcjTyuKMVdyb2Heh2ZLAPQM/sIPRYA3hBghF69x4zQPcuizbxvfirOmOjZ6gIAhaRYnJIstzWSSWY5mYBXunALClsSHizjwoalxjvTuNeAOumdz2SzOShNMed1y+XcfAbyfTKQhR6OgJgTah73rBHqE1AseIybF6+QmpfN5YVciaZaAKgwkzQz5HY4kixoYHe11+tITQayVraUlI7Il3Ah3A5klSIAFJLhOBlZzBJFUowb9Zyh/aOGa9MY0rex+uedwt1eHdeeuGeN0Pki4VQ6EXquMg4A9KRn4mHhFhRnMhXqT3KysUqn7iQLha8LF250d+eMQlk5Z9Q7W7jwX7XEAh9y5fI3c8+anIwAsmEuViRSUc1tlGqu8csdT7xVD7NYmtAsWrwrKSkpOHToELKyslBZWYmQkBBMnToVTz75pA2xaVpaGt5//33k5OQgICAAM2bMwDPPPNPo87Zu3Ypdu3ahpKQEERERSEhIwMCBA23Oqa6uxpo1a/D1119Dr9ejf//+WLlypbXU2xEoJ3a2v1b6IsqJq+Ziy0PPuXAvzmA9F3YKIaUjzGRS2yeE4+XPOB9IjVPoOKPgASMKZMK9k84G4Fs5dy/8LVwxBEu2wGhsAYDReGfDXF29uc3ghXyuEAIAQumR9RBNUPNoccX55JNPEBQUhMWLF8PHxwcnTpzAW2+9hfz8fCxZsgQAkJGRgfnz52PixIlYsmQJ0tPT8fbbb8PJycnaDwTUG6D169dj0aJFiIqKwp49e/Dcc89hz5496NGjh/W8l156CVlZWXjllVegVCqxYcMGzJw5E4cOHWpEO84i0CQ8tNbJS4O8cuGFAhZIqNxOkNFIl9uaLBIUSoQvmlfkEpQQ+Z3H9UZcIyryJDIL3Pr5Ch7nfN4CE2H4uoaUIf2qcIHBaqkUXczCva//VsiQrhVePfYPp644RkiDDzby74dOKkEJ0RLgWesKLcHS0FlZBa1e+HtRqlfAQNz7DhIjOgUJN2BXC+2vbhVzQs2jxeq4srKyRmXRq1evxmeffYZTp05BLpdjzpw5qKysxJ49e6znvPLKK/jhhx9w9OhRSKVS6PV6DBo0CE888YRVssFkMmHChAno1q0bEhMTAQBnzpzBE088gc2bN1sbUwsLCzF69GgsX768VXrnrUFm2KOCx3h6cTvaC9d4Z/5XsroqkmyJyCGpW/qQEhBx3YtaPqkJ/PoH18PuSXLjlZJVdR84l1LjhkqFG+YGVEu4Ja8fqRzsb+EeNi9XLqSaW8dV1fWL4ErJAaDjsR/osQCwuPPUlk/6P6y5/Jldc/3V0OI2pDntcZ1OZ6Xf+eWXX/DSSy/ZnPPII49g9+7dyMrKQnR0NNLT01FVVYXx48dbz5HJZBg3bhySk5NhsVggkUiQlpYGDw8PDBkyxHpeUFAQ4uLicPToUYcZoQqiukp7gwtVeZJEjQDQW8dVHrF9NDngEuKh7lx/kQsZIQlUkwuYmStoYDmtA0k9KH87Mtl6ssH5ijM3rn8AR2dVXszdC/bZrq1oRxbtdpv57ge1qp4+fRqenp7w8fHBpUuXYDAYGpHVNQjS5ebmIjo62irBcOt5ERERqK2txfXr1xHeFV3lAAAgAElEQVQYGAi1Wo2uXbs2EtKLiIiwkuw5ApeJ6qPeUi4fpDYqqXEAMH4pN3bXu1yu5ZKEW9yv1nDXeeZnLtTxUK98alxOFrfwKcl8iTNZ8p4h4wooAKDAzLUSeBHhWwCY/88B1LiS5Y0VmluDPCl3nb0HcxslR0AsTGgego3Q2bNnsXfvXixYsAAymQyVlfUElLfqTzT8v+HvGo0GcrkcCoXtzr6BTaGiogKBgYG31RJq+CxHwIfoiXB140I5AVW8J1T2WTY1rrOBczEynMiQDJl8V5FVdcW5nNFzIUveFXd4L+tOSokDQAjpffmQxbKm9LPUOCNZMdoV3LNmIdsIHAHRBDUPQU9dcXExFi5ciOjo6FZrRdytKJcJfwHITS38VTU4W8X1CcUN4eLf1/O5kuIqC7eLLtdzYcMbZP9NeCjHht3hd854sc24PmQuCeB3z7VkM5SKNHwSJeddmshcUj5ZNRjlwhkvR0AMxzWPVq8AVVVVmDt3LhQKBZKSkuDsXL/INXgyGo3G5vyG/zf8XaVSQa/XQ6fT2cjGNng3DdIOKpUK1641TiA6UksI4HoWf67xRn9FOTWfjrVgEs4zMZDTackFzExywLGeidnA/S4ScmGXkSX2k3V67HC5syXMJpKJy8CyTJPsDmaylpx9lyw6kjHVATCJvlCzaJUR0ul0mDdvHkpLS/H555/Dy+vPXX1oaCicnZ2Rm5trw3Cdk5MDAOjatSuAP3NBarXahqxUrVbD3d0dAQEB1vOOHz9uLVS4+fMaPssRYOS9eyg0cO8gPFavLvBGHbFGhxv0WLSH8xSmGvXQSISPDXBWUDtpL2cdjESjq6dEj8iHhOfarvygxAmj8E2Jv8SMB2OEd86fy/Sn+oS+cq5FBZGjiZGqoCHugwoyhLLelwVUt5dRfQ2p3wrv2wqwSFBnEf6M9nCuxnWDcG/IXG1E3gnhnnBof/tzSWJOqHm0+AQYjUb885//xMWLF7Fjxw4EBwfb/F0ul2PAgAFISUnBzJkzrccPHz4MPz8/9OrVCwAQFxcHDw8PHDlyxGqETCYTUlJSMGTIEKvBGTZsGDZt2oSffvrJatSuXbuG9PR0LF++3CFfGgD0xG5KZ5Ch+Ibwh9hJYkF3I5FolgB9TFwS1l1WC3dCWjrG6AYQXk0t8/0AdAkpg4bITxfXuaErUW4tgxnnMv0Fj6uSONGS4p4S4UYhwCRFAFmTpyO94BADt1AWHFdQOlt6owweRKWbwsUIJTEu+xhXBJN9zBN9Wz7tthBNUPNo0Qi98cYb+OGHH5CQkACtVovMzEzr3yIiIqBUKrFgwQJMmzYNK1euxIQJE5Ceno49e/bg1VdftVa5yeVyzJs3D+vXr4e3t7e1WTUvLw/r1q2zfmZMTIxVtmHp0qVQKpVITExEx44dMXnyZId98TImQkI+SWw/BADoSCNUbeLCY/5mLnrNSjL4V3LfT0ZG2Z3JkBPTGAsAXSRc/iJcz4eObpCktx6kkb1WzeXZuoVweb0KUgixztR+LGWiJ9Q8WrwrDWXRa9eubfS37du3o3///ujTpw8+/PBDvPfee9i/fz/8/f2xbNkyG7YEAJg9ezaAeonYkpISdOvWDZs3b7ZhSwCAdevWYc2aNXj99dettD2JiYkOY0sAADfimXCScQtfXi1fov27K+dhxJCLZq6c230PNHIFDZ0e5+7pqW2c8brPWdPySU3gDyNXeJFu5nKIXi68ECLb0+RCEphKyfXVm4gqAECtntvwlJEbJUdALExoHvesntCJIOFeVXgM1/2ekc7xnAFAhZTbvSlI/Zs0bq3FRC1Xvq4gReZYaYX7e3Nd87+cDaLGnVBwmwFPkkgWAGpJby/ESDarksU6DGUPAFw2cCXoA9uRMWFO5ymtPvfjy1/YNddfDfcsi7abi/BFU0vF8ICenYuRl8ftbLUWNi/AjfMityQ1RIIZAAJUJNNCHReuOn9WeD4IACpJSWm22tAe5v8bEu638SIVSyWkKxQWyxGRnszk2hbkHdrPHxGr45rHPWuEGP6p4AK+1NZDwfVEVNdyC4M3uDCeq/bOhiz0Ou4RlJILH1uiHUAQ3gJAR1LXCQACyLSQijQmnibut5GSekJ1N7j36QELt3GpuEK6+XCElMPdjY0bN+KDDz5odHzx4sXWNMrt0Bp1hOZgt5SDyWRCcnIy0tLSkJOTA5PJhMjISPzjH/9o8iLuFikHZn+aZ3RDXKBwMTwAKLrB7d6cyMe3wsItREXO3MIQauAW6es1XGilgzNpZJ2M8Pbi8lfnbwhfinroTLhK/qYeZJHIJZIDzsdI9lBJuetkjVeJgTMmHmQFpyNg/gtkPRQKBbZt22ZzLCio5TB0a9URmoPdUg5arRYfffQRHnvsMcyePRtOTk7Yt28fnn32WSQlJSE+Pl7wxd4RKQdC8TJmjgyA8LDa11t4h7OSpNEhCZFRSC4MgWTzqJeUVFYl+kQAIMC5DmXlwqurTkPJVK7jhswCpqyyVmLBZfKxKQJXjamSc79pJOk9G4o541xEVv9FkMbSEbj7TRAglUoRGxsraIxer0dSUhKmT59u9Zj69euHCRMmICkpyaqOcDu0eDf/8z//04ZJe8CAAaitrcWuXbuwaNEiKBQKfPfddzZsBoMHD8bly5eRnJxsNUKtvdgzZ87gxx9/tJFyiIyMxOjRo7F3716HsWhbiG7tsq+4woQAwnA1oFDK7foiyRJfZ2eyvJfMQ4R3535T9UUuQKIjuepUZPgvTcZ5XX3NnIcIAFoyHNeTZBQwkHQ/3oEcyW6gmrxOw51lrrgZ/7+WaLdWHeF2sFvKwd/fvxGdjkQiQY8ePXD69GnBF3unpByYRPql/Ma/RWsQ3oVbaAFAncdVZdWShQnVpAd1kWAlB4CAIm73LSUrwK6CM+pO5BryMFnJ9aDfdW5CAAfLhYv2AcAlMn2l0HMMDZICLkSdIycrRjW8Ye9Jj6yH5S9ghLRaLQYOHIjKykqEhobimWeeaXG9ba06wu1gt5RDUzCbzcjIyLC5sLtNyoFZpMN9uVLUM7lcRRYAsP11HhauKktD8nL1N3PkkKzseRXZjDukp3DKHgA4eYET0Tvjwv2epaQhAfi+HRdynKeCC/+x3HFRei6E621H07i9MAowQnv37sW+fftaff6kSZPsbuQPDQ3Fyy+/jKioKOj1enz11Vd44403UFZWhhdeeKHZca1VR7gd7JZyaAo7duzApUuXsGrVKsEXe6ekHDwtwhPp5RXcrt3fWYs6MgzUyZkLWeSTGkZdSObTS+B+G0M553p168gJqf3vBc6z9CaeFwDQkgWorEEAgEtk71VvUlohMJxrAHb2JA30Me5ZU7hy99AREOIJFRQU4OTJk60+v1+/fo2OVVVV4caNGy2ODQoKgqurKyZOnGhzvCEVsmXLFsyePRtubhxLRWvgcCmHkydPYu3atZg1axYeeOABh1xkW8CVeFE7hnAvGwDkXuJCedU6LtTBSFUAQCnpeXXVc0lftlm1robzhPxIVU4N2W2vJPu8iqQWBJu4RbqOrKjUS7hnRqfhHhqLmcvtXHbm7kWUTztWxwk4Nzg4uEnDcrvzb0VqaiqWLVvW4tgG1pumMHbsWOzduxc5OTno3bt3k+e0Vh3hdrBbyuFmXLhwAfPnz8eoUaOQkJBAXeydknLIIzRJ8i67IpisPLKQ7YfpTlwOo6+JS4gXSrgdTweCLBUAQsK5EGf6RY6Fwo1sHnWHESaCEGeI1ozdriTPnYzbgPQnaW2qyLx9UaGq5ZOams/IXeeDHTgvuKas/eS9hRDTTJ482e7wmiM+ozVorTrC7WC3lEMD8vLyMGfOHERFRWHNmjWNKiLuNikHhqGkl5zzhDINvPF8di4Xl9m2hTMmew1XqXHdwYW5vrjE9X493o3L7WRkc7mWzkpO2n2tidtEFJu1KAC3kchyvrOL7TPPckUppzdzG4JT1VxUYfIyzlg6An/F6rgjR45AoVCgW7duzZ7TWnWE28FuKQegPkw3a9Ys+Pr64sMPP4Rc3vgluNukHO7vILxiTeXHeUHdc/iu+ZQkMg9FPvQT5I3vb2vgp+d+m0gFV9CQl8uVvXck1TW/03PzRZBVAg+RhKkAcFLOLe6hZi6slvcZ55kEenEe4rlqzujd2JpNjQOAzs3n5luFu522Z/LkyXjsscfQpUsXGAwGHDlyBIcOHcK///u/2/RmzpgxA4WFhUhNTQXQenWE28FuKQcnJyfMmTMHpaWlWLp0qVXMrgENzU93m5TDoRpfwWMerOR2pj878Um9f1vO9cN89DZXFn6e3H0HyrjvKK3ljOzE/vnUuIzjXBgv3p3jOftPst/nmh3OjJwM/WaTjcNzJ3KbrJ93cIbWjcxbuvtx388RuNs9odDQUGzbtg3FxfWMMBEREXj77bfx+OOP25xnNpthMtluclqrjtAcWmTRHjFiBAoKmg59bN++HcHBwRg5cmSz4y9evGjz/61bt2Lnzp3Wi70dbc9XX31lQ9sTEhLSqi/VGvzWeYLDPqslKD34hOiNUq7KrdjM7Ra/V3Avy8Q6bvfN6vsYyPLey2Q/UyWZL1FxPwsGunPeBQDs1HPibWVkXu8lJVe1WlDChceuESKBANBbyeUfAaBn9hF6LACMCxnX6nNT8lPsmuuvhntWyuFQ4NSWT7oFMZ1aLnlsDmeucr1CmS5cddUgLbegfOvKrbZ/J0klO/XgFrATmVwO6oGeHJ3/wRwud3XeiSzYIENjAKAlKbi9SYN5n5ELxcrJysgiMs/mL+U3g/0L99JjAWCMACP09T1mhO5ZFm0zEbKoIVVA7cGix7mE+P7PuV1mpokL443Wc93vp7O46xzTuZAal5LNGZMeJm6hPSzlxqkBuEm411NIY+TN6ODEhdWeeZHLl/3+HkcG7ELum6OX8bpe9uKvwJjQXrhnjVC0HyctrPDgGt58qrncR/qnnOHztXA78C5yzpi4yUjuOAP3cv5xWXhODwC6STijwLKSP6dzxs+unGviRwrbVZIhzg5m7jorvshp+aQmYDRzxstA5rwMpy62fFIb4W7PCbUn7lkjdLrET/CYweGFKMoTvnOv1TvjOoTHsQOgxwP/we3ejr1WjPMuwm+vB0zIMAmPnSuc5SjUCi9O8Jbp8b9E4cZIiQYduwgvmS/Nd8dPOuGLn4sF6C0V7pV+KVVCbRHOejHErMQRifD78LDFCz9buHzSQxIfHJcI/45PuZuhnDlU8LjK106jpEr4vTdIJKiTCjdE5loTXB7oInic7tQlwWNuhYlUOr4X0GJOqCU9oVtx7tw5/O1vf4NCoUBGRobN3wwGAzZs2IB9+/ahqqoK0dHRWLFiBXr2tKUHLC4uxltvvYWffvoJEokEw4cPx/Lly5skU2WREzVG8Bg3T7LbvoSXn2B7IjqYuXj71wruZZlSx41TycnflCTN9HHjPKFvTVyvF8mChMF6rpQcAL6TczkTdq8+mJR2dye953NSrhJzpC9PCts5M5UeCwDDO41q9bk/Xv3Wrrn+arBbT+hmmM1mvPbaa/D29kZtbeNS39WrV2P//v1YunQpgoOD8fHHH2PmzJk4ePCgtVnVaDRizpw5MBgMeOedd2A0GrF27VrMnz8fn332Wauan1qDP0qF74aj5Fy+JLeSb5Kb+Dg355F9nPGqtHDJWwVZPZZh4cJ/o0O4AoOcfK7k/X5CfwoATsu5cOo3pCEBgBsSzij4kyHHfs9wv81vu7hAjI+R2/B49ycp4h2Av4KoXXvBbj2hmxtTd+/ejaqqKjz++OPYsWOHzedcv34dn3/+OVasWIEnnngCQH1P0MiRI7Ft2zYsXrwYAPDNN9/gwoULOHz4sLVT19/fH1OnTsXRo0etxHr2ooQQxioo4nbDIW4cCSkA7DrELZr+pPv/Elmme1zGGdo4kl6orJin5WdQbeGsLFup1lvHL1qlTpyXGGbg7v2Fz7kv2bUbt8GS5nAbrFP7eeaSER/RQwH8NUTt2gsO0RMCgLKyMqxfvx5r167FmTNnGo05duwYTCYTHn74YesxpVKJ+Ph4HD161GqE0tLSEBkZaUMVERcXh+DgYKSlpTnMCBUSm7BgI7cQnSRDOQDw/LvNU2bcDm8u4ZKw+widJQCIJjeZZyxcaOVvsVyzas5JzvO6358rz0+r5AooUhScQbAHP5Dhsc1PcEbo6HYu38nkOgFg9jCO6skREAsTmofD9ITeffddxMXFYejQoU0aIbVaDV9f30a8cxERETh8+DDMZjOkUinUajUiIiIajY+IiEBubi5zuU0iQi/8ofCSc/mEYRYtivRcXsiU3vi3bA166rnF/QrJMNSHzAsUkGXB1QXcQlRFJLQBoPA6t5EoI42JPTQvetIL7iDh7oXUn2uo7iDhuBijyXYf5wjhxUiOgmiEmodD9IQyMjLw5Zdf4vDhw82Oa04nqEOHDjAYDKitrYVSqbytnlCDMJ4jcMNJ+GIUQuqt2IOr+zjDF2rmrrUXOOOlIEufu5q4BbOihLvO7qT4nqucLEEnc14A4EquW2RtCa1hVLGPqx6rsHCeUDdvjvkgezvPhRS7quVzbgexOq552K0nZDKZ8Prrr2PmzJkOpdVpawQbhD8URVCgdwDXYFdaxOV2goZwHkba11xCPJs0Jn3IMJ6OlCEP9eS43FTQIrOI2BHrXaiy4DBYcE7O7oK53+ZHKcdC0VHKeetThnD9PsEFXK40rYoLcU59kmv8dgTEZtXmYbee0O7du1FcXIynnnoKGk29e63T1fvLN6upqlQqVFU1fggqKyvh7OxsVe5r7jxH6wl5E+JmcqkZN4qF72xNFgk8pZwxUadyO/5OBrJEW8aVTJtI0bcgGeeZlJVyv0uuUQmm19HJYoGrWfhCstuljspKe0qcUU4WNURJOO+rnCxKKf+eM3ouZOWgTxXnVThNeoIa5wjco+xorYLdekK5ubkoKSmxyi7cjL59+2L69OlYsWIFwsPDUVpaioqKChu1PbVajc6dO1t7jsLDw3H+/PlGn5WTk4Phw4cL/X7NQk9UO1WYuYU23J1XZNWTxRBXnblx8Wauyq2QzCX5syEnMjx2kSyg8DVxA4MkXKn1MM4hBQDsU3AbCRnJROAWyN0Lg5Z7RqtruXtR8G/bqXEAEH7uaXosIOaEbge79YSmTZuGUaNsG7H27duHI0eOYMuWLQgMrI/7Dh48GFKpFCkpKZg6tZ48tKamBt9//70NXfiwYcNw4MABqNVqqxBeZmYmCgoKHFYZBwAS4qHoE8Y1u2Vc4YTUAGD4kxwx6IkvuPmOgCubTTBwu+9MZ67UerQH97vcV8QtBoFSzmP7nWDKAIADdlTHGcj8gxtZmKDoy/Hx5Z3h7qEL4ZECQODE9hO1Ez2h5mG3nlBYWBjCwsJsxpw8eRIymcxGuzwgIABPPvkk3n33XTg5OSEoKAjJyckA6oWSGvDQQw+he/fuWLhwIV588UWYTCasWbMGffr0adLbYlEsFf7CqQru/EMsceUWMU8T99Cz5M0uElLCmnw3JaRYXIkTt4v2NnLjNODCsDo7EtmsR1Np4a5V4sKFRlkvn2UDkXRqPwJTE8TChObQ4pJz7NgxAMDatWsb/W379u02hqYlLFu2DG5ubnj//fettD2ffPKJjQ65k5MTPv74Y7z11ltISEiw0vasWLHCYWwJANDNSfguTOXBxUg8UYe6Os6Y5H/BvXC+pDXpK+OSzE4ky7SPift+pWSzaqiBW2j14BbMSJKFwJ+UVQCAfBlnoD1IjSZ9+mVqnDPZ4Oxh5rxE7Ve/UeMAwG0ePRSAyJhwO9yzekK/Bk8SPIY1QgBwrZzrpdCSix9bdZYj58b11nGLe49Qjs2cldWoruU2A+UGbr5PXfmwWlcLl0+qJL3SXgbuWRvSgasYZbkYv7/MSdCP6Mw3qwYd/4EeCwC9Alq/Wc+6fsKuuf5quGdZtIuNwheV4nIX9Azkciasomc52ZoUwkokkMJfvcly4vJiLpRzVs9VSkZKuLJgTyc9DIS0wlwd8JKUY7X2dOZyif5kuXwHsmeruoZ7tnMI/kYAuE/BF/q0F0RPqHncs0bIQsTN3WHElSLh0slayBBIlr/qSGEzFRk3jyJlwWvI3XdwNLegVJ/iPJpiE/f9PCTc/Vsj18IXwkOHQySeNOEY23SqkXEbiVo9F3KMCuE8qPyrnPEqzONbPDgd3z8h9gk1jxZXuNZKOeh0OmzevBkHDhxAUVERvLy8MGzYMLz55ps2n7d161bs2rULJSUliIiIQEJCAgYOHGhzTnV1NdasWYOvv/4aer0e/fv3x8qVK9GpE1eF0xROuAp/4aa7cQvmuTKuURUASsj4flfSEwrTswUGZKFANucJachyeR+yD4qVDwhpQu6kNYjU8onsq87cnGyRCAuPMG6j5HqdC/2eM/PsFQ/QI+shekLNwyFSDmazGfPnz8fly5fx/PPPIywsDEVFRTh37pzNZ23duhXr16/HokWLEBUVhT179uC5557Dnj170KNHD+t5L730ErKysvDKK69AqVRiw4YNmDlzJg4dOgRXV16b52aM0gpfjKQe3IMkt4uyg2Vv5haiX0kFgfE6bnH3686xaOf8ItwjBQBPFVdq7aLhjFAduIU2R85TzLAaRmxYRK/nnrXLv3KeyWUDV5QSRRQjOQoibU/zcIiUw//8z/8gMzMTR44csal0e/TRR63/1uv1SEpKwvTp0zF79mwAQL9+/TBhwgQkJSUhMTERAHDmzBn8+OOP2Lx5s7UvKDIyEqNHj8bevXvx9NP2NY01oJJ45VxKueICFztccReyYsndQobjSLE4T1IszjmYXNxJD+NqOVeRdUXB3UMDuQMm+z8BAOfl3IKnJPN6EZ25PGnZdc6YBBBsJwBw2cTLf9jrCd3t4bgRI0agoKDpwo333nsP48ePb3bsM888g5MnTzY6/sUXXyA6OrrFuR0i5bBnzx6MHTvWxgDdivT0dFRVVdl8GZlMhnHjxiE5ORkWiwUSiQRpaWnw8PDAkCFDrOcFBQUhLi4OR48edZgRMhM5oU5hHF8ZABy/3JEapyH7YUxkr8jPci7U0V3HhcdKj3OFAuxuv1d3ruE4N5cLBRcQulUAUGpHttbDwhkTN3KdvHiZY6dmn1EZuaDfp+KITx0By13uCX3wwQfQ622N+8cff4wff/wRgwYNanF8XFxcI5HTBrKBlmC3lIPBYMDvv/+O4cOHY8mSJfjmm29gsVgwaNAgrFy5EkFB9Sm9BgbsWy8sIiICtbW1uH79OgIDA6FWq9G1a9dG0uERERHWniVHIJzI77h35efrkMt5JhEGbjWqlHLj3MmmOj3JMF5czHmXbH+R2UhS05BriAcZTi2UmhBEMqHryUXamfS6VU6cZ2Im58uzcCH5Pj15cUl7cbfT9kRFRdn832QyITMzE4MHD24kv9MUVCoVYmNjqbntlnIoKyuDwWDAli1bEBcXhw0bNkCj0WDdunV47rnnsH//fjg5OdmQmd6MBlLSiooKBAYG3lbKobKSI0psCtdqhLvm1751p/ta7vQ+iK2O8yHLe1n4B3DMxvpCboE2kvkLdzN3BwdqJThGhvJYOJMeBltc4kLy+Mnl3DPqXklWRl7kQr8AwGm5/om/WjvmL7/8guLiYpuUSlvBbikH8/+9nO7u7ti0aZPVyISGhmLKlClITU3FuHHjHHzZ9qNGInwRU1mMqK4QXuKbr1FRIQQLJIiScMnUM85KRBqF52n2GAoxRC68IDXTyRWDZcI3Cd/cCEQfCP+Ovp41uF4h3Is6fL0jJSneQ1WBUqI4YZ2LHr0JTaEzFg06SITnr9ItGowwcwn/vc7VuB/Cr3V0Tw21y0rP7AgDYTDLZTLEKoSHxrt8tQql0xIEj/PZ2ZgtRijudk/oVhw8eBBKpRIjR45s1fknT55Enz59YDQacd9992HhwoWNqp6bg91SDiqVChKJBHFxcTZeTnR0NJRKJbKzszFu3DioVCro9XrodDq4uPy5kDd4Nw3M2iqVCteuXWs0v6OlHJgy1gdMRlTWCC8fKyLzAgAweBmXi/jlXxU44yz8WrtLfXHDIjy80tvshHRi8TNLgNMQbkz+1r0cfhBuTPJ/DYGaEO7rWWeCq7PwfJk7nKCG8Io8X6kCeUQC3leiwCliPgDwhhyXILxZ2W1sT2q+TjmcRLumzhNntcKftdDVK+DeS7gXpV29AvKPvhY87maYBHjSe/fuxb59+1p9/qRJkzB58mTmspqEVqtFamoqxowZY7NWN4e+ffvi0UcfRefOnVFSUoJt27Zh1qxZSE5ObpUhslvKwdXVtRGzdgMkEolVW6ghF6RWq23ij2q1Gu7u7taihvDwcBw/ftxaqNCAnJwcdO1qR1LmFgQYhe9Makk+to4kiSUA1Ow/S47kBAbvI5VV853IKjATFzrSlXJhtTKSgSKPLAvuSIbiutqh4ntMypW9y0huRtP5y9Q4nxCuorI2h2tWrfuDp92yl7pYSHVcQUFBk9VmzaFfv36NjlVVVeHGjRstjg0KCmrU9vLdd9+hpqam1aG4hQsX2vx/5MiRePTRR/HBBx84xgi1JOUAAPHx8UhJSYFWq7V6Q7/99huqqqrQq1cvAPXVEx4eHjhy5IjVCJlMJqSkpGDIkCFWgzNs2DBs2rQJP/30k5U1+9q1a0hPT8fy5ctb/EKtRaGz8BeutxuXv0jX8BHlWRe5BrtFei5ZfJho4gWAmeCSvl2f4eL7n+zk+MOeiuV233szOKNeBlKenSxoAIBBZm4joSQTl9u+5JROu+q5XFIwyT6yJ4dXfv4HPbIeQnJCwcHBTRqW251/K1JTU7Fs2bIWxzZFQn3w4EEEBgYKIqe+GXK5HCNHjsSuXbtadb7dUg5KpRKzZ8/GwYMH8fzzz2PmzJnQaDRYv369tZgDyOYAABF+SURBVL+n4cLmzZuH9evXw9vb29qsmpeXh3Xr1lk/MyYmxsqavXTpUiiVSiQmJqJjx44OdTmHSYRXx7mQGi8+5Twl8lIz1z1aQS5iRRZu0ZSSzA65n3HGspee+35nTnJ8bH4kLRErjZEr4xuF2Oo4BUnbs7xPITXujxMck0iNiWsHmP4wRxPkCAjJCU2ePNnutY79jLKyMhw7dgwzZsxoVKHcVmiRRft2TUw3W9ELFy5g9erVyMzMhFwux7Bhw7B06VL4+trukrZu3YqdO3eipKQE3bp1uy1tz1dffWVD2xMSwu9kbsUnwdMEj/EmCR4BTk4cAHYquBDJAwRBKwCQUTV63/63d1vXS3Arvl6UTY1jK8Auyblv+CUpEggAYyXcIk3uB2jOOS9SuyrAwpHlVpEVnHqSRQQAHiv6lB4LAL6qyFafW6L5w6657MGuXbvwxhtv4MCBAzYsNkKg1+sxYcIE+Pv7Y8eOHS2ef89KOXwWJLzptaOJMyQAYCK70W/IuBfO38TtpK/JuF1md5KdOmIA10B4/ji3QEsl3ON+huSOy3Li84H9SWJQljHBj2AJB4CJ3lwDsMyZu86sPK451seJM3oA0Leg9YUCTcFLGdHqc8urc+yayx78/e9/R21tLQ4dOtTk35cvX479+/fj999/BwCcOnUKH3/8MUaPHo3g4GCUlJRg+/btyMrKQnJycqtCevcsizbTrX1V5oLecq5X6VcjV9n3mxNnTIbouFv7K9nz0UXLeQouA1v/ct6Mov/lku+wAAMjGldftoQwlOMIoWMTY3TGf0u43jI/Z9LQkn1CbADw1HV/alykC0cIfI2tNrWDCsle/BVKtPPy8pCZmYmXX3652XPMZjNMNzWK+/n5wWAwYP369aioqIBCoUBMTAy2b9+O+++/v1Xz3rOe0JcBUwWPiQ5pudqkKWTnc4lbADip4HbDQ3VcbucnFy4H9YiMozTy78IVe5w6x9EgRYdyeYFvrnHzZZGbiGA2mQSALDik+eqiZdw9lJBeaamWY0xwl/Fe6YDCvfRYAFAJoFvR1OTaNddfDfesJxTmLvzFKSMlpSOC+bzA78WB1LgScFVnlyRcyIJhoAAAj3LOWFbKOM+LDeUUkqXWbmQY9jEVn0TfVcVtejKcuQIa/zrOKLDqv+VO3LhQoi3DURClHJrHPWuErtQIL33u1ZELq/x6javIAoByOdlnQj70HuQjoZJxxqtDd27hcyvkvl83Py4HdaWCM15pMi5smFrJzQcAIElv7zNy995MxrkCpNwGxGDmjJ6nnM8J2Yu7nUW7PXHPGiEzEbIovM61rEnt2AUFm7hdn45sPIwg56sxcY+SRM7N50d6bIUlXN+VDxk60ki5YpZLMlLYCYA7yaJdThaPDfPhDHtdHeetd6jlQtROsvZjshY9oeZxzxohplS3wsI9/AqYESjnqFTc9Nyuj6W7ryYrpBguPgCoyeYWBgWZa8kgeNwAwNfIXWeglLt/KtKQAICK2WEBKCc9KBd37l6w1XEepGxI55E8Y4K9MN/lUg7tiXvWCJUTOYUICVmRBZ72pZiUau5t5q71CMEkAQBzFVxyWtmTewTPXOGMSaiRS07nOXMLX76JK13PRw26yzjP20XCXWsN2ZDLGpOiQu77qVw5L7jyN94Q2E3bI3pCzeKeNUIuZuEPRT5c0UXOLSrOZFK0mtwQVxu4hSiUrMrS6LnmWLOW8xCDjSR7hZzbDftAi1yjcKLVufDGAdIL9iRfT1bwL4zkqyu9xmlCKch2gKJqbkOnqeOeUYBlYvwTohFqHvdsibYIESJEiGh/3BlyIBEiRIgQIaIJiEZIhAgRIkS0G0QjJEKECBEi2g2iERIhQoQIEe0G0QiJECFChIh2g2iERIgQIUJEu0E0QiJEiBAhot0gGiERIkSIENFuEI2QCBEiRIhoN4hGSIQIESJEtBtEIyRChAgRItoNohESIUKECBHtBtEIiRAhQoSIdoNohESIECFCRLtBNEIiRIgQIaLdIBqh/8Ply5cxe/Zs9OnTBwMGDMCqVatQV8eJkbWElJQUzJ8/H8OGDUNsbCwmTJiATz/9FGbznZEArqmpwdChQ9G9e3ecPXu2Tefav38/Jk+ejN69e6N///549tlnUVZW1iZzffvtt5gyZQr69OmDBx98EC+88AIuX77skM++cuUKXn31VUycOBFRUVF45JFHmjwvLS0NkyZNQnR0NEaNGoUdO3a0yXwmkwlbtmzBtGnTMGDAAPTt2xdPP/00fv755zaZ71acO3cOPXv2RJ8+fdp0Pp1Oh40bN2LUqFG47777MGTIEKxcubJN5jMajdi8eTPGjh2LmJgYjBgxAm+//Taqq6sFzyei9bhnlVVvhkajwfTp0xEUFITExESUlZVh9erVKCsrw/r16x0+3yeffIKgoCAsXrwYPj4+OHHiBN566y3k5+djyZIlDp/vVnzwwQcwmUxtPk9SUhI2b96M5557DkuWLEFVVRVOnDgBg4GT2L4dfv75Z/zjH//Ao48+ikWLFkGj0eCDDz7As88+i0OHDkGp5NQ/G5CdnY20tDTExMTAbDY3qZSZkZGB+fPnY+LEiViyZAnS09Px9ttvw8nJCVOnTnXofFqtFh999BEee+wxzJ49G05OTti3bx+effZZJCUlIT4+3uHfrwFmsxmvvfYavL29UVvLyci3Zj6z2Yz58+fj8uXLeP755xEWFoaioiKcO3euTebbtGkTNm/ejBdeeAGxsbFQq9VYv349CgoKsGnTJup7imgFLCIsH330kSUmJsZSWlpqPXbw4EFLZGSk5Y8//nD4fDfP04C3337bEh0dbdHpdA6f72ZcvHjREhsba/n8888tkZGRlt9++61N5lGr1ZaoqCjL999/3yaffyuWL19uiY+Pt5jNZuuxM2fOWCIjIy0//vij3Z9vMpms/16yZIll/Pjxjc6ZPXu2ZcqUKTbHVq5caXnwwQdtxjtiPqPRaKmoqLA5ZjabLZMmTbJMmzZN0Fytme9mfPbZZ5aHHnrIsm7dOktsbKzguVo73+7duy1xcXGWoqIiag6h840aNcqyePFim2ObN2+29OjRw1JTU2P3NYhoGmI4DsDRo0cxYMAAeHt7W4+NGTMGcrkcR48edfh8N8/TgJ49e0Kn06GiosLh892MN954A08//TQ6d+7cpvPs3bsXQUFBgnfkLIxGI9zd3SGRSKzHPDw8HPb5UuntXxW9Xo9ffvkFDz/8sM3xRx55BMXFxcjKynLofDKZDB06dLA5JpFI0KNHD9y4cUPQXK2ZrwEN0YEVK1bA2dlZ8DxC5tuzZw/Gjh2LgIAAeh4h8xmNxkbPjEqlgsViua1nKMI+iEYIgFqtRkREhM0xuVyO0NBQ5Obm3pFrOH36NDw9PeHj49Nmc+zfvx9XrlzBvHnz2myOBpw5cwbdu3fHhx9+iAcffBC9evXClClTcPLkyTaZb9KkScjNzcWOHTug0Whw9epVvPPOOwgPD8fAgQPbZM6bkZeXB4PBgPDwcJvj3bp1A4A78hyZzWZkZGQ0ugZH4t1330VcXByGDh3aZnMAgMFgwO+//47g4GAsWbIEffr0QWxsLObPn4/CwsI2mXPKlCk4cOAAjh8/jpqaGpw9exbJycmYNGkS3N3d22ROEWJOCEB9TkilUjU6rlKpUFlZ2ebznz17Fnv37sWCBQsgk8naZI6qqiqsXbsWS5YsuSMvVHFxMc6dO4cLFy5gxYoVUCqVSE5Oxpw5c3DkyBF06tTJofMNGDAAGzduxMsvv4w333wTABAZGYlPPvkEcrncoXM1hYbn5NbnqOH/d+I52rFjBy5duoRVq1a1yednZGTgyy+/xOHDh9vk829GRUUFDAYDtmzZgri4OGzYsAEajQbr1q3Dc889h/3798PJybHL14IFC2AymTBr1iyr5zN27Ng2+z1F1EP0hNoZxcXFWLhwIaKjozF37tw2m+f9999HWFgYHn300Tab42ZYLBbU1tZi48aNePjhhzF06FAkJSVBqVRi69atDp8vPT0dS5YswZQpU7Bt2zYkJiZCIpFg3rx50Gq1Dp/vbsPJkyexdu1azJo1Cw888IDDP99kMuH111/HzJkzERIS4vDPvxUNlaLu7u7YtGkThgwZgvHjxyMxMRHZ2dlITU11+Jw7d+7E9u3bsXTpUuzcuRP/8R//gV9++cW6qRHRNhA9IdTvVjUaTaPjGo0GXbt2bbN5q6qqMHfuXCgUCiQlJdkVY78dsrOz8fnnnyM5Odn6PRuqmmpra1FdXW139ditUKlU8PT0RM+ePa3HXF1dERMTg+zsbIfOBQBvvvkm+vfvj+XLl1uPxcbGYvjw4Thw4AD+/ve/O3zOm9GQn7n1OWr4/635G0fiwoULmD9/PkaNGoWEhIQ2mWP37t0oLi7GU089Zf1OOp0OQP13lMvlUCgUDptPpVJBIpEgLi7O5nOjo6OhVCqRnZ2NcePGOWy+8vJyvPPOO0hISMD06dMBAH379oVSqURCQgJmzJiBLl26OGw+EX9CNEIAwsPDoVarbY7p9Xrk5eVh8uTJbTKnTqfDvHnzUFpais8//xxeXl5tMg9Q3yNhNBqtL9fNmD59Onr06IEDBw44dM6IiAjk5eU1+beGxcuRUKvVGDFihM2xwMBAeHl5NXsdjkRoaCicnZ2Rm5trky/JyckBgDbbzOTl5WHOnDmIiorCmjVrbAozHInc3FyUlJQ0mQvq27cvpk+fjhUrVjhsPldXVwQHBzf5N4lE4vBnKD8/H3q93mbTBABRUVEA6n9n0Qi1DUQjBFhDReXl5VZjkJqaCr1ej2HDhjl8PqPRiH/+85+4ePEiduzY0ezL5ijExcVh+/btNsfOnz+P1atX4/XXX0evXr0cPmd8fDz27t2LrKws6+fX1tYiMzMTY8aMcfh8QUFBjSrQCgoKUF5e3ua/L1BfyDJgwACkpKRg5syZ1uOHDx+Gn59fm/zGxcXFmDVrFnx9ffHhhx+2ae5r2rRpGDVqlM2xffv24ciRI9iyZQsCAwMdPmd8fDxSUlKg1Wqt3tBvv/2Gqqoqh/+eQUFBAICsrCz07dvXeryhJ8nROUwRf0I0QgCefPJJ7Ny5E/Pnz8f8+fNRWlqKf/3rX3j44YcbVc05Am+88QZ++OEHJCQkQKvVIjMz0/q3iIgIh4fGvL290b9//yb/1qtXL0RHRzt0PgAYNWoUevfujYULF2LRokVwd3dHcnIytFotnn32WYfP9/TTT2PVqlVYtWoVRo4ciYqKCiQlJcHHx8chYZu6ujqkpaUBqDdu1dXV+OqrrwDUh4iCg4OxYMECTJs2DStXrsSECROQnp6OPXv24NVXX211CXRr5/Px8cGcOXNQWlqKpUuXWj2uBsTGxjp0vrCwMISFhdmMOXnyJGQyWbPPlj3zBQcHY/bs2Th48CCef/55zJw5ExqNBuvXr0dkZCRGjx7t8PnGjBmDxMREmEwm9OrVCzk5Odi4cSMGDRrUphWH9zokFrEAHgBw6dIlvPnmmzh9+jRcXFwwfvx4JCQkwNXV1eFzjRgxAgUFBU3+bfv27dRLLRQnTpzA9OnT8cUXX7SJEQLqe0rWrFmD7777DjqdDjExMVi8eHGbzGexWLB79258+umnyMvLg7u7O2JiYvDiiy86ZAG5evUqRo4c2eTfVq9ebQ3bpqWl4b333oNarYa/vz9mzpzZZBjU3vn69evX7N8B4OLFiw6dr6mw9MaNG5GcnIyMjAxBcwmZ78KFC1i9ejUyMzMhl8sxbNgwLF26FL6+vg6fr7q6GklJSUhNTUVRURH8/PwQHx+PF154oU1zevc6RCMkQoQIESLaDWKJtggRIkSIaDeIRkiECBEiRLQbRCMkQoQIESLaDaIREiFChAgR7QbRCIkQIUKEiHaDaIREiBAhQkS7QTRCIkSIECGi3SAaIREiRIgQ0W4QjZAIESJEiGg3/D/57vjL3jWaCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(y_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_report(y_pred,\n",
    "                    y_true, \n",
    "                    labels=None,\n",
    "                    results_dir: str=None):\n",
    "    \"\"\"Create a performance report for the current experiment and \n",
    "    consolidate the information to a general report of all runs \n",
    "    Parameters\n",
    "    ----------\n",
    "    opt : sklearn.model_selection.Object\n",
    "        A hyperparameter \n",
    "    X_test: numpy array or pandas Dataframe \n",
    "        Input test data\n",
    "    y_test: numpy array or pandas Dataframe \n",
    "        Target test data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    logger.info(\"Generating Evaluation Report:\")\n",
    "    \n",
    "\n",
    "    res = classification_report(y_true, y_pred, labels=labels, output_dict=True)\n",
    "    res = pd.DataFrame(res)\n",
    "\n",
    "    logger.info(\"Test report:\")\n",
    "    logger.info('\\n \\t'+ res.to_string().replace('\\n', '\\n\\t'))\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, labels=labels, average='macro')\n",
    "    \n",
    "    steps= [*pipeline.named_steps]\n",
    "\n",
    "    cv_mean ,cv_std = opt.best_score_,opt.cv_results_['std_test_score'][opt.best_index_]\n",
    "\n",
    "    tmp= pd.DataFrame({\"Scaling\":[steps[0]],\n",
    "                        \"Model\":[steps[1]],\n",
    "                        \"params\":[opt.best_params_],\n",
    "                        'CV Mean':[cv_mean],\n",
    "                        'CV Std':[cv_std],\n",
    "                        'Test dataset':f1,\n",
    "                        })\n",
    "\n",
    "    if os.path.exists(path+\"/results.csv\"):\n",
    "        current_csv =pd.read_csv(path+\"/results.csv\")\n",
    "        pd.concat([current_csv, tmp], \n",
    "                   ignore_index=True\n",
    "                 ).to_csv(path+\"/results.csv\",\n",
    "                          index=False)    \n",
    "    else:\n",
    "        tmp.to_csv(path+\"/results.csv\",\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = LightningClassifier(backbone_name='gluon_seresnext50_32x4d',\n",
    "                            pretrained=True,\n",
    "                            num_classes=19,\n",
    "                            pool_size=1,\n",
    "                            pool_type='avgmax',\n",
    "                            head_type='linear',\n",
    "                            hidden_size=None,\n",
    "                            lr=2e-03,\n",
    "                            weight_decay=0.01,\n",
    "                            seed=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bqplot.pyplot as plt\n",
    "# from dataclasses import dataclass\n",
    "# @dataclass\n",
    "# class LRTunerConfig:\n",
    "    \n",
    "#     min_lr: float = 1e-08\n",
    "#     max_lr: float = 1.0\n",
    "#     num_training: int = 50\n",
    "#     mode: str = 'exponential'\n",
    "#     early_stop_threshold: float = 4.0\n",
    "\n",
    "# cfg = OmegaConf.structured(LRTunerConfig())\n",
    "\n",
    "# lr_tuner = trainer.tuner.lr_find(model,\n",
    "#                                  data,\n",
    "#                                  **cfg)\n",
    "# lr_tuner_results = lr_tuner.results\n",
    "# best_lr = lr_tuner.suggestion()\n",
    "\n",
    "# suggestion = {\"lr\": best_lr,\n",
    "#               \"loss\":lr_tuner_results['loss'][lr_tuner._optimal_idx]}\n",
    "\n",
    "# plt.figure()\n",
    "# fig = lr_tuner.plot(suggest=True)\n",
    "# lr_tuner_results_dir = os.path.join(results_dir, f\"task_{task_id}\", \"lr_tuner\")\n",
    "\n",
    "# plot_fname = 'lr_tuner_results_loss-vs-lr.png'\n",
    "# plot_path = Path(lr_tuner_results_dir) / plot_fname\n",
    "# plt.title(f\"Suggested lr={best_lr:.4e} |\\n| Searched {lr_tuner.num_training} lr values $\\in$ [{lr_tuner.lr_min},{lr_tuner.lr_max}] |\\n| bsz = {config.data.batch_size}\", style={\"fontsize\":'small'})\n",
    "# fig.save_png(filename=str(plot_path))\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.plot(x=lr_tuner.results['lr'],\n",
    "#          y=lr_tuner.results['loss'],\n",
    "#         figure=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Display available global pool types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aside: Verify task_0 and task_1 label maps all agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_0_labels = data.label_encoder\n",
    "\n",
    "data.setup(stage='fit', task_id=1)\n",
    "\n",
    "task_1_labels = data.label_encoder\n",
    "\n",
    "task_0_labels\n",
    "task_1_labels\n",
    "\n",
    "task_0_labels\n",
    "print(f\"label|task_0_idx|task_1_idx\")\n",
    "for label, idx in task_1_labels.class2idx.items():\n",
    "    print(f\"{label}|{task_0_labels.class2idx[label]}|{idx}\")\n",
    "    \n",
    "    assert task_0_labels.class2idx[label] == idx\n",
    "    \n",
    "print(f\"Success, all labels in task_1 have identical integer mappings to their corresponding values in task_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2idx = data.label_encoder.class2idx\n",
    "family_counts = df.value_counts(\"family\").to_dict()\n",
    "\n",
    "df = df.assign(class_idx=df.family.apply(lambda x: class2idx[x]),\n",
    "               score = df.family.apply(lambda x: family_counts[x]))\n",
    "\n",
    "df#.clear_intent()\n",
    "\n",
    "df.groupby('class_idx').mean()\n",
    "\n",
    "# df.exported.keys()\n",
    "\n",
    "df.exported['Distribution']\n",
    "\n",
    "df.compute_metadata()\n",
    "\n",
    "df\n",
    "\n",
    "df.clear_intent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['backbone_name', 'pretrained', 'num_classes', 'pool_size', 'pool_type', 'head_type', 'hidden_size', 'backbone', 'feature_size'])\n",
      "dict_keys(['backbone_name', 'pretrained', 'num_classes', 'pool_size', 'pool_type', 'head_type', 'hidden_size', 'backbone', 'feature_size'])\n",
      "dict_keys(['backbone_name', 'pretrained', 'num_classes', 'pool_size', 'pool_type', 'head_type', 'hidden_size', 'backbone', 'feature_size'])\n",
      "backbone=gluon_seresnext50_32x4d|pretrained=True|num_classes=1000|head_type=linear|hidden_size=0\n",
      "pool_type=avg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'classifier.weight'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'classifier.bias'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span><span style=\"font-weight: bold\">])}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'classifier.weight'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m, \u001b[1;36m4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'classifier.bias'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool_type=max\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'classifier.weight'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'classifier.bias'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span><span style=\"font-weight: bold\">])}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'classifier.weight'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m, \u001b[1;36m4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'classifier.bias'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool_type=avgmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'classifier.weight'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'classifier.bias'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span><span style=\"font-weight: bold\">])}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'classifier.weight'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m, \u001b[1;36m8192\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'classifier.bias'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backbone_name='gluon_seresnext50_32x4d'\n",
    "pretrained=True\n",
    "num_classes=1000\n",
    "\n",
    "head_type='linear'\n",
    "hidden_size=0\n",
    "\n",
    "pool_types = [\"avg\", \"max\", \"avgmax\"]\n",
    "\n",
    "models = OrderedDict({})\n",
    "\n",
    "for pool_type in pool_types:\n",
    "    models[pool_type] = build_model(backbone_name=backbone_name,\n",
    "                                    pretrained=pretrained,\n",
    "                                    num_classes=num_classes,\n",
    "                                    pool_size=1,\n",
    "                                    pool_type=pool_type,\n",
    "                                    head_type=head_type,\n",
    "                                    hidden_size=hidden_size)\n",
    "print(f\"backbone={backbone_name}|pretrained={pretrained}|num_classes={num_classes}|head_type={head_type}|hidden_size={hidden_size}\")\n",
    "for pool_type, model in models.items():\n",
    "    print(f\"pool_type={pool_type}\")\n",
    "    pp({k: v.shape for k,v in model.head.named_parameters()})\n",
    "#     pp(list(dict(model.head.named_parameters()).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Display available head types (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['backbone_name', 'pretrained', 'num_classes', 'pool_size', 'pool_type', 'head_type', 'hidden_size', 'backbone', 'feature_size'])\n",
      "dict_keys(['backbone_name', 'pretrained', 'num_classes', 'pool_size', 'pool_type', 'head_type', 'hidden_size', 'backbone', 'feature_size'])\n",
      "dict_keys(['backbone_name', 'pretrained', 'num_classes', 'pool_size', 'pool_type', 'head_type', 'hidden_size', 'backbone', 'feature_size'])\n",
      "backbone=gluon_seresnext50_32x4d|pretrained=True|num_classes=1000|head_type=linear|hidden_size=0\n",
      "pool_type=avg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'classifier.weight'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'classifier.bias'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span><span style=\"font-weight: bold\">])}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'classifier.weight'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m, \u001b[1;36m4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'classifier.bias'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool_type=max\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'classifier.weight'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'classifier.bias'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span><span style=\"font-weight: bold\">])}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'classifier.weight'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m, \u001b[1;36m4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'classifier.bias'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool_type=avgmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'classifier.weight'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'classifier.bias'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span><span style=\"font-weight: bold\">])}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'classifier.weight'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m, \u001b[1;36m8192\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'classifier.bias'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backbone_name='gluon_seresnext50_32x4d'\n",
    "pretrained=True\n",
    "num_classes=1000\n",
    "\n",
    "head_types=['linear', 'custom']\n",
    "hidden_size=0\n",
    "\n",
    "pool_types = [\"avg\", \"max\", \"avgmax\"]\n",
    "\n",
    "models = OrderedDict({})\n",
    "\n",
    "for pool_type in pool_types:\n",
    "    models[pool_type] = build_model(backbone_name=backbone_name,\n",
    "                                    pretrained=pretrained,\n",
    "                                    num_classes=num_classes,\n",
    "                                    pool_size=1,\n",
    "                                    pool_type=pool_type,\n",
    "                                    head_type=head_type,\n",
    "                                    hidden_size=hidden_size)\n",
    "print(f\"backbone={backbone_name}|pretrained={pretrained}|num_classes={num_classes}|head_type={head_type}|hidden_size={hidden_size}\")\n",
    "for pool_type, model in models.items():\n",
    "    print(f\"pool_type={pool_type}\")\n",
    "    pp({k: v.shape for k,v in model.head.named_parameters()})\n",
    "#     pp(list(dict(model.head.named_parameters()).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borrowed from fastai2 library\n",
    "\n",
    "bn_types = (torch.nn.modules.batchnorm.BatchNorm1d,torch.nn.modules.batchnorm.BatchNorm2d,torch.nn.modules.batchnorm.BatchNorm3d)\n",
    " \n",
    "def set_bn_eval(m:nn.Module)->None:\n",
    "    \"Set bn layers in eval mode for all recursive children of `m`.\"\n",
    "    for l in m.children():\n",
    "        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n",
    "            l.eval()\n",
    "        set_bn_eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,model,train_dl,valid_dl,loss_fn,opt,device=None,bn_eval=False):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write(['epoch','train_loss','valid_loss','trn_acc','val_acc'],table=True)\n",
    "    model.to(device)\n",
    "\n",
    "    for i in mb:    \n",
    "        trn_loss,val_loss = 0.0,0.0\n",
    "        trn_acc,val_acc = 0,0\n",
    "        trn_n,val_n = len(train_dl.dataset),len(valid_dl.dataset)\n",
    "        model.train()\n",
    "        if bn_eval:set_bn_eval(model)\n",
    "        for xb,yb in progress_bar(train_dl,parent=mb):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(out,yb)\n",
    "            _,pred = torch.max(out.data, 1)\n",
    "            trn_acc += (pred == yb).sum().item()\n",
    "            trn_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        trn_loss /= mb.child.total\n",
    "        trn_acc /= trn_n\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in progress_bar(valid_dl,parent=mb):\n",
    "                xb,yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = loss_fn(out,yb)\n",
    "                val_loss += loss.item()\n",
    "                _,pred = torch.max(out.data, 1)\n",
    "                val_acc += (pred == yb).sum().item()\n",
    "        val_loss /= mb.child.total\n",
    "        val_acc /= val_n\n",
    "\n",
    "        mb.write([i,f'{trn_loss:.6f}',f'{val_loss:.6f}',f'{trn_acc:.6f}',f'{val_acc:.6f}'],table=True)        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(model,bn_freeze=True):\n",
    "    for name,param in model.named_parameters():\n",
    "        if bn_freeze:\n",
    "            param.requires_grad = False\n",
    "        elif name.find('bn') == -1:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def unfreeze(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def get_model(lrs=[1e-3,1e-3],bn_freeze=True):\n",
    "    model = MyResNet()\n",
    "    freeze(model.body,bn_freeze=bn_freeze)\n",
    "    opt = optim.Adam([{'params': model.body.parameters(), 'lr':lrs[0]},\n",
    "                {'params': model.head.parameters(), 'lr': lrs[1]}])\n",
    "    return model,opt\n",
    "\n",
    "def update_lr(lr,opt):\n",
    "    opt.param_groups[0]['lr'] = lr/100\n",
    "    opt.param_groups[1]['lr'] = lr\n",
    "    \n",
    "\n",
    "### Freeze the complete resnet body\n",
    "\n",
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=True)\n",
    "fit(2,model,trn_dl,valid_dl,loss_fn,opt)\n",
    "\n",
    "### Freeze the complete resnet body and place BN layers in eval mode.\n",
    "\n",
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=True)\n",
    "fit(2,model,trn_dl,valid_dl,loss_fn,opt,bn_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Freeze the complete resnet body and place BN layers in eval mode and train the body at a lesser learning rate for the second epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.115747</td>\n",
       "      <td>0.044909</td>\n",
       "      <td>0.964774</td>\n",
       "      <td>0.987107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=True)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt,bn_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.074065</td>\n",
       "      <td>0.024733</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.991509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update_lr(lr/2,opt)\n",
    "unfreeze(model)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Freeze the resnet body except fot BN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.105815</td>\n",
       "      <td>0.032677</td>\n",
       "      <td>0.960843</td>\n",
       "      <td>0.988365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.052449</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.982151</td>\n",
       "      <td>0.990881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model,opt = get_model(lrs=[1e-3,1e-3],bn_freeze=False)\n",
    "fit(2,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the resnet body except fot BN layers and try smaller leraning rate for the resnet body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.145204</td>\n",
       "      <td>0.052064</td>\n",
       "      <td>0.951329</td>\n",
       "      <td>0.983648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.064199</td>\n",
       "      <td>0.020451</td>\n",
       "      <td>0.977669</td>\n",
       "      <td>0.993711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=False)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)\n",
    "update_lr(lr/2,opt)\n",
    "unfreeze(model)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try adjusting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveConcatPooling(nn.Module):\n",
    "    def forward(self,x):\n",
    "        avg_pool = F.adaptive_avg_pool2d(x,1)\n",
    "        max_pool = F.adaptive_max_pool2d(x,1)\n",
    "        return torch.cat([avg_pool,max_pool],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.body = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.head = nn.Sequential(AdaptiveConcatPooling(),Flatten(),nn.Linear(512*2,2))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.body(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.124573</td>\n",
       "      <td>0.033491</td>\n",
       "      <td>0.952115</td>\n",
       "      <td>0.987421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.057533</td>\n",
       "      <td>0.032403</td>\n",
       "      <td>0.980893</td>\n",
       "      <td>0.990252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=False)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)\n",
    "update_lr(lr/2,opt)\n",
    "unfreeze(model)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the complexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        nf = 512*2\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.body = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.head = nn.Sequential(AdaptiveConcatPooling(),Flatten(),nn.BatchNorm1d(nf),nn.Dropout(p=0.25),\n",
    "                      nn.Linear(nf,nf//2,bias=False),nn.ReLU(inplace=True),nn.BatchNorm1d(nf//2),nn.Dropout(p=0.75),\n",
    "                      nn.Linear(nf//2,2,bias=False))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.body(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.097272</td>\n",
       "      <td>0.027744</td>\n",
       "      <td>0.966032</td>\n",
       "      <td>0.989308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.056163</td>\n",
       "      <td>0.023533</td>\n",
       "      <td>0.979557</td>\n",
       "      <td>0.992767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=False)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)\n",
    "update_lr(lr/2,opt)\n",
    "unfreeze(model)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "                \n",
    "\n",
    "# ## WIP: display_layer_status\n",
    "#     @classmethod\n",
    "#     def display_layer_status(cls,\n",
    "#                              model: nn.Module,\n",
    "#                              max_depth: int=3):\n",
    "#         \"\"\"\n",
    "#         Return a formatted display of model's layers alongside relevant training status info.\n",
    "#         \"\"\"\n",
    "#         modules = []\n",
    "        \n",
    "# #         for name, module in model.named_modules():\n",
    "#         for name, module in model.named_children():\n",
    "# #             print(name, 'max_depth:', max_depth)\n",
    "#             if name==\"\": continue\n",
    "# #             if (max_depth>0) and (len(list(module.named_modules())) > 0):\n",
    "#             if (max_depth>0) and (len(list(module.named_children())) > 0):\n",
    "#                 modules.extend(cls.display_layer_status(module, max_depth=max_depth-1))\n",
    "#                 continue\n",
    "#             module_out = {\"name\":name,\n",
    "#                           \"training\":module.training,\n",
    "#                           \"type\":type(module),\n",
    "#                           \"params\":[]}\n",
    "#             for param_name, param in module.named_parameters():\n",
    "#                 module_out[\"params\"].append({\n",
    "#                     \"name\":param_name,\n",
    "#                     \"type\":type(param),\n",
    "#                     \"requires_grad\":param.requires_grad,\n",
    "#                     \"shape\":param.shape\n",
    "#                 })\n",
    "#             print(name)\n",
    "#             pp(module_out)\n",
    "#             modules.append(module_out)\n",
    "#         return modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for unfreeze_down_to in reversed(range(0,-8,-1)):\n",
    "# model.freeze_backbone(freeze_bn=False)\n",
    "# model.freeze_backbone(freeze_bn=True)\n",
    "# summary(model.model)\n",
    "# count_trainable_batchnorm_layers(model)\n",
    "\n",
    "# for unfreeze_down_to in range(0,-9,-1):\n",
    "#     print(unfreeze_down_to)\n",
    "#     print(f\"Unfreezing backbone down to layer: {unfreeze_down_to}\")\n",
    "#     model.unfreeze_backbone_top_layers(unfreeze_down_to=unfreeze_down_to)\n",
    "# #     summary(model.model)\n",
    "#     model.count_trainable_batchnorm_layers()\n",
    "# #     count_trainable_batchnorm_layers(model)\n",
    "\n",
    "#     print(f\"trainable parameters: {len(list(model.get_trainable_parameters()))}\")\n",
    "#     print(f\"non-trainable parameters: {len(list(model.get_nontrainable_parameters()))}\")\n",
    "\n",
    "# unfreeze_down_to = -2\n",
    "\n",
    "# model.unfreeze_backbone_top_layers(unfreeze_down_to=unfreeze_down_to)\n",
    "# summary(model.model)\n",
    "# count_trainable_batchnorm_layers(model)\n",
    "\n",
    "# print(f\"trainable parameters: {len(list(model.get_trainable_parameters()))}\")\n",
    "# print(f\"non-trainable parameters: {len(list(model.get_nontrainable_parameters()))}\")\n",
    "\n",
    "# unfreeze_down_to = -3\n",
    "\n",
    "# model.unfreeze_backbone_top_layers(unfreeze_down_to=unfreeze_down_to)\n",
    "# summary(model.model)\n",
    "# print(f\"trainable: {len(list(model.get_trainable_parameters()))}\")\n",
    "# print(f\"non-trainable: {len(list(model.get_nontrainable_parameters()))}\")\n",
    "\n",
    "# unfreeze_down_to = -4\n",
    "\n",
    "# model.unfreeze_backbone_top_layers(unfreeze_down_to=unfreeze_down_to)\n",
    "# summary(model.model)\n",
    "# print(f\"trainable: {len(list(model.get_trainable_parameters()))}\")\n",
    "# print(f\"non-trainable: {len(list(model.get_nontrainable_parameters()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     model = timm.create_model(model_name=backbone_name, num_classes=1000, pretrained=pretrained)\n",
    "#     if isinstance(pretrained, str) and pretrained != \"imagenet\":\n",
    "#         model = load_model_checkpoint(model, ckpt_path=pretrained)\n",
    "# #         ckpt_pth = glob.glob(hydra.utils.to_absolute_path(pretrained))\n",
    "# #         model = model.load_state_dict(torch.load(ckpt_pth[0]))\n",
    "        \n",
    "#     body = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "    \n",
    "#     feature_size = model.fc.in_features\n",
    "    \n",
    "#     head = OrderedDict()\n",
    "#     global_pool, feature_size = build_global_pool(pool_type=pool_type,\n",
    "#                                                   pool_size=pool_size,\n",
    "#                                                   feature_size=feature_size)\n",
    "#     head[\"global_pool\"] = global_pool\n",
    "#     head[\"flatten\"] = Flatten()\n",
    "    \n",
    "#     classifier_input_feature_size = feature_size*(pool_size*2)        \n",
    "#     if head_type=='linear':\n",
    "#         head[\"classifier\"] = nn.Linear(classifier_input_feature_size, num_classes)\n",
    "#     elif head_type=='custom':\n",
    "#         head[\"classifier\"] = nn.Sequential(nn.Linear(classifier_input_feature_size, hidden_size),\n",
    "#                                 nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False),\n",
    "#                                 nn.BatchNorm1d(hidden_size),\n",
    "#                                 nn.Linear(hidden_size, num_classes))\n",
    "        \n",
    "#     head = nn.Sequential(head)\n",
    "\n",
    "\n",
    "#     model = nn.Sequential(OrderedDict({\n",
    "#         \"body\":body,\n",
    "#         \"head\":head\n",
    "#     }))\n",
    "#     return model\n",
    "\n",
    "# def build_model(backbone_name='gluon_seresnext50_32x4d',\n",
    "#                 pretrained: Union[bool, str]=True,\n",
    "#                 num_classes: int=1000,\n",
    "#                 pool_size: int=1,\n",
    "#                 pool_type: str='avg',\n",
    "#                 head_type: str='linear',\n",
    "#                 hidden_size: Optional[int]=512):\n",
    "    \n",
    "#     try:\n",
    "#         model = build_timm_custom(backbone_name=backbone_name,\n",
    "#                                   pretrained=pretrained,\n",
    "#                                   num_classes=num_classes,\n",
    "#                                   pool_size=pool_size,\n",
    "#                                   pool_type=pool_type,\n",
    "#                                   head_type=hidden_size,\n",
    "#                                   hidden_size=hidden_size)\n",
    "\n",
    "#     except:\n",
    "#         print\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
