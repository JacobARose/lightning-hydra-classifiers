{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Transfer Learning Experiments\n",
    "\n",
    "Created by: Jacob A Rose  \n",
    "Created On: Wednesday Oct 6th, 2021  \n",
    "\n",
    "Based on Notebook located at: https://jarvislabs.ai/blogs/transfer-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "max_lr = 0.3\n",
    "base_lr = 0.1\n",
    "optim_lr = 0.5\n",
    "\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "model = torch.nn.Sequential(OrderedDict({\"head\":torch.nn.Linear(2, 1),\n",
    "                                         \"backbone\":torch.nn.Linear(100,2)}))\n",
    "                            \n",
    "optimizer = torch.optim.SGD([{\"params\":model.backbone.parameters(), \"lr\":optim_lr*0.1, \"weight_decay\": 0.01},\n",
    "                             {\"params\":model.head.parameters(), \"lr\":optim_lr, \"weight_decay\": 0.01}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_factor = 0.1\n",
    "# end_factor = 1.0\n",
    "# total_iters = 5 #3 # epochs\n",
    "# last_epoch=-1\n",
    "# factor_range = end_factor - start_factor\n",
    "# warmup_lambda = lambda epoch: (1.0+factor_range/(start_factor*epoch + factor_range*(last_epoch-1)+0.00001))\n",
    "\n",
    "\n",
    "\n",
    "mult_init = 0.05\n",
    "\n",
    "lr_max = lr\n",
    "lr_init = lr*mult_init\n",
    "T_max = 5\n",
    "T_init = 0\n",
    "\n",
    "warmup_lambda = lambda epoch: lr_init + ((lr_max-lr_init) / (warmup_epochs)\n",
    "\n",
    "\n",
    "plt.plot(range(20), list(map(warmup_lambda, range(20))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, Subset\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule\n",
    "\n",
    "\n",
    "class RandomDictDataset(Dataset):\n",
    "    def __init__(self, size: int, length: int):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        a = self.data[index]\n",
    "        b = a + 2\n",
    "        return {\"a\": a, \"b\": b}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, size: int, length: int):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class RandomIterableDataset(IterableDataset):\n",
    "    def __init__(self, size: int, count: int):\n",
    "        self.count = count\n",
    "        self.size = size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.count):\n",
    "            yield torch.randn(self.size)\n",
    "\n",
    "\n",
    "class RandomIterableDatasetWithLen(IterableDataset):\n",
    "    def __init__(self, size: int, count: int):\n",
    "        self.count = count\n",
    "        self.size = size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(len(self)):\n",
    "            yield torch.randn(self.size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "\n",
    "class BoringModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        \"\"\"Testing PL Module.\n",
    "        Use as follows:\n",
    "        - subclass\n",
    "        - modify the behavior for what you want\n",
    "        class TestModel(BaseTestModel):\n",
    "            def training_step(...):\n",
    "                # do your own thing\n",
    "        or:\n",
    "        model = BaseTestModel()\n",
    "        model.training_epoch_end = None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "    def loss(self, batch, prediction):\n",
    "        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\n",
    "        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\n",
    "\n",
    "    def step(self, x):\n",
    "        x = self(x)\n",
    "        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        loss = self.loss(batch, output)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def training_step_end(self, training_step_outputs):\n",
    "        return training_step_outputs\n",
    "\n",
    "    def training_epoch_end(self, outputs) -> None:\n",
    "        torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        loss = self.loss(batch, output)\n",
    "        return {\"x\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> None:\n",
    "        torch.stack([x[\"x\"] for x in outputs]).mean()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        loss = self.loss(batch, output)\n",
    "        return {\"y\": loss}\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> None:\n",
    "        torch.stack([x[\"y\"] for x in outputs]).mean()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(RandomDataset(32, 64))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(RandomDataset(32, 64))\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(RandomDataset(32, 64))\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(RandomDataset(32, 64))\n",
    "\n",
    "\n",
    "class BoringDataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.non_picklable = None\n",
    "        self.checkpoint_state: Optional[str] = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.random_full = RandomDataset(32, 64 * 4)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.random_train = Subset(self.random_full, indices=range(64))\n",
    "\n",
    "        if stage in (\"fit\", \"validate\") or stage is None:\n",
    "            self.random_val = Subset(self.random_full, indices=range(64, 64 * 2))\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.random_test = Subset(self.random_full, indices=range(64 * 2, 64 * 3))\n",
    "\n",
    "        if stage == \"predict\" or stage is None:\n",
    "            self.random_predict = Subset(self.random_full, indices=range(64 * 3, 64 * 4))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.random_train)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.random_val)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.random_test)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.random_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls '/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/tests'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class WarmupLRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Warmup learning rate until `total_steps`\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): wrapped optimizer.\n",
    "        configs (DictConfig): configuration set.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer\n",
    "    ) -> None:\n",
    "        warmup_steps = 6\n",
    "        peak_lr = 0.1\n",
    "        self.init_lr = 0.001\n",
    "#         warmup_steps = 2\n",
    "\n",
    "        if warmup_steps != 0:\n",
    "            warmup_rate = peak_lr - self.init_lr\n",
    "            self.warmup_rate = warmup_rate / warmup_steps\n",
    "        else:\n",
    "            self.warmup_rate = 0\n",
    "            \n",
    "        print(f\"self.warmup_rate={self.warmup_rate}\")\n",
    "        self.update_steps = 1\n",
    "        self.lr = self.init_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def set_lr(self, optimizer, lr):\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "    def step(self, val_loss = None):\n",
    "        print(self.lr)\n",
    "        if self.update_steps < self.warmup_steps:\n",
    "            lr = self.init_lr + self.warmup_rate * self.update_steps\n",
    "            self.set_lr(self.optimizer, lr)\n",
    "            self.lr = lr\n",
    "            print(f\"new_lr={self.lr}\")\n",
    "        else:\n",
    "            print(f\"No update: self.update_steps:{self.update_steps}, self.warmup_steps:{self.warmup_steps}\")\n",
    "        self.update_steps += 1\n",
    "        return self.lr\n",
    "\n",
    "class TestModel(BoringModel):\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\n",
    "        return [optimizer], [{\"scheduler\":WarmupLRScheduler(optimizer), \"interval\": \"step\"}]\n",
    "\n",
    "model = TestModel()\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=5,\n",
    ")\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## lr_scheduler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import math\n",
    "from torch._six import inf\n",
    "from functools import wraps\n",
    "import warnings\n",
    "import weakref\n",
    "from collections import Counter\n",
    "from bisect import bisect_right\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "EPOCH_DEPRECATION_WARNING = (\n",
    "    \"The epoch parameter in `scheduler.step()` was not necessary and is being \"\n",
    "    \"deprecated where possible. Please use `scheduler.step()` to step the \"\n",
    "    \"scheduler. During the deprecation, if epoch is different from None, the \"\n",
    "    \"closed form is used instead of the new chainable form, where available. \"\n",
    "    \"Please open an issue if you are unable to replicate your use case: \"\n",
    "    \"https://github.com/pytorch/pytorch/issues/new/choose.\"\n",
    ")\n",
    "\n",
    "class _LRScheduler(object):\n",
    "\n",
    "    def __init__(self, optimizer, last_epoch=-1, verbose=False):\n",
    "\n",
    "        # Attach optimizer\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Initialize epoch and base learning rates\n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "        self.base_lrs = [group['initial_lr'] for group in optimizer.param_groups]\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "        # Following https://github.com/pytorch/pytorch/issues/20124\n",
    "        # We would like to ensure that `lr_scheduler.step()` is called after\n",
    "        # `optimizer.step()`\n",
    "        def with_counter(method):\n",
    "            if getattr(method, '_with_counter', False):\n",
    "                # `optimizer.step()` has already been replaced, return.\n",
    "                return method\n",
    "\n",
    "            # Keep a weak reference to the optimizer instance to prevent\n",
    "            # cyclic references.\n",
    "            instance_ref = weakref.ref(method.__self__)\n",
    "            # Get the unbound method for the same purpose.\n",
    "            func = method.__func__\n",
    "            cls = instance_ref().__class__\n",
    "            del method\n",
    "\n",
    "            @wraps(func)\n",
    "            def wrapper(*args, **kwargs):\n",
    "                instance = instance_ref()\n",
    "                instance._step_count += 1\n",
    "                wrapped = func.__get__(instance, cls)\n",
    "                return wrapped(*args, **kwargs)\n",
    "\n",
    "            # Note that the returned function here is no longer a bound method,\n",
    "            # so attributes like `__func__` and `__self__` no longer exist.\n",
    "            wrapper._with_counter = True\n",
    "            return wrapper\n",
    "\n",
    "        self.optimizer.step = with_counter(self.optimizer.step)\n",
    "        self.optimizer._step_count = 0\n",
    "        self._step_count = 0\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.step()\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        \"\"\"\n",
    "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(state_dict)\n",
    "\n",
    "    def get_last_lr(self):\n",
    "        \"\"\" Return last computed learning rate by current scheduler.\n",
    "        \"\"\"\n",
    "        return self._last_lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        # Compute learning rate using chainable form of the scheduler\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def print_lr(self, is_verbose, group, lr, epoch=None):\n",
    "        \"\"\"Display the current learning rate.\n",
    "        \"\"\"\n",
    "        if is_verbose:\n",
    "            if epoch is None:\n",
    "                print('Adjusting learning rate'\n",
    "                      ' of group {} to {:.4e}.'.format(group, lr))\n",
    "            else:\n",
    "                print('Epoch {:5d}: adjusting learning rate'\n",
    "                      ' of group {} to {:.4e}.'.format(epoch, group, lr))\n",
    "\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        # Raise a warning if old pattern is detected\n",
    "        # https://github.com/pytorch/pytorch/issues/20124\n",
    "        if self._step_count == 1:\n",
    "            if not hasattr(self.optimizer.step, \"_with_counter\"):\n",
    "                warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n",
    "                              \"initialization. Please, make sure to call `optimizer.step()` before \"\n",
    "                              \"`lr_scheduler.step()`. See more details at \"\n",
    "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
    "\n",
    "            # Just check if there were two first lr_scheduler.step() calls before optimizer.step()\n",
    "            elif self.optimizer._step_count < 1:\n",
    "                warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
    "                              \"In PyTorch 1.1.0 and later, you should call them in the opposite order: \"\n",
    "                              \"`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this \"\n",
    "                              \"will result in PyTorch skipping the first value of the learning rate schedule. \"\n",
    "                              \"See more details at \"\n",
    "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
    "        self._step_count += 1\n",
    "\n",
    "        class _enable_get_lr_call:\n",
    "\n",
    "            def __init__(self, o):\n",
    "                self.o = o\n",
    "\n",
    "            def __enter__(self):\n",
    "                self.o._get_lr_called_within_step = True\n",
    "                return self\n",
    "\n",
    "            def __exit__(self, type, value, traceback):\n",
    "                self.o._get_lr_called_within_step = False\n",
    "\n",
    "        with _enable_get_lr_call(self):\n",
    "            if epoch is None:\n",
    "                self.last_epoch += 1\n",
    "                values = self.get_lr()\n",
    "            else:\n",
    "                warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
    "                self.last_epoch = epoch\n",
    "                if hasattr(self, \"_get_closed_form_lr\"):\n",
    "                    values = self._get_closed_form_lr()\n",
    "                else:\n",
    "                    values = self.get_lr()\n",
    "\n",
    "        for i, data in enumerate(zip(self.optimizer.param_groups, values)):\n",
    "            param_group, lr = data\n",
    "            param_group['lr'] = lr\n",
    "            self.print_lr(self.verbose, i, lr, epoch)\n",
    "\n",
    "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "class LambdaLR(_LRScheduler):\n",
    "    \"\"\"Sets the learning rate of each parameter group to the initial lr\n",
    "    times a given function. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        lr_lambda (function or list): A function which computes a multiplicative\n",
    "            factor given an integer parameter epoch, or a list of such\n",
    "            functions, one for each group in optimizer.param_groups.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer has two groups.\n",
    "        >>> lambda1 = lambda epoch: epoch // 30\n",
    "        >>> lambda2 = lambda epoch: 0.95 ** epoch\n",
    "        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if not isinstance(lr_lambda, list) and not isinstance(lr_lambda, tuple):\n",
    "            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)\n",
    "        else:\n",
    "            if len(lr_lambda) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"Expected {} lr_lambdas, but got {}\".format(\n",
    "                    len(optimizer.param_groups), len(lr_lambda)))\n",
    "            self.lr_lambdas = list(lr_lambda)\n",
    "        super(LambdaLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        The learning rate lambda functions will only be saved if they are callable objects\n",
    "        and not if they are functions or lambdas.\n",
    "\n",
    "        When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.\n",
    "        \"\"\"\n",
    "\n",
    "        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', 'lr_lambdas')}\n",
    "        state_dict['lr_lambdas'] = [None] * len(self.lr_lambdas)\n",
    "\n",
    "        for idx, fn in enumerate(self.lr_lambdas):\n",
    "            if not isinstance(fn, types.FunctionType):\n",
    "                state_dict['lr_lambdas'][idx] = fn.__dict__.copy()\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "\n",
    "        lr_lambdas = state_dict.pop('lr_lambdas')\n",
    "        self.__dict__.update(state_dict)\n",
    "        # Restore state_dict keys in order to prevent side effects\n",
    "        # https://github.com/pytorch/pytorch/issues/32756\n",
    "        state_dict['lr_lambdas'] = lr_lambdas\n",
    "\n",
    "        for idx, fn in enumerate(lr_lambdas):\n",
    "            if fn is not None:\n",
    "                self.lr_lambdas[idx].__dict__.update(fn)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "\n",
    "        return [base_lr * lmbda(self.last_epoch)\n",
    "                for lmbda, base_lr in zip(self.lr_lambdas, self.base_lrs)]\n",
    "\n",
    "\n",
    "class MultiplicativeLR(_LRScheduler):\n",
    "    \"\"\"Multiply the learning rate of each parameter group by the factor given\n",
    "    in the specified function. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        lr_lambda (function or list): A function which computes a multiplicative\n",
    "            factor given an integer parameter epoch, or a list of such\n",
    "            functions, one for each group in optimizer.param_groups.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> lmbda = lambda epoch: 0.95\n",
    "        >>> scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if not isinstance(lr_lambda, list) and not isinstance(lr_lambda, tuple):\n",
    "            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)\n",
    "        else:\n",
    "            if len(lr_lambda) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"Expected {} lr_lambdas, but got {}\".format(\n",
    "                    len(optimizer.param_groups), len(lr_lambda)))\n",
    "            self.lr_lambdas = list(lr_lambda)\n",
    "        super(MultiplicativeLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        The learning rate lambda functions will only be saved if they are callable objects\n",
    "        and not if they are functions or lambdas.\n",
    "        \"\"\"\n",
    "        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', 'lr_lambdas')}\n",
    "        state_dict['lr_lambdas'] = [None] * len(self.lr_lambdas)\n",
    "\n",
    "        for idx, fn in enumerate(self.lr_lambdas):\n",
    "            if not isinstance(fn, types.FunctionType):\n",
    "                state_dict['lr_lambdas'][idx] = fn.__dict__.copy()\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        lr_lambdas = state_dict.pop('lr_lambdas')\n",
    "        self.__dict__.update(state_dict)\n",
    "        # Restore state_dict keys in order to prevent side effects\n",
    "        # https://github.com/pytorch/pytorch/issues/32756\n",
    "        state_dict['lr_lambdas'] = lr_lambdas\n",
    "\n",
    "        for idx, fn in enumerate(lr_lambdas):\n",
    "            if fn is not None:\n",
    "                self.lr_lambdas[idx].__dict__.update(fn)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch > 0:\n",
    "            return [group['lr'] * lmbda(self.last_epoch)\n",
    "                    for lmbda, group in zip(self.lr_lambdas, self.optimizer.param_groups)]\n",
    "        else:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "class StepLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by gamma every\n",
    "    step_size epochs. Notice that such decay can happen simultaneously with\n",
    "    other changes to the learning rate from outside this scheduler. When\n",
    "    last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        step_size (int): Period of learning rate decay.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "            Default: 0.1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    "        >>> # lr = 0.05     if epoch < 30\n",
    "        >>> # lr = 0.005    if 30 <= epoch < 60\n",
    "        >>> # lr = 0.0005   if 60 <= epoch < 90\n",
    "        >>> # ...\n",
    "        >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False):\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        super(StepLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if (self.last_epoch == 0) or (self.last_epoch % self.step_size != 0):\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        return [group['lr'] * self.gamma\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [base_lr * self.gamma ** (self.last_epoch // self.step_size)\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class MultiStepLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by gamma once the\n",
    "    number of epoch reaches one of the milestones. Notice that such decay can\n",
    "    happen simultaneously with other changes to the learning rate from outside\n",
    "    this scheduler. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        milestones (list): List of epoch indices. Must be increasing.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "            Default: 0.1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    "        >>> # lr = 0.05     if epoch < 30\n",
    "        >>> # lr = 0.005    if 30 <= epoch < 80\n",
    "        >>> # lr = 0.0005   if epoch >= 80\n",
    "        >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False):\n",
    "        self.milestones = Counter(milestones)\n",
    "        self.gamma = gamma\n",
    "        super(MultiStepLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch not in self.milestones:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        return [group['lr'] * self.gamma ** self.milestones[self.last_epoch]\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        milestones = list(sorted(self.milestones.elements()))\n",
    "        return [base_lr * self.gamma ** bisect_right(milestones, self.last_epoch)\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ConstantLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by a small constant factor until the\n",
    "    number of epoch reaches a pre-defined milestone: total_iters. Notice that such decay can\n",
    "    happen simultaneously with other changes to the learning rate from outside this scheduler.\n",
    "    When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        factor (float): The number we multiply learning rate until the milestone. Default: 1./3.\n",
    "        total_iters (int): The number of steps that the scheduler decays the learning rate.\n",
    "            Default: 5.\n",
    "        last_epoch (int): The index of the last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    "        >>> # lr = 0.025   if epoch == 0\n",
    "        >>> # lr = 0.025   if epoch == 1\n",
    "        >>> # lr = 0.025   if epoch == 2\n",
    "        >>> # lr = 0.025   if epoch == 3\n",
    "        >>> # lr = 0.05    if epoch >= 4\n",
    "        >>> scheduler = ConstantLR(self.opt, factor=0.5, total_iters=4)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, factor=1.0 / 3, total_iters=5, last_epoch=-1, verbose=False):\n",
    "        if factor > 1.0 or factor < 0:\n",
    "            raise ValueError('Constant multiplicative factor expected to be between 0 and 1.')\n",
    "\n",
    "        self.factor = factor\n",
    "        self.total_iters = total_iters\n",
    "        super(ConstantLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [group['lr'] * self.factor for group in self.optimizer.param_groups]\n",
    "\n",
    "        if (self.last_epoch > self.total_iters or\n",
    "                (self.last_epoch != self.total_iters)):\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "        if (self.last_epoch == self.total_iters):\n",
    "            return [group['lr'] * (1.0 / self.factor) for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [base_lr * (self.factor + (self.last_epoch >= self.total_iters) * (1 - self.factor))\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class LinearLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by linearly changing small\n",
    "    multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters.\n",
    "    Notice that such decay can happen simultaneously with other changes to the learning rate\n",
    "    from outside this scheduler. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        start_factor (float): The number we multiply learning rate in the first epoch.\n",
    "            The multiplication factor changes towards end_factor in the following epochs.\n",
    "            Default: 1./3.\n",
    "        end_factor (float): The number we multiply learning rate at the end of linear changing\n",
    "            process. Default: 1.0.\n",
    "        total_iters (int): The number of iterations that multiplicative factor reaches to 1.\n",
    "            Default: 5.\n",
    "        last_epoch (int): The index of the last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    "        >>> # lr = 0.025    if epoch == 0\n",
    "        >>> # lr = 0.03125  if epoch == 1\n",
    "        >>> # lr = 0.0375   if epoch == 2\n",
    "        >>> # lr = 0.04375  if epoch == 3\n",
    "        >>> # lr = 0.005    if epoch >= 4\n",
    "        >>> scheduler = LinearLR(self.opt, start_factor=0.5, total_iters=4)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, start_factor=1.0 / 3, end_factor=1.0, total_iters=5, last_epoch=-1,\n",
    "                 verbose=False):\n",
    "        if start_factor > 1.0 or start_factor < 0:\n",
    "            raise ValueError('Starting multiplicative factor expected to be between 0 and 1.')\n",
    "\n",
    "        if end_factor > 1.0 or end_factor < 0:\n",
    "            raise ValueError('Ending multiplicative factor expected to be between 0 and 1.')\n",
    "\n",
    "        self.start_factor = start_factor\n",
    "        self.end_factor = end_factor\n",
    "        self.total_iters = total_iters\n",
    "        super(LinearLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [group['lr'] * self.start_factor for group in self.optimizer.param_groups]\n",
    "\n",
    "        if (self.last_epoch > self.total_iters):\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "        return [group['lr'] * (1. + (self.end_factor - self.start_factor) /\n",
    "                (self.total_iters * self.start_factor + (self.last_epoch - 1) * (self.end_factor - self.start_factor)))\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [base_lr * (self.start_factor +\n",
    "                (self.end_factor - self.start_factor) * min(self.total_iters, self.last_epoch) / self.total_iters)\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by gamma every epoch.\n",
    "    When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, gamma, last_epoch=-1, verbose=False):\n",
    "        self.gamma = gamma\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        return [group['lr'] * self.gamma\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [base_lr * self.gamma ** self.last_epoch\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class SequentialLR(_LRScheduler):\n",
    "    \"\"\"Receives the list of schedulers that is expected to be called sequentially during\n",
    "    optimization process and milestone points that provides exact intervals to reflect\n",
    "    which scheduler is supposed to be called at a given epoch.\n",
    "\n",
    "    Args:\n",
    "        schedulers (list): List of chained schedulers.\n",
    "        milestones (list): List of integers that reflects milestone points.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 1. for all groups\n",
    "        >>> # lr = 0.1     if epoch == 0\n",
    "        >>> # lr = 0.1     if epoch == 1\n",
    "        >>> # lr = 0.9     if epoch == 2\n",
    "        >>> # lr = 0.81    if epoch == 3\n",
    "        >>> # lr = 0.729   if epoch == 4\n",
    "        >>> scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)\n",
    "        >>> scheduler2 = ExponentialLR(self.opt, gamma=0.9)\n",
    "        >>> scheduler = SequentialLR(self.opt, schedulers=[scheduler1, scheduler2], milestones=[2])\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, schedulers, milestones, last_epoch=-1, verbose=False):\n",
    "        for scheduler_idx in range(1, len(schedulers)):\n",
    "            if (schedulers[scheduler_idx].optimizer != schedulers[0].optimizer):\n",
    "                raise ValueError(\n",
    "                    \"Sequential Schedulers expects all schedulers to belong to the same optimizer, but \"\n",
    "                    \"got schedulers at index {} and {} to be different\".format(0, scheduler_idx)\n",
    "                )\n",
    "        if (len(milestones) != len(schedulers) - 1):\n",
    "            raise ValueError(\n",
    "                \"Sequential Schedulers expects number of schedulers provided to be one more \"\n",
    "                \"than the number of milestone points, but got number of schedulers {} and the \"\n",
    "                \"number of milestones to be equal to {}\".format(len(schedulers), len(milestones))\n",
    "            )\n",
    "        self._schedulers = schedulers\n",
    "        self._milestones = milestones\n",
    "        self.last_epoch = last_epoch + 1\n",
    "\n",
    "    def step(self):\n",
    "        self.last_epoch += 1\n",
    "        idx = bisect_right(self._milestones, self.last_epoch)\n",
    "        if idx > 0 and self._milestones[idx - 1] == self.last_epoch:\n",
    "            self._schedulers[idx].step(0)\n",
    "        else:\n",
    "            self._schedulers[idx].step()\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        The wrapped scheduler states will also be saved.\n",
    "        \"\"\"\n",
    "        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', '_schedulers')}\n",
    "        state_dict['_schedulers'] = [None] * len(self._schedulers)\n",
    "\n",
    "        for idx, s in enumerate(self._schedulers):\n",
    "            state_dict['_schedulers'][idx] = s.state_dict()\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        _schedulers = state_dict.pop('_schedulers')\n",
    "        self.__dict__.update(state_dict)\n",
    "        # Restore state_dict keys in order to prevent side effects\n",
    "        # https://github.com/pytorch/pytorch/issues/32756\n",
    "        state_dict['_schedulers'] = _schedulers\n",
    "\n",
    "        for idx, s in enumerate(_schedulers):\n",
    "            self._schedulers[idx].load_state_dict(s)\n",
    "\n",
    "\n",
    "class CosineAnnealingLR(_LRScheduler):\n",
    "    r\"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
    "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
    "\n",
    "    .. math::\n",
    "        \\begin{aligned}\n",
    "            \\eta_t & = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1\n",
    "            + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right),\n",
    "            & T_{cur} \\neq (2k+1)T_{max}; \\\\\n",
    "            \\eta_{t+1} & = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\n",
    "            \\left(1 - \\cos\\left(\\frac{1}{T_{max}}\\pi\\right)\\right),\n",
    "            & T_{cur} = (2k+1)T_{max}.\n",
    "        \\end{aligned}\n",
    "\n",
    "    When last_epoch=-1, sets initial lr as lr. Notice that because the schedule\n",
    "    is defined recursively, the learning rate can be simultaneously modified\n",
    "    outside this scheduler by other operators. If the learning rate is set\n",
    "    solely by this scheduler, the learning rate at each step becomes:\n",
    "\n",
    "    .. math::\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n",
    "        \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)\n",
    "\n",
    "    It has been proposed in\n",
    "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n",
    "    implements the cosine annealing part of SGDR, and not the restarts.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        T_max (int): Maximum number of iterations.\n",
    "        eta_min (float): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
    "        https://arxiv.org/abs/1608.03983\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        super(CosineAnnealingLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        elif (self.last_epoch - 1 - self.T_max) % (2 * self.T_max) == 0:\n",
    "            return [group['lr'] + (base_lr - self.eta_min) *\n",
    "                    (1 - math.cos(math.pi / self.T_max)) / 2\n",
    "                    for base_lr, group in\n",
    "                    zip(self.base_lrs, self.optimizer.param_groups)]\n",
    "        return [(1 + math.cos(math.pi * self.last_epoch / self.T_max)) /\n",
    "                (1 + math.cos(math.pi * (self.last_epoch - 1) / self.T_max)) *\n",
    "                (group['lr'] - self.eta_min) + self.eta_min\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ChainedScheduler(_LRScheduler):\n",
    "    \"\"\"Chains list of learning rate schedulers. It takes a list of chainable learning\n",
    "    rate schedulers and performs consecutive step() functions belong to them by just\n",
    "    one call.\n",
    "\n",
    "    Args:\n",
    "        schedulers (list): List of chained schedulers.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 1. for all groups\n",
    "        >>> # lr = 0.09     if epoch == 0\n",
    "        >>> # lr = 0.081    if epoch == 1\n",
    "        >>> # lr = 0.729    if epoch == 2\n",
    "        >>> # lr = 0.6561   if epoch == 3\n",
    "        >>> # lr = 0.59049  if epoch >= 4\n",
    "        >>> scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)\n",
    "        >>> scheduler2 = ExponentialLR(self.opt, gamma=0.9)\n",
    "        >>> scheduler = ChainedScheduler([scheduler1, scheduler2])\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schedulers):\n",
    "        for scheduler_idx in range(1, len(schedulers)):\n",
    "            if (schedulers[scheduler_idx].optimizer != schedulers[0].optimizer):\n",
    "                raise ValueError(\n",
    "                    \"ChainedScheduler expects all schedulers to belong to the same optimizer, but \"\n",
    "                    \"got schedulers at index {} and {} to be different\".format(0, scheduler_idx)\n",
    "                )\n",
    "        self._schedulers = list(schedulers)\n",
    "\n",
    "    def step(self):\n",
    "        for scheduler in self._schedulers:\n",
    "            scheduler.step()\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        The wrapped scheduler states will also be saved.\n",
    "        \"\"\"\n",
    "        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', '_schedulers')}\n",
    "        state_dict['_schedulers'] = [None] * len(self._schedulers)\n",
    "\n",
    "        for idx, s in enumerate(self._schedulers):\n",
    "            state_dict['_schedulers'][idx] = s.state_dict()\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        _schedulers = state_dict.pop('_schedulers')\n",
    "        self.__dict__.update(state_dict)\n",
    "        # Restore state_dict keys in order to prevent side effects\n",
    "        # https://github.com/pytorch/pytorch/issues/32756\n",
    "        state_dict['_schedulers'] = _schedulers\n",
    "\n",
    "        for idx, s in enumerate(_schedulers):\n",
    "            self._schedulers[idx].load_state_dict(s)\n",
    "\n",
    "\n",
    "class ReduceLROnPlateau(object):\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    Models often benefit from reducing the learning rate by a factor\n",
    "    of 2-10 once learning stagnates. This scheduler reads a metrics\n",
    "    quantity and if no improvement is seen for a 'patience' number\n",
    "    of epochs, the learning rate is reduced.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        mode (str): One of `min`, `max`. In `min` mode, lr will\n",
    "            be reduced when the quantity monitored has stopped\n",
    "            decreasing; in `max` mode it will be reduced when the\n",
    "            quantity monitored has stopped increasing. Default: 'min'.\n",
    "        factor (float): Factor by which the learning rate will be\n",
    "            reduced. new_lr = lr * factor. Default: 0.1.\n",
    "        patience (int): Number of epochs with no improvement after\n",
    "            which learning rate will be reduced. For example, if\n",
    "            `patience = 2`, then we will ignore the first 2 epochs\n",
    "            with no improvement, and will only decrease the LR after the\n",
    "            3rd epoch if the loss still hasn't improved then.\n",
    "            Default: 10.\n",
    "        threshold (float): Threshold for measuring the new optimum,\n",
    "            to only focus on significant changes. Default: 1e-4.\n",
    "        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n",
    "            dynamic_threshold = best * ( 1 + threshold ) in 'max'\n",
    "            mode or best * ( 1 - threshold ) in `min` mode.\n",
    "            In `abs` mode, dynamic_threshold = best + threshold in\n",
    "            `max` mode or best - threshold in `min` mode. Default: 'rel'.\n",
    "        cooldown (int): Number of epochs to wait before resuming\n",
    "            normal operation after lr has been reduced. Default: 0.\n",
    "        min_lr (float or list): A scalar or a list of scalars. A\n",
    "            lower bound on the learning rate of all param groups\n",
    "            or each group respectively. Default: 0.\n",
    "        eps (float): Minimal decay applied to lr. If the difference\n",
    "            between new and old lr is smaller than eps, the update is\n",
    "            ignored. Default: 1e-8.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     train(...)\n",
    "        >>>     val_loss = validate(...)\n",
    "        >>>     # Note that step should be called after validate()\n",
    "        >>>     scheduler.step(val_loss)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, mode='min', factor=0.1, patience=10,\n",
    "                 threshold=1e-4, threshold_mode='rel', cooldown=0,\n",
    "                 min_lr=0, eps=1e-8, verbose=False):\n",
    "\n",
    "        if factor >= 1.0:\n",
    "            raise ValueError('Factor should be < 1.0.')\n",
    "        self.factor = factor\n",
    "\n",
    "        # Attach optimizer\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(min_lr, list) or isinstance(min_lr, tuple):\n",
    "            if len(min_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} min_lrs, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(min_lr)))\n",
    "            self.min_lrs = list(min_lr)\n",
    "        else:\n",
    "            self.min_lrs = [min_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.threshold_mode = threshold_mode\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = None\n",
    "        self.mode_worse = None  # the worse value for the chosen mode\n",
    "        self.eps = eps\n",
    "        self.last_epoch = 0\n",
    "        self._init_is_better(mode=mode, threshold=threshold,\n",
    "                             threshold_mode=threshold_mode)\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets num_bad_epochs counter and cooldown counter.\"\"\"\n",
    "        self.best = self.mode_worse\n",
    "        self.cooldown_counter = 0\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, metrics, epoch=None):\n",
    "        # convert `metrics` to float, in case it's a zero-dim Tensor\n",
    "        current = float(metrics)\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        else:\n",
    "            warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
    "        self.last_epoch = epoch\n",
    "\n",
    "        if self.is_better(current, self.best):\n",
    "            self.best = current\n",
    "            self.num_bad_epochs = 0\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.in_cooldown:\n",
    "            self.cooldown_counter -= 1\n",
    "            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\n",
    "\n",
    "        if self.num_bad_epochs > self.patience:\n",
    "            self._reduce_lr(epoch)\n",
    "            self.cooldown_counter = self.cooldown\n",
    "            self.num_bad_epochs = 0\n",
    "\n",
    "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _reduce_lr(self, epoch):\n",
    "        for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "            old_lr = float(param_group['lr'])\n",
    "            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n",
    "            if old_lr - new_lr > self.eps:\n",
    "                param_group['lr'] = new_lr\n",
    "                if self.verbose:\n",
    "                    print('Epoch {:5d}: reducing learning rate'\n",
    "                          ' of group {} to {:.4e}.'.format(epoch, i, new_lr))\n",
    "\n",
    "    @property\n",
    "    def in_cooldown(self):\n",
    "        return self.cooldown_counter > 0\n",
    "\n",
    "    def is_better(self, a, best):\n",
    "        if self.mode == 'min' and self.threshold_mode == 'rel':\n",
    "            rel_epsilon = 1. - self.threshold\n",
    "            return a < best * rel_epsilon\n",
    "\n",
    "        elif self.mode == 'min' and self.threshold_mode == 'abs':\n",
    "            return a < best - self.threshold\n",
    "\n",
    "        elif self.mode == 'max' and self.threshold_mode == 'rel':\n",
    "            rel_epsilon = self.threshold + 1.\n",
    "            return a > best * rel_epsilon\n",
    "\n",
    "        else:  # mode == 'max' and epsilon_mode == 'abs':\n",
    "            return a > best + self.threshold\n",
    "\n",
    "    def _init_is_better(self, mode, threshold, threshold_mode):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if threshold_mode not in {'rel', 'abs'}:\n",
    "            raise ValueError('threshold mode ' + threshold_mode + ' is unknown!')\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.mode_worse = inf\n",
    "        else:  # mode == 'max':\n",
    "            self.mode_worse = -inf\n",
    "\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.threshold_mode = threshold_mode\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.__dict__.update(state_dict)\n",
    "        self._init_is_better(mode=self.mode, threshold=self.threshold, threshold_mode=self.threshold_mode)\n",
    "\n",
    "\n",
    "class CyclicLR(_LRScheduler):\n",
    "    r\"\"\"Sets the learning rate of each parameter group according to\n",
    "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
    "    rate between two boundaries with a constant frequency, as detailed in\n",
    "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
    "    The distance between the two boundaries can be scaled on a per-iteration\n",
    "    or per-cycle basis.\n",
    "\n",
    "    Cyclical learning rate policy changes the learning rate after every batch.\n",
    "    `step` should be called after a batch has been used for training.\n",
    "\n",
    "    This class has three built-in policies, as put forth in the paper:\n",
    "\n",
    "    * \"triangular\": A basic triangular cycle without amplitude scaling.\n",
    "    * \"triangular2\": A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    * \"exp_range\": A cycle that scales initial amplitude by :math:`\\text{gamma}^{\\text{cycle iterations}}`\n",
    "      at each cycle iteration.\n",
    "\n",
    "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        base_lr (float or list): Initial learning rate which is the\n",
    "            lower boundary in the cycle for each parameter group.\n",
    "        max_lr (float or list): Upper learning rate boundaries in the cycle\n",
    "            for each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size_up (int): Number of training iterations in the\n",
    "            increasing half of a cycle. Default: 2000\n",
    "        step_size_down (int): Number of training iterations in the\n",
    "            decreasing half of a cycle. If step_size_down is None,\n",
    "            it is set to step_size_up. Default: None\n",
    "        mode (str): One of {triangular, triangular2, exp_range}.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "            Default: 'triangular'\n",
    "        gamma (float): Constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "            Default: 1.0\n",
    "        scale_fn (function): Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            If specified, then 'mode' is ignored.\n",
    "            Default: None\n",
    "        scale_mode (str): {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle).\n",
    "            Default: 'cycle'\n",
    "        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
    "            to learning rate between 'base_momentum' and 'max_momentum'.\n",
    "            Default: True\n",
    "        base_momentum (float or list): Lower momentum boundaries in the cycle\n",
    "            for each parameter group. Note that momentum is cycled inversely\n",
    "            to learning rate; at the peak of a cycle, momentum is\n",
    "            'base_momentum' and learning rate is 'max_lr'.\n",
    "            Default: 0.8\n",
    "        max_momentum (float or list): Upper momentum boundaries in the cycle\n",
    "            for each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_momentum - base_momentum).\n",
    "            The momentum at any cycle is the difference of max_momentum\n",
    "            and some scaling of the amplitude; therefore\n",
    "            base_momentum may not actually be reached depending on\n",
    "            scaling function. Note that momentum is cycled inversely\n",
    "            to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
    "            and learning rate is 'base_lr'\n",
    "            Default: 0.9\n",
    "        last_epoch (int): The index of the last batch. This parameter is used when\n",
    "            resuming a training job. Since `step()` should be invoked after each\n",
    "            batch instead of after each epoch, this number represents the total\n",
    "            number of *batches* computed, not the total number of epochs computed.\n",
    "            When last_epoch=-1, the schedule is started from the beginning.\n",
    "            Default: -1\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         train_batch(...)\n",
    "        >>>         scheduler.step()\n",
    "\n",
    "\n",
    "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer,\n",
    "                 base_lr,\n",
    "                 max_lr,\n",
    "                 step_size_up=2000,\n",
    "                 step_size_down=None,\n",
    "                 mode='triangular',\n",
    "                 gamma=1.,\n",
    "                 scale_fn=None,\n",
    "                 scale_mode='cycle',\n",
    "                 cycle_momentum=True,\n",
    "                 base_momentum=0.8,\n",
    "                 max_momentum=0.9,\n",
    "                 last_epoch=-1,\n",
    "                 verbose=False):\n",
    "\n",
    "        # Attach optimizer\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        base_lrs = self._format_param('base_lr', optimizer, base_lr)\n",
    "        if last_epoch == -1:\n",
    "            for lr, group in zip(base_lrs, optimizer.param_groups):\n",
    "                group['lr'] = lr\n",
    "\n",
    "        self.max_lrs = self._format_param('max_lr', optimizer, max_lr)\n",
    "\n",
    "        step_size_up = float(step_size_up)\n",
    "        step_size_down = float(step_size_down) if step_size_down is not None else step_size_up\n",
    "        self.total_size = step_size_up + step_size_down\n",
    "        self.step_ratio = step_size_up / self.total_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.cycle_momentum = cycle_momentum\n",
    "        if cycle_momentum:\n",
    "            if 'momentum' not in optimizer.defaults:\n",
    "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
    "\n",
    "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
    "            if last_epoch == -1:\n",
    "                for momentum, group in zip(base_momentums, optimizer.param_groups):\n",
    "                    group['momentum'] = momentum\n",
    "            self.base_momentums = [group['momentum'] for group in optimizer.param_groups]\n",
    "            self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
    "\n",
    "        super(CyclicLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "        self.base_lrs = base_lrs\n",
    "\n",
    "    def _format_param(self, name, optimizer, param):\n",
    "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
    "        if isinstance(param, (list, tuple)):\n",
    "            if len(param) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
    "                    len(optimizer.param_groups), name, len(param)))\n",
    "            return param\n",
    "        else:\n",
    "            return [param] * len(optimizer.param_groups)\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Calculates the learning rate at batch index. This function treats\n",
    "        `self.last_epoch` as the last batch index.\n",
    "\n",
    "        If `self.cycle_momentum` is ``True``, this function has a side effect of\n",
    "        updating the optimizer's momentum.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        cycle = math.floor(1 + self.last_epoch / self.total_size)\n",
    "        x = 1. + self.last_epoch / self.total_size - cycle\n",
    "        if x <= self.step_ratio:\n",
    "            scale_factor = x / self.step_ratio\n",
    "        else:\n",
    "            scale_factor = (x - 1) / (self.step_ratio - 1)\n",
    "\n",
    "        lrs = []\n",
    "        for base_lr, max_lr in zip(self.base_lrs, self.max_lrs):\n",
    "            base_height = (max_lr - base_lr) * scale_factor\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_epoch)\n",
    "            lrs.append(lr)\n",
    "\n",
    "        if self.cycle_momentum:\n",
    "            momentums = []\n",
    "            for base_momentum, max_momentum in zip(self.base_momentums, self.max_momentums):\n",
    "                base_height = (max_momentum - base_momentum) * scale_factor\n",
    "                if self.scale_mode == 'cycle':\n",
    "                    momentum = max_momentum - base_height * self.scale_fn(cycle)\n",
    "                else:\n",
    "                    momentum = max_momentum - base_height * self.scale_fn(self.last_epoch)\n",
    "                momentums.append(momentum)\n",
    "            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n",
    "                param_group['momentum'] = momentum\n",
    "\n",
    "        return lrs\n",
    "\n",
    "\n",
    "class CosineAnnealingWarmRestarts(_LRScheduler):\n",
    "    r\"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr, :math:`T_{cur}`\n",
    "    is the number of epochs since the last restart and :math:`T_{i}` is the number\n",
    "    of epochs between two warm restarts in SGDR:\n",
    "\n",
    "    .. math::\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n",
    "        \\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)\n",
    "\n",
    "    When :math:`T_{cur}=T_{i}`, set :math:`\\eta_t = \\eta_{min}`.\n",
    "    When :math:`T_{cur}=0` after restart, set :math:`\\eta_t=\\eta_{max}`.\n",
    "\n",
    "    It has been proposed in\n",
    "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        T_0 (int): Number of iterations for the first restart.\n",
    "        T_mult (int, optional): A factor increases :math:`T_{i}` after a restart. Default: 1.\n",
    "        eta_min (float, optional): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int, optional): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
    "        https://arxiv.org/abs/1608.03983\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        self.T_0 = T_0\n",
    "        self.T_i = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmRestarts, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        return [self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * self.T_cur / self.T_i)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        \"\"\"Step could be called after every batch update\n",
    "\n",
    "        Example:\n",
    "            >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n",
    "            >>> iters = len(dataloader)\n",
    "            >>> for epoch in range(20):\n",
    "            >>>     for i, sample in enumerate(dataloader):\n",
    "            >>>         inputs, labels = sample['inputs'], sample['labels']\n",
    "            >>>         optimizer.zero_grad()\n",
    "            >>>         outputs = net(inputs)\n",
    "            >>>         loss = criterion(outputs, labels)\n",
    "            >>>         loss.backward()\n",
    "            >>>         optimizer.step()\n",
    "            >>>         scheduler.step(epoch + i / iters)\n",
    "\n",
    "        This function can be called in an interleaved way.\n",
    "\n",
    "        Example:\n",
    "            >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n",
    "            >>> for epoch in range(20):\n",
    "            >>>     scheduler.step()\n",
    "            >>> scheduler.step(26)\n",
    "            >>> scheduler.step() # scheduler.step(27), instead of scheduler(20)\n",
    "        \"\"\"\n",
    "\n",
    "        if epoch is None and self.last_epoch < 0:\n",
    "            epoch = 0\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = self.T_i * self.T_mult\n",
    "        else:\n",
    "            if epoch < 0:\n",
    "                raise ValueError(\"Expected non-negative epoch, but got {}\".format(epoch))\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "\n",
    "        class _enable_get_lr_call:\n",
    "\n",
    "            def __init__(self, o):\n",
    "                self.o = o\n",
    "\n",
    "            def __enter__(self):\n",
    "                self.o._get_lr_called_within_step = True\n",
    "                return self\n",
    "\n",
    "            def __exit__(self, type, value, traceback):\n",
    "                self.o._get_lr_called_within_step = False\n",
    "                return self\n",
    "\n",
    "        with _enable_get_lr_call(self):\n",
    "            for i, data in enumerate(zip(self.optimizer.param_groups, self.get_lr())):\n",
    "                param_group, lr = data\n",
    "                param_group['lr'] = lr\n",
    "                self.print_lr(self.verbose, i, lr, epoch)\n",
    "\n",
    "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "class OneCycleLR(_LRScheduler):\n",
    "    r\"\"\"Sets the learning rate of each parameter group according to the\n",
    "    1cycle learning rate policy. The 1cycle policy anneals the learning\n",
    "    rate from an initial learning rate to some maximum learning rate and then\n",
    "    from that maximum learning rate to some minimum learning rate much lower\n",
    "    than the initial learning rate.\n",
    "    This policy was initially described in the paper `Super-Convergence:\n",
    "    Very Fast Training of Neural Networks Using Large Learning Rates`_.\n",
    "\n",
    "    The 1cycle learning rate policy changes the learning rate after every batch.\n",
    "    `step` should be called after a batch has been used for training.\n",
    "\n",
    "    This scheduler is not chainable.\n",
    "\n",
    "    Note also that the total number of steps in the cycle can be determined in one\n",
    "    of two ways (listed in order of precedence):\n",
    "\n",
    "    #. A value for total_steps is explicitly provided.\n",
    "    #. A number of epochs (epochs) and a number of steps per epoch\n",
    "       (steps_per_epoch) are provided.\n",
    "       In this case, the number of total steps is inferred by\n",
    "       total_steps = epochs * steps_per_epoch\n",
    "\n",
    "    You must either provide a value for total_steps or provide a value for both\n",
    "    epochs and steps_per_epoch.\n",
    "\n",
    "    The default behaviour of this scheduler follows the fastai implementation of 1cycle, which\n",
    "    claims that \"unpublished work has shown even better results by using only two phases\". To\n",
    "    mimic the behaviour of the original paper instead, set ``three_phase=True``.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        max_lr (float or list): Upper learning rate boundaries in the cycle\n",
    "            for each parameter group.\n",
    "        total_steps (int): The total number of steps in the cycle. Note that\n",
    "            if a value is not provided here, then it must be inferred by providing\n",
    "            a value for epochs and steps_per_epoch.\n",
    "            Default: None\n",
    "        epochs (int): The number of epochs to train for. This is used along\n",
    "            with steps_per_epoch in order to infer the total number of steps in the cycle\n",
    "            if a value for total_steps is not provided.\n",
    "            Default: None\n",
    "        steps_per_epoch (int): The number of steps per epoch to train for. This is\n",
    "            used along with epochs in order to infer the total number of steps in the\n",
    "            cycle if a value for total_steps is not provided.\n",
    "            Default: None\n",
    "        pct_start (float): The percentage of the cycle (in number of steps) spent\n",
    "            increasing the learning rate.\n",
    "            Default: 0.3\n",
    "        anneal_strategy (str): {'cos', 'linear'}\n",
    "            Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for\n",
    "            linear annealing.\n",
    "            Default: 'cos'\n",
    "        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
    "            to learning rate between 'base_momentum' and 'max_momentum'.\n",
    "            Default: True\n",
    "        base_momentum (float or list): Lower momentum boundaries in the cycle\n",
    "            for each parameter group. Note that momentum is cycled inversely\n",
    "            to learning rate; at the peak of a cycle, momentum is\n",
    "            'base_momentum' and learning rate is 'max_lr'.\n",
    "            Default: 0.85\n",
    "        max_momentum (float or list): Upper momentum boundaries in the cycle\n",
    "            for each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_momentum - base_momentum).\n",
    "            Note that momentum is cycled inversely\n",
    "            to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
    "            and learning rate is 'base_lr'\n",
    "            Default: 0.95\n",
    "        div_factor (float): Determines the initial learning rate via\n",
    "            initial_lr = max_lr/div_factor\n",
    "            Default: 25\n",
    "        final_div_factor (float): Determines the minimum learning rate via\n",
    "            min_lr = initial_lr/final_div_factor\n",
    "            Default: 1e4\n",
    "        three_phase (bool): If ``True``, use a third phase of the schedule to annihilate the\n",
    "            learning rate according to 'final_div_factor' instead of modifying the second\n",
    "            phase (the first two phases will be symmetrical about the step indicated by\n",
    "            'pct_start').\n",
    "        last_epoch (int): The index of the last batch. This parameter is used when\n",
    "            resuming a training job. Since `step()` should be invoked after each\n",
    "            batch instead of after each epoch, this number represents the total\n",
    "            number of *batches* computed, not the total number of epochs computed.\n",
    "            When last_epoch=-1, the schedule is started from the beginning.\n",
    "            Default: -1\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         train_batch(...)\n",
    "        >>>         scheduler.step()\n",
    "\n",
    "\n",
    "    .. _Super-Convergence\\: Very Fast Training of Neural Networks Using Large Learning Rates:\n",
    "        https://arxiv.org/abs/1708.07120\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 optimizer,\n",
    "                 max_lr,\n",
    "                 total_steps=None,\n",
    "                 epochs=None,\n",
    "                 steps_per_epoch=None,\n",
    "                 pct_start=0.3,\n",
    "                 anneal_strategy='cos',\n",
    "                 cycle_momentum=True,\n",
    "                 base_momentum=0.85,\n",
    "                 max_momentum=0.95,\n",
    "                 div_factor=25.,\n",
    "                 final_div_factor=1e4,\n",
    "                 three_phase=False,\n",
    "                 last_epoch=-1,\n",
    "                 verbose=False):\n",
    "\n",
    "        # Validate optimizer\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Validate total_steps\n",
    "        if total_steps is None and epochs is None and steps_per_epoch is None:\n",
    "            raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n",
    "        elif total_steps is not None:\n",
    "            if total_steps <= 0 or not isinstance(total_steps, int):\n",
    "                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n",
    "            self.total_steps = total_steps\n",
    "        else:\n",
    "            if epochs <= 0 or not isinstance(epochs, int):\n",
    "                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n",
    "            if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n",
    "                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n",
    "            self.total_steps = epochs * steps_per_epoch\n",
    "\n",
    "        if three_phase:\n",
    "            self._schedule_phases = [\n",
    "                {\n",
    "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
    "                    'start_lr': 'initial_lr',\n",
    "                    'end_lr': 'max_lr',\n",
    "                    'start_momentum': 'max_momentum',\n",
    "                    'end_momentum': 'base_momentum',\n",
    "                },\n",
    "                {\n",
    "                    'end_step': float(2 * pct_start * self.total_steps) - 2,\n",
    "                    'start_lr': 'max_lr',\n",
    "                    'end_lr': 'initial_lr',\n",
    "                    'start_momentum': 'base_momentum',\n",
    "                    'end_momentum': 'max_momentum',\n",
    "                },\n",
    "                {\n",
    "                    'end_step': self.total_steps - 1,\n",
    "                    'start_lr': 'initial_lr',\n",
    "                    'end_lr': 'min_lr',\n",
    "                    'start_momentum': 'max_momentum',\n",
    "                    'end_momentum': 'max_momentum',\n",
    "                },\n",
    "            ]\n",
    "        else:\n",
    "            self._schedule_phases = [\n",
    "                {\n",
    "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
    "                    'start_lr': 'initial_lr',\n",
    "                    'end_lr': 'max_lr',\n",
    "                    'start_momentum': 'max_momentum',\n",
    "                    'end_momentum': 'base_momentum',\n",
    "                },\n",
    "                {\n",
    "                    'end_step': self.total_steps - 1,\n",
    "                    'start_lr': 'max_lr',\n",
    "                    'end_lr': 'min_lr',\n",
    "                    'start_momentum': 'base_momentum',\n",
    "                    'end_momentum': 'max_momentum',\n",
    "                },\n",
    "            ]\n",
    "\n",
    "        # Validate pct_start\n",
    "        if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n",
    "            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n",
    "\n",
    "        # Validate anneal_strategy\n",
    "        if anneal_strategy not in ['cos', 'linear']:\n",
    "            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n",
    "        elif anneal_strategy == 'cos':\n",
    "            self.anneal_func = self._annealing_cos\n",
    "        elif anneal_strategy == 'linear':\n",
    "            self.anneal_func = self._annealing_linear\n",
    "\n",
    "        # Initialize learning rate variables\n",
    "        max_lrs = self._format_param('max_lr', self.optimizer, max_lr)\n",
    "        if last_epoch == -1:\n",
    "            for idx, group in enumerate(self.optimizer.param_groups):\n",
    "                group['initial_lr'] = max_lrs[idx] / div_factor\n",
    "                group['max_lr'] = max_lrs[idx]\n",
    "                group['min_lr'] = group['initial_lr'] / final_div_factor\n",
    "\n",
    "        # Initialize momentum variables\n",
    "        self.cycle_momentum = cycle_momentum\n",
    "        if self.cycle_momentum:\n",
    "            if 'momentum' not in self.optimizer.defaults and 'betas' not in self.optimizer.defaults:\n",
    "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
    "            self.use_beta1 = 'betas' in self.optimizer.defaults\n",
    "            max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
    "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
    "            if last_epoch == -1:\n",
    "                for m_momentum, b_momentum, group in zip(max_momentums, base_momentums, optimizer.param_groups):\n",
    "                    if self.use_beta1:\n",
    "                        _, beta2 = group['betas']\n",
    "                        group['betas'] = (m_momentum, beta2)\n",
    "                    else:\n",
    "                        group['momentum'] = m_momentum\n",
    "                    group['max_momentum'] = m_momentum\n",
    "                    group['base_momentum'] = b_momentum\n",
    "\n",
    "        super(OneCycleLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def _format_param(self, name, optimizer, param):\n",
    "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
    "        if isinstance(param, (list, tuple)):\n",
    "            if len(param) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
    "                    len(optimizer.param_groups), name, len(param)))\n",
    "            return param\n",
    "        else:\n",
    "            return [param] * len(optimizer.param_groups)\n",
    "\n",
    "    def _annealing_cos(self, start, end, pct):\n",
    "        \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "        cos_out = math.cos(math.pi * pct) + 1\n",
    "        return end + (start - end) / 2.0 * cos_out\n",
    "\n",
    "    def _annealing_linear(self, start, end, pct):\n",
    "        \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "        return (end - start) * pct + start\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        lrs = []\n",
    "        step_num = self.last_epoch\n",
    "\n",
    "        if step_num > self.total_steps:\n",
    "            raise ValueError(\"Tried to step {} times. The specified number of total steps is {}\"\n",
    "                             .format(step_num + 1, self.total_steps))\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            start_step = 0\n",
    "            for i, phase in enumerate(self._schedule_phases):\n",
    "                end_step = phase['end_step']\n",
    "                if step_num <= end_step or i == len(self._schedule_phases) - 1:\n",
    "                    pct = (step_num - start_step) / (end_step - start_step)\n",
    "                    computed_lr = self.anneal_func(group[phase['start_lr']], group[phase['end_lr']], pct)\n",
    "                    if self.cycle_momentum:\n",
    "                        computed_momentum = self.anneal_func(group[phase['start_momentum']], group[phase['end_momentum']], pct)\n",
    "                    break\n",
    "                start_step = phase['end_step']\n",
    "\n",
    "            lrs.append(computed_lr)\n",
    "            if self.cycle_momentum:\n",
    "                if self.use_beta1:\n",
    "                    _, beta2 = group['betas']\n",
    "                    group['betas'] = (computed_momentum, beta2)\n",
    "                else:\n",
    "                    group['momentum'] = computed_momentum\n",
    "\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WarmupLRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## lr_scheduler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import math\n",
    "from torch._six import inf\n",
    "from functools import wraps\n",
    "import warnings\n",
    "import weakref\n",
    "from collections import Counter\n",
    "from bisect import bisect_right\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "EPOCH_DEPRECATION_WARNING = (\n",
    "    \"The epoch parameter in `scheduler.step()` was not necessary and is being \"\n",
    "    \"deprecated where possible. Please use `scheduler.step()` to step the \"\n",
    "    \"scheduler. During the deprecation, if epoch is different from None, the \"\n",
    "    \"closed form is used instead of the new chainable form, where available. \"\n",
    "    \"Please open an issue if you are unable to replicate your use case: \"\n",
    "    \"https://github.com/pytorch/pytorch/issues/new/choose.\"\n",
    ")\n",
    "\n",
    "class _LRScheduler(object):\n",
    "\n",
    "    def __init__(self, optimizer, last_epoch=-1, verbose=False):\n",
    "\n",
    "        # Attach optimizer\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Initialize epoch and base learning rates\n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "        self.base_lrs = [group['initial_lr'] for group in optimizer.param_groups]\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "        # Following https://github.com/pytorch/pytorch/issues/20124\n",
    "        # We would like to ensure that `lr_scheduler.step()` is called after\n",
    "        # `optimizer.step()`\n",
    "        def with_counter(method):\n",
    "            if getattr(method, '_with_counter', False):\n",
    "                # `optimizer.step()` has already been replaced, return.\n",
    "                return method\n",
    "\n",
    "            # Keep a weak reference to the optimizer instance to prevent\n",
    "            # cyclic references.\n",
    "            instance_ref = weakref.ref(method.__self__)\n",
    "            # Get the unbound method for the same purpose.\n",
    "            func = method.__func__\n",
    "            cls = instance_ref().__class__\n",
    "            del method\n",
    "\n",
    "            @wraps(func)\n",
    "            def wrapper(*args, **kwargs):\n",
    "                instance = instance_ref()\n",
    "                instance._step_count += 1\n",
    "                wrapped = func.__get__(instance, cls)\n",
    "                return wrapped(*args, **kwargs)\n",
    "\n",
    "            # Note that the returned function here is no longer a bound method,\n",
    "            # so attributes like `__func__` and `__self__` no longer exist.\n",
    "            wrapper._with_counter = True\n",
    "            return wrapper\n",
    "\n",
    "        self.optimizer.step = with_counter(self.optimizer.step)\n",
    "        self.optimizer._step_count = 0\n",
    "        self._step_count = 0\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.step()\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        \"\"\"\n",
    "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(state_dict)\n",
    "\n",
    "    def get_last_lr(self):\n",
    "        \"\"\" Return last computed learning rate by current scheduler.\n",
    "        \"\"\"\n",
    "        return self._last_lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        # Compute learning rate using chainable form of the scheduler\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def print_lr(self, is_verbose, group, lr, epoch=None):\n",
    "        \"\"\"Display the current learning rate.\n",
    "        \"\"\"\n",
    "        if is_verbose:\n",
    "            if epoch is None:\n",
    "                print('Adjusting learning rate'\n",
    "                      ' of group {} to {:.4e}.'.format(group, lr))\n",
    "            else:\n",
    "                print('Epoch {:5d}: adjusting learning rate'\n",
    "                      ' of group {} to {:.4e}.'.format(epoch, group, lr))\n",
    "\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        # Raise a warning if old pattern is detected\n",
    "        # https://github.com/pytorch/pytorch/issues/20124\n",
    "        if self._step_count == 1:\n",
    "            if not hasattr(self.optimizer.step, \"_with_counter\"):\n",
    "                warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n",
    "                              \"initialization. Please, make sure to call `optimizer.step()` before \"\n",
    "                              \"`lr_scheduler.step()`. See more details at \"\n",
    "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
    "\n",
    "            # Just check if there were two first lr_scheduler.step() calls before optimizer.step()\n",
    "            elif self.optimizer._step_count < 1:\n",
    "                warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
    "                              \"In PyTorch 1.1.0 and later, you should call them in the opposite order: \"\n",
    "                              \"`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this \"\n",
    "                              \"will result in PyTorch skipping the first value of the learning rate schedule. \"\n",
    "                              \"See more details at \"\n",
    "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
    "        self._step_count += 1\n",
    "\n",
    "        class _enable_get_lr_call:\n",
    "\n",
    "            def __init__(self, o):\n",
    "                self.o = o\n",
    "\n",
    "            def __enter__(self):\n",
    "                self.o._get_lr_called_within_step = True\n",
    "                return self\n",
    "\n",
    "            def __exit__(self, type, value, traceback):\n",
    "                self.o._get_lr_called_within_step = False\n",
    "\n",
    "        with _enable_get_lr_call(self):\n",
    "            if epoch is None:\n",
    "                self.last_epoch += 1\n",
    "                values = self.get_lr()\n",
    "            else:\n",
    "                warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
    "                self.last_epoch = epoch\n",
    "                if hasattr(self, \"_get_closed_form_lr\"):\n",
    "                    values = self._get_closed_form_lr()\n",
    "                else:\n",
    "                    values = self.get_lr()\n",
    "\n",
    "        for i, data in enumerate(zip(self.optimizer.param_groups, values)):\n",
    "            param_group, lr = data\n",
    "            param_group['lr'] = lr\n",
    "            self.print_lr(self.verbose, i, lr, epoch)\n",
    "\n",
    "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "class LambdaLR(_LRScheduler):\n",
    "    \"\"\"Sets the learning rate of each parameter group to the initial lr\n",
    "    times a given function. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        lr_lambda (function or list): A function which computes a multiplicative\n",
    "            factor given an integer parameter epoch, or a list of such\n",
    "            functions, one for each group in optimizer.param_groups.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer has two groups.\n",
    "        >>> lambda1 = lambda epoch: epoch // 30\n",
    "        >>> lambda2 = lambda epoch: 0.95 ** epoch\n",
    "        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if not isinstance(lr_lambda, list) and not isinstance(lr_lambda, tuple):\n",
    "            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)\n",
    "        else:\n",
    "            if len(lr_lambda) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"Expected {} lr_lambdas, but got {}\".format(\n",
    "                    len(optimizer.param_groups), len(lr_lambda)))\n",
    "            self.lr_lambdas = list(lr_lambda)\n",
    "        super(LambdaLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        The learning rate lambda functions will only be saved if they are callable objects\n",
    "        and not if they are functions or lambdas.\n",
    "\n",
    "        When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.\n",
    "        \"\"\"\n",
    "\n",
    "        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', 'lr_lambdas')}\n",
    "        state_dict['lr_lambdas'] = [None] * len(self.lr_lambdas)\n",
    "\n",
    "        for idx, fn in enumerate(self.lr_lambdas):\n",
    "            if not isinstance(fn, types.FunctionType):\n",
    "                state_dict['lr_lambdas'][idx] = fn.__dict__.copy()\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "\n",
    "        lr_lambdas = state_dict.pop('lr_lambdas')\n",
    "        self.__dict__.update(state_dict)\n",
    "        # Restore state_dict keys in order to prevent side effects\n",
    "        # https://github.com/pytorch/pytorch/issues/32756\n",
    "        state_dict['lr_lambdas'] = lr_lambdas\n",
    "\n",
    "        for idx, fn in enumerate(lr_lambdas):\n",
    "            if fn is not None:\n",
    "                self.lr_lambdas[idx].__dict__.update(fn)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "\n",
    "        return [base_lr * lmbda(self.last_epoch)\n",
    "                for lmbda, base_lr in zip(self.lr_lambdas, self.base_lrs)]\n",
    "\n",
    "\n",
    "class MultiplicativeLR(_LRScheduler):\n",
    "    \"\"\"Multiply the learning rate of each parameter group by the factor given\n",
    "    in the specified function. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        lr_lambda (function or list): A function which computes a multiplicative\n",
    "            factor given an integer parameter epoch, or a list of such\n",
    "            functions, one for each group in optimizer.param_groups.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> lmbda = lambda epoch: 0.95\n",
    "        >>> scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if not isinstance(lr_lambda, list) and not isinstance(lr_lambda, tuple):\n",
    "            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)\n",
    "        else:\n",
    "            if len(lr_lambda) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"Expected {} lr_lambdas, but got {}\".format(\n",
    "                    len(optimizer.param_groups), len(lr_lambda)))\n",
    "            self.lr_lambdas = list(lr_lambda)\n",
    "        super(MultiplicativeLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        The learning rate lambda functions will only be saved if they are callable objects\n",
    "        and not if they are functions or lambdas.\n",
    "        \"\"\"\n",
    "        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', 'lr_lambdas')}\n",
    "        state_dict['lr_lambdas'] = [None] * len(self.lr_lambdas)\n",
    "\n",
    "        for idx, fn in enumerate(self.lr_lambdas):\n",
    "            if not isinstance(fn, types.FunctionType):\n",
    "                state_dict['lr_lambdas'][idx] = fn.__dict__.copy()\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        lr_lambdas = state_dict.pop('lr_lambdas')\n",
    "        self.__dict__.update(state_dict)\n",
    "        # Restore state_dict keys in order to prevent side effects\n",
    "        # https://github.com/pytorch/pytorch/issues/32756\n",
    "        state_dict['lr_lambdas'] = lr_lambdas\n",
    "\n",
    "        for idx, fn in enumerate(lr_lambdas):\n",
    "            if fn is not None:\n",
    "                self.lr_lambdas[idx].__dict__.update(fn)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch > 0:\n",
    "            return [group['lr'] * lmbda(self.last_epoch)\n",
    "                    for lmbda, group in zip(self.lr_lambdas, self.optimizer.param_groups)]\n",
    "        else:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "class StepLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by gamma every\n",
    "    step_size epochs. Notice that such decay can happen simultaneously with\n",
    "    other changes to the learning rate from outside this scheduler. When\n",
    "    last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        step_size (int): Period of learning rate decay.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "            Default: 0.1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    "        >>> # lr = 0.05     if epoch < 30\n",
    "        >>> # lr = 0.005    if 30 <= epoch < 60\n",
    "        >>> # lr = 0.0005   if 60 <= epoch < 90\n",
    "        >>> # ...\n",
    "        >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False):\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        super(StepLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if (self.last_epoch == 0) or (self.last_epoch % self.step_size != 0):\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        return [group['lr'] * self.gamma\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [base_lr * self.gamma ** (self.last_epoch // self.step_size)\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class MultiStepLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by gamma once the\n",
    "    number of epoch reaches one of the milestones. Notice that such decay can\n",
    "    happen simultaneously with other changes to the learning rate from outside\n",
    "    this scheduler. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        milestones (list): List of epoch indices. Must be increasing.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "            Default: 0.1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    "        >>> # lr = 0.05     if epoch < 30\n",
    "        >>> # lr = 0.005    if 30 <= epoch < 80\n",
    "        >>> # lr = 0.0005   if epoch >= 80\n",
    "        >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False):\n",
    "        self.milestones = Counter(milestones)\n",
    "        self.gamma = gamma\n",
    "        super(MultiStepLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch not in self.milestones:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        return [group['lr'] * self.gamma ** self.milestones[self.last_epoch]\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        milestones = list(sorted(self.milestones.elements()))\n",
    "        return [base_lr * self.gamma ** bisect_right(milestones, self.last_epoch)\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ConstantLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by a small constant factor until the\n",
    "    number of epoch reaches a pre-defined milestone: total_iters. Notice that such decay can\n",
    "    happen simultaneously with other changes to the learning rate from outside this scheduler.\n",
    "    When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        factor (float): The number we multiply learning rate until the milestone. Default: 1./3.\n",
    "        total_iters (int): The number of steps that the scheduler decays the learning rate.\n",
    "            Default: 5.\n",
    "        last_epoch (int): The index of the last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    "        >>> # lr = 0.025   if epoch == 0\n",
    "        >>> # lr = 0.025   if epoch == 1\n",
    "        >>> # lr = 0.025   if epoch == 2\n",
    "        >>> # lr = 0.025   if epoch == 3\n",
    "        >>> # lr = 0.05    if epoch >= 4\n",
    "        >>> scheduler = ConstantLR(self.opt, factor=0.5, total_iters=4)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, factor=1.0 / 3, total_iters=5, last_epoch=-1, verbose=False):\n",
    "        if factor > 1.0 or factor < 0:\n",
    "            raise ValueError('Constant multiplicative factor expected to be between 0 and 1.')\n",
    "\n",
    "        self.factor = factor\n",
    "        self.total_iters = total_iters\n",
    "        super(ConstantLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [group['lr'] * self.factor for group in self.optimizer.param_groups]\n",
    "\n",
    "        if (self.last_epoch > self.total_iters or\n",
    "                (self.last_epoch != self.total_iters)):\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "        if (self.last_epoch == self.total_iters):\n",
    "            return [group['lr'] * (1.0 / self.factor) for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [base_lr * (self.factor + (self.last_epoch >= self.total_iters) * (1 - self.factor))\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class LinearLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by linearly changing small\n",
    "    multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters.\n",
    "    Notice that such decay can happen simultaneously with other changes to the learning rate\n",
    "    from outside this scheduler. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        start_factor (float): The number we multiply learning rate in the first epoch.\n",
    "            The multiplication factor changes towards end_factor in the following epochs.\n",
    "            Default: 1./3.\n",
    "        end_factor (float): The number we multiply learning rate at the end of linear changing\n",
    "            process. Default: 1.0.\n",
    "        total_iters (int): The number of iterations that multiplicative factor reaches to 1.\n",
    "            Default: 5.\n",
    "        last_epoch (int): The index of the last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    "        >>> # lr = 0.025    if epoch == 0\n",
    "        >>> # lr = 0.03125  if epoch == 1\n",
    "        >>> # lr = 0.0375   if epoch == 2\n",
    "        >>> # lr = 0.04375  if epoch == 3\n",
    "        >>> # lr = 0.005    if epoch >= 4\n",
    "        >>> scheduler = LinearLR(self.opt, start_factor=0.5, total_iters=4)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, start_factor=1.0 / 3, end_factor=1.0, total_iters=5, last_epoch=-1,\n",
    "                 verbose=False):\n",
    "        if start_factor > 1.0 or start_factor < 0:\n",
    "            raise ValueError('Starting multiplicative factor expected to be between 0 and 1.')\n",
    "\n",
    "        if end_factor > 1.0 or end_factor < 0:\n",
    "            raise ValueError('Ending multiplicative factor expected to be between 0 and 1.')\n",
    "\n",
    "        self.start_factor = start_factor\n",
    "        self.end_factor = end_factor\n",
    "        self.total_iters = total_iters\n",
    "        super(LinearLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [group['lr'] * self.start_factor for group in self.optimizer.param_groups]\n",
    "\n",
    "        if (self.last_epoch > self.total_iters):\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "        return [group['lr'] * (1. + (self.end_factor - self.start_factor) /\n",
    "                (self.total_iters * self.start_factor + (self.last_epoch - 1) * (self.end_factor - self.start_factor)))\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [base_lr * (self.start_factor +\n",
    "                (self.end_factor - self.start_factor) * min(self.total_iters, self.last_epoch) / self.total_iters)\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    \"\"\"Decays the learning rate of each parameter group by gamma every epoch.\n",
    "    When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, gamma, last_epoch=-1, verbose=False):\n",
    "        self.gamma = gamma\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        return [group['lr'] * self.gamma\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [base_lr * self.gamma ** self.last_epoch\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class SequentialLR(_LRScheduler):\n",
    "    \"\"\"Receives the list of schedulers that is expected to be called sequentially during\n",
    "    optimization process and milestone points that provides exact intervals to reflect\n",
    "    which scheduler is supposed to be called at a given epoch.\n",
    "\n",
    "    Args:\n",
    "        schedulers (list): List of chained schedulers.\n",
    "        milestones (list): List of integers that reflects milestone points.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 1. for all groups\n",
    "        >>> # lr = 0.1     if epoch == 0\n",
    "        >>> # lr = 0.1     if epoch == 1\n",
    "        >>> # lr = 0.9     if epoch == 2\n",
    "        >>> # lr = 0.81    if epoch == 3\n",
    "        >>> # lr = 0.729   if epoch == 4\n",
    "        >>> scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)\n",
    "        >>> scheduler2 = ExponentialLR(self.opt, gamma=0.9)\n",
    "        >>> scheduler = SequentialLR(self.opt, schedulers=[scheduler1, scheduler2], milestones=[2])\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, schedulers, milestones, last_epoch=-1, verbose=False):\n",
    "        for scheduler_idx in range(1, len(schedulers)):\n",
    "            if (schedulers[scheduler_idx].optimizer != schedulers[0].optimizer):\n",
    "                raise ValueError(\n",
    "                    \"Sequential Schedulers expects all schedulers to belong to the same optimizer, but \"\n",
    "                    \"got schedulers at index {} and {} to be different\".format(0, scheduler_idx)\n",
    "                )\n",
    "        if (len(milestones) != len(schedulers) - 1):\n",
    "            raise ValueError(\n",
    "                \"Sequential Schedulers expects number of schedulers provided to be one more \"\n",
    "                \"than the number of milestone points, but got number of schedulers {} and the \"\n",
    "                \"number of milestones to be equal to {}\".format(len(schedulers), len(milestones))\n",
    "            )\n",
    "        self._schedulers = schedulers\n",
    "        self._milestones = milestones\n",
    "        self.last_epoch = last_epoch + 1\n",
    "\n",
    "    def step(self):\n",
    "        self.last_epoch += 1\n",
    "        idx = bisect_right(self._milestones, self.last_epoch)\n",
    "        if idx > 0 and self._milestones[idx - 1] == self.last_epoch:\n",
    "            self._schedulers[idx].step(0)\n",
    "        else:\n",
    "            self._schedulers[idx].step()\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        The wrapped scheduler states will also be saved.\n",
    "        \"\"\"\n",
    "        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', '_schedulers')}\n",
    "        state_dict['_schedulers'] = [None] * len(self._schedulers)\n",
    "\n",
    "        for idx, s in enumerate(self._schedulers):\n",
    "            state_dict['_schedulers'][idx] = s.state_dict()\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        _schedulers = state_dict.pop('_schedulers')\n",
    "        self.__dict__.update(state_dict)\n",
    "        # Restore state_dict keys in order to prevent side effects\n",
    "        # https://github.com/pytorch/pytorch/issues/32756\n",
    "        state_dict['_schedulers'] = _schedulers\n",
    "\n",
    "        for idx, s in enumerate(_schedulers):\n",
    "            self._schedulers[idx].load_state_dict(s)\n",
    "\n",
    "\n",
    "class CosineAnnealingLR(_LRScheduler):\n",
    "    r\"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
    "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
    "\n",
    "    .. math::\n",
    "        \\begin{aligned}\n",
    "            \\eta_t & = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1\n",
    "            + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right),\n",
    "            & T_{cur} \\neq (2k+1)T_{max}; \\\\\n",
    "            \\eta_{t+1} & = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\n",
    "            \\left(1 - \\cos\\left(\\frac{1}{T_{max}}\\pi\\right)\\right),\n",
    "            & T_{cur} = (2k+1)T_{max}.\n",
    "        \\end{aligned}\n",
    "\n",
    "    When last_epoch=-1, sets initial lr as lr. Notice that because the schedule\n",
    "    is defined recursively, the learning rate can be simultaneously modified\n",
    "    outside this scheduler by other operators. If the learning rate is set\n",
    "    solely by this scheduler, the learning rate at each step becomes:\n",
    "\n",
    "    .. math::\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n",
    "        \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)\n",
    "\n",
    "    It has been proposed in\n",
    "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n",
    "    implements the cosine annealing part of SGDR, and not the restarts.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        T_max (int): Maximum number of iterations.\n",
    "        eta_min (float): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
    "        https://arxiv.org/abs/1608.03983\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        super(CosineAnnealingLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        elif (self.last_epoch - 1 - self.T_max) % (2 * self.T_max) == 0:\n",
    "            return [group['lr'] + (base_lr - self.eta_min) *\n",
    "                    (1 - math.cos(math.pi / self.T_max)) / 2\n",
    "                    for base_lr, group in\n",
    "                    zip(self.base_lrs, self.optimizer.param_groups)]\n",
    "        return [(1 + math.cos(math.pi * self.last_epoch / self.T_max)) /\n",
    "                (1 + math.cos(math.pi * (self.last_epoch - 1) / self.T_max)) *\n",
    "                (group['lr'] - self.eta_min) + self.eta_min\n",
    "                for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ChainedScheduler(_LRScheduler):\n",
    "    \"\"\"Chains list of learning rate schedulers. It takes a list of chainable learning\n",
    "    rate schedulers and performs consecutive step() functions belong to them by just\n",
    "    one call.\n",
    "\n",
    "    Args:\n",
    "        schedulers (list): List of chained schedulers.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 1. for all groups\n",
    "        >>> # lr = 0.09     if epoch == 0\n",
    "        >>> # lr = 0.081    if epoch == 1\n",
    "        >>> # lr = 0.729    if epoch == 2\n",
    "        >>> # lr = 0.6561   if epoch == 3\n",
    "        >>> # lr = 0.59049  if epoch >= 4\n",
    "        >>> scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)\n",
    "        >>> scheduler2 = ExponentialLR(self.opt, gamma=0.9)\n",
    "        >>> scheduler = ChainedScheduler([scheduler1, scheduler2])\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "        >>>     scheduler.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schedulers):\n",
    "        for scheduler_idx in range(1, len(schedulers)):\n",
    "            if (schedulers[scheduler_idx].optimizer != schedulers[0].optimizer):\n",
    "                raise ValueError(\n",
    "                    \"ChainedScheduler expects all schedulers to belong to the same optimizer, but \"\n",
    "                    \"got schedulers at index {} and {} to be different\".format(0, scheduler_idx)\n",
    "                )\n",
    "        self._schedulers = list(schedulers)\n",
    "\n",
    "    def step(self):\n",
    "        for scheduler in self._schedulers:\n",
    "            scheduler.step()\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        The wrapped scheduler states will also be saved.\n",
    "        \"\"\"\n",
    "        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', '_schedulers')}\n",
    "        state_dict['_schedulers'] = [None] * len(self._schedulers)\n",
    "\n",
    "        for idx, s in enumerate(self._schedulers):\n",
    "            state_dict['_schedulers'][idx] = s.state_dict()\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        _schedulers = state_dict.pop('_schedulers')\n",
    "        self.__dict__.update(state_dict)\n",
    "        # Restore state_dict keys in order to prevent side effects\n",
    "        # https://github.com/pytorch/pytorch/issues/32756\n",
    "        state_dict['_schedulers'] = _schedulers\n",
    "\n",
    "        for idx, s in enumerate(_schedulers):\n",
    "            self._schedulers[idx].load_state_dict(s)\n",
    "\n",
    "\n",
    "class ReduceLROnPlateau(object):\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    Models often benefit from reducing the learning rate by a factor\n",
    "    of 2-10 once learning stagnates. This scheduler reads a metrics\n",
    "    quantity and if no improvement is seen for a 'patience' number\n",
    "    of epochs, the learning rate is reduced.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        mode (str): One of `min`, `max`. In `min` mode, lr will\n",
    "            be reduced when the quantity monitored has stopped\n",
    "            decreasing; in `max` mode it will be reduced when the\n",
    "            quantity monitored has stopped increasing. Default: 'min'.\n",
    "        factor (float): Factor by which the learning rate will be\n",
    "            reduced. new_lr = lr * factor. Default: 0.1.\n",
    "        patience (int): Number of epochs with no improvement after\n",
    "            which learning rate will be reduced. For example, if\n",
    "            `patience = 2`, then we will ignore the first 2 epochs\n",
    "            with no improvement, and will only decrease the LR after the\n",
    "            3rd epoch if the loss still hasn't improved then.\n",
    "            Default: 10.\n",
    "        threshold (float): Threshold for measuring the new optimum,\n",
    "            to only focus on significant changes. Default: 1e-4.\n",
    "        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n",
    "            dynamic_threshold = best * ( 1 + threshold ) in 'max'\n",
    "            mode or best * ( 1 - threshold ) in `min` mode.\n",
    "            In `abs` mode, dynamic_threshold = best + threshold in\n",
    "            `max` mode or best - threshold in `min` mode. Default: 'rel'.\n",
    "        cooldown (int): Number of epochs to wait before resuming\n",
    "            normal operation after lr has been reduced. Default: 0.\n",
    "        min_lr (float or list): A scalar or a list of scalars. A\n",
    "            lower bound on the learning rate of all param groups\n",
    "            or each group respectively. Default: 0.\n",
    "        eps (float): Minimal decay applied to lr. If the difference\n",
    "            between new and old lr is smaller than eps, the update is\n",
    "            ignored. Default: 1e-8.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     train(...)\n",
    "        >>>     val_loss = validate(...)\n",
    "        >>>     # Note that step should be called after validate()\n",
    "        >>>     scheduler.step(val_loss)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, mode='min', factor=0.1, patience=10,\n",
    "                 threshold=1e-4, threshold_mode='rel', cooldown=0,\n",
    "                 min_lr=0, eps=1e-8, verbose=False):\n",
    "\n",
    "        if factor >= 1.0:\n",
    "            raise ValueError('Factor should be < 1.0.')\n",
    "        self.factor = factor\n",
    "\n",
    "        # Attach optimizer\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(min_lr, list) or isinstance(min_lr, tuple):\n",
    "            if len(min_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} min_lrs, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(min_lr)))\n",
    "            self.min_lrs = list(min_lr)\n",
    "        else:\n",
    "            self.min_lrs = [min_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.threshold_mode = threshold_mode\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = None\n",
    "        self.mode_worse = None  # the worse value for the chosen mode\n",
    "        self.eps = eps\n",
    "        self.last_epoch = 0\n",
    "        self._init_is_better(mode=mode, threshold=threshold,\n",
    "                             threshold_mode=threshold_mode)\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets num_bad_epochs counter and cooldown counter.\"\"\"\n",
    "        self.best = self.mode_worse\n",
    "        self.cooldown_counter = 0\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, metrics, epoch=None):\n",
    "        # convert `metrics` to float, in case it's a zero-dim Tensor\n",
    "        current = float(metrics)\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        else:\n",
    "            warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
    "        self.last_epoch = epoch\n",
    "\n",
    "        if self.is_better(current, self.best):\n",
    "            self.best = current\n",
    "            self.num_bad_epochs = 0\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.in_cooldown:\n",
    "            self.cooldown_counter -= 1\n",
    "            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\n",
    "\n",
    "        if self.num_bad_epochs > self.patience:\n",
    "            self._reduce_lr(epoch)\n",
    "            self.cooldown_counter = self.cooldown\n",
    "            self.num_bad_epochs = 0\n",
    "\n",
    "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _reduce_lr(self, epoch):\n",
    "        for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "            old_lr = float(param_group['lr'])\n",
    "            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n",
    "            if old_lr - new_lr > self.eps:\n",
    "                param_group['lr'] = new_lr\n",
    "                if self.verbose:\n",
    "                    print('Epoch {:5d}: reducing learning rate'\n",
    "                          ' of group {} to {:.4e}.'.format(epoch, i, new_lr))\n",
    "\n",
    "    @property\n",
    "    def in_cooldown(self):\n",
    "        return self.cooldown_counter > 0\n",
    "\n",
    "    def is_better(self, a, best):\n",
    "        if self.mode == 'min' and self.threshold_mode == 'rel':\n",
    "            rel_epsilon = 1. - self.threshold\n",
    "            return a < best * rel_epsilon\n",
    "\n",
    "        elif self.mode == 'min' and self.threshold_mode == 'abs':\n",
    "            return a < best - self.threshold\n",
    "\n",
    "        elif self.mode == 'max' and self.threshold_mode == 'rel':\n",
    "            rel_epsilon = self.threshold + 1.\n",
    "            return a > best * rel_epsilon\n",
    "\n",
    "        else:  # mode == 'max' and epsilon_mode == 'abs':\n",
    "            return a > best + self.threshold\n",
    "\n",
    "    def _init_is_better(self, mode, threshold, threshold_mode):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if threshold_mode not in {'rel', 'abs'}:\n",
    "            raise ValueError('threshold mode ' + threshold_mode + ' is unknown!')\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.mode_worse = inf\n",
    "        else:  # mode == 'max':\n",
    "            self.mode_worse = -inf\n",
    "\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.threshold_mode = threshold_mode\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.__dict__.update(state_dict)\n",
    "        self._init_is_better(mode=self.mode, threshold=self.threshold, threshold_mode=self.threshold_mode)\n",
    "\n",
    "\n",
    "class CyclicLR(_LRScheduler):\n",
    "    r\"\"\"Sets the learning rate of each parameter group according to\n",
    "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
    "    rate between two boundaries with a constant frequency, as detailed in\n",
    "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
    "    The distance between the two boundaries can be scaled on a per-iteration\n",
    "    or per-cycle basis.\n",
    "\n",
    "    Cyclical learning rate policy changes the learning rate after every batch.\n",
    "    `step` should be called after a batch has been used for training.\n",
    "\n",
    "    This class has three built-in policies, as put forth in the paper:\n",
    "\n",
    "    * \"triangular\": A basic triangular cycle without amplitude scaling.\n",
    "    * \"triangular2\": A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    * \"exp_range\": A cycle that scales initial amplitude by :math:`\\text{gamma}^{\\text{cycle iterations}}`\n",
    "      at each cycle iteration.\n",
    "\n",
    "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        base_lr (float or list): Initial learning rate which is the\n",
    "            lower boundary in the cycle for each parameter group.\n",
    "        max_lr (float or list): Upper learning rate boundaries in the cycle\n",
    "            for each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size_up (int): Number of training iterations in the\n",
    "            increasing half of a cycle. Default: 2000\n",
    "        step_size_down (int): Number of training iterations in the\n",
    "            decreasing half of a cycle. If step_size_down is None,\n",
    "            it is set to step_size_up. Default: None\n",
    "        mode (str): One of {triangular, triangular2, exp_range}.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "            Default: 'triangular'\n",
    "        gamma (float): Constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "            Default: 1.0\n",
    "        scale_fn (function): Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            If specified, then 'mode' is ignored.\n",
    "            Default: None\n",
    "        scale_mode (str): {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle).\n",
    "            Default: 'cycle'\n",
    "        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
    "            to learning rate between 'base_momentum' and 'max_momentum'.\n",
    "            Default: True\n",
    "        base_momentum (float or list): Lower momentum boundaries in the cycle\n",
    "            for each parameter group. Note that momentum is cycled inversely\n",
    "            to learning rate; at the peak of a cycle, momentum is\n",
    "            'base_momentum' and learning rate is 'max_lr'.\n",
    "            Default: 0.8\n",
    "        max_momentum (float or list): Upper momentum boundaries in the cycle\n",
    "            for each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_momentum - base_momentum).\n",
    "            The momentum at any cycle is the difference of max_momentum\n",
    "            and some scaling of the amplitude; therefore\n",
    "            base_momentum may not actually be reached depending on\n",
    "            scaling function. Note that momentum is cycled inversely\n",
    "            to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
    "            and learning rate is 'base_lr'\n",
    "            Default: 0.9\n",
    "        last_epoch (int): The index of the last batch. This parameter is used when\n",
    "            resuming a training job. Since `step()` should be invoked after each\n",
    "            batch instead of after each epoch, this number represents the total\n",
    "            number of *batches* computed, not the total number of epochs computed.\n",
    "            When last_epoch=-1, the schedule is started from the beginning.\n",
    "            Default: -1\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         train_batch(...)\n",
    "        >>>         scheduler.step()\n",
    "\n",
    "\n",
    "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer,\n",
    "                 base_lr,\n",
    "                 max_lr,\n",
    "                 step_size_up=2000,\n",
    "                 step_size_down=None,\n",
    "                 mode='triangular',\n",
    "                 gamma=1.,\n",
    "                 scale_fn=None,\n",
    "                 scale_mode='cycle',\n",
    "                 cycle_momentum=True,\n",
    "                 base_momentum=0.8,\n",
    "                 max_momentum=0.9,\n",
    "                 last_epoch=-1,\n",
    "                 verbose=False):\n",
    "\n",
    "        # Attach optimizer\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        base_lrs = self._format_param('base_lr', optimizer, base_lr)\n",
    "        if last_epoch == -1:\n",
    "            for lr, group in zip(base_lrs, optimizer.param_groups):\n",
    "                group['lr'] = lr\n",
    "\n",
    "        self.max_lrs = self._format_param('max_lr', optimizer, max_lr)\n",
    "\n",
    "        step_size_up = float(step_size_up)\n",
    "        step_size_down = float(step_size_down) if step_size_down is not None else step_size_up\n",
    "        self.total_size = step_size_up + step_size_down\n",
    "        self.step_ratio = step_size_up / self.total_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.cycle_momentum = cycle_momentum\n",
    "        if cycle_momentum:\n",
    "            if 'momentum' not in optimizer.defaults:\n",
    "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
    "\n",
    "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
    "            if last_epoch == -1:\n",
    "                for momentum, group in zip(base_momentums, optimizer.param_groups):\n",
    "                    group['momentum'] = momentum\n",
    "            self.base_momentums = [group['momentum'] for group in optimizer.param_groups]\n",
    "            self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
    "\n",
    "        super(CyclicLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "        self.base_lrs = base_lrs\n",
    "\n",
    "    def _format_param(self, name, optimizer, param):\n",
    "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
    "        if isinstance(param, (list, tuple)):\n",
    "            if len(param) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
    "                    len(optimizer.param_groups), name, len(param)))\n",
    "            return param\n",
    "        else:\n",
    "            return [param] * len(optimizer.param_groups)\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Calculates the learning rate at batch index. This function treats\n",
    "        `self.last_epoch` as the last batch index.\n",
    "\n",
    "        If `self.cycle_momentum` is ``True``, this function has a side effect of\n",
    "        updating the optimizer's momentum.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        cycle = math.floor(1 + self.last_epoch / self.total_size)\n",
    "        x = 1. + self.last_epoch / self.total_size - cycle\n",
    "        if x <= self.step_ratio:\n",
    "            scale_factor = x / self.step_ratio\n",
    "        else:\n",
    "            scale_factor = (x - 1) / (self.step_ratio - 1)\n",
    "\n",
    "        lrs = []\n",
    "        for base_lr, max_lr in zip(self.base_lrs, self.max_lrs):\n",
    "            base_height = (max_lr - base_lr) * scale_factor\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_epoch)\n",
    "            lrs.append(lr)\n",
    "\n",
    "        if self.cycle_momentum:\n",
    "            momentums = []\n",
    "            for base_momentum, max_momentum in zip(self.base_momentums, self.max_momentums):\n",
    "                base_height = (max_momentum - base_momentum) * scale_factor\n",
    "                if self.scale_mode == 'cycle':\n",
    "                    momentum = max_momentum - base_height * self.scale_fn(cycle)\n",
    "                else:\n",
    "                    momentum = max_momentum - base_height * self.scale_fn(self.last_epoch)\n",
    "                momentums.append(momentum)\n",
    "            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n",
    "                param_group['momentum'] = momentum\n",
    "\n",
    "        return lrs\n",
    "\n",
    "\n",
    "class CosineAnnealingWarmRestarts(_LRScheduler):\n",
    "    r\"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr, :math:`T_{cur}`\n",
    "    is the number of epochs since the last restart and :math:`T_{i}` is the number\n",
    "    of epochs between two warm restarts in SGDR:\n",
    "\n",
    "    .. math::\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n",
    "        \\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)\n",
    "\n",
    "    When :math:`T_{cur}=T_{i}`, set :math:`\\eta_t = \\eta_{min}`.\n",
    "    When :math:`T_{cur}=0` after restart, set :math:`\\eta_t=\\eta_{max}`.\n",
    "\n",
    "    It has been proposed in\n",
    "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        T_0 (int): Number of iterations for the first restart.\n",
    "        T_mult (int, optional): A factor increases :math:`T_{i}` after a restart. Default: 1.\n",
    "        eta_min (float, optional): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int, optional): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
    "        https://arxiv.org/abs/1608.03983\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        self.T_0 = T_0\n",
    "        self.T_i = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmRestarts, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        return [self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * self.T_cur / self.T_i)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        \"\"\"Step could be called after every batch update\n",
    "\n",
    "        Example:\n",
    "            >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n",
    "            >>> iters = len(dataloader)\n",
    "            >>> for epoch in range(20):\n",
    "            >>>     for i, sample in enumerate(dataloader):\n",
    "            >>>         inputs, labels = sample['inputs'], sample['labels']\n",
    "            >>>         optimizer.zero_grad()\n",
    "            >>>         outputs = net(inputs)\n",
    "            >>>         loss = criterion(outputs, labels)\n",
    "            >>>         loss.backward()\n",
    "            >>>         optimizer.step()\n",
    "            >>>         scheduler.step(epoch + i / iters)\n",
    "\n",
    "        This function can be called in an interleaved way.\n",
    "\n",
    "        Example:\n",
    "            >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n",
    "            >>> for epoch in range(20):\n",
    "            >>>     scheduler.step()\n",
    "            >>> scheduler.step(26)\n",
    "            >>> scheduler.step() # scheduler.step(27), instead of scheduler(20)\n",
    "        \"\"\"\n",
    "\n",
    "        if epoch is None and self.last_epoch < 0:\n",
    "            epoch = 0\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = self.T_i * self.T_mult\n",
    "        else:\n",
    "            if epoch < 0:\n",
    "                raise ValueError(\"Expected non-negative epoch, but got {}\".format(epoch))\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "\n",
    "        class _enable_get_lr_call:\n",
    "\n",
    "            def __init__(self, o):\n",
    "                self.o = o\n",
    "\n",
    "            def __enter__(self):\n",
    "                self.o._get_lr_called_within_step = True\n",
    "                return self\n",
    "\n",
    "            def __exit__(self, type, value, traceback):\n",
    "                self.o._get_lr_called_within_step = False\n",
    "                return self\n",
    "\n",
    "        with _enable_get_lr_call(self):\n",
    "            for i, data in enumerate(zip(self.optimizer.param_groups, self.get_lr())):\n",
    "                param_group, lr = data\n",
    "                param_group['lr'] = lr\n",
    "                self.print_lr(self.verbose, i, lr, epoch)\n",
    "\n",
    "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "class OneCycleLR(_LRScheduler):\n",
    "    r\"\"\"Sets the learning rate of each parameter group according to the\n",
    "    1cycle learning rate policy. The 1cycle policy anneals the learning\n",
    "    rate from an initial learning rate to some maximum learning rate and then\n",
    "    from that maximum learning rate to some minimum learning rate much lower\n",
    "    than the initial learning rate.\n",
    "    This policy was initially described in the paper `Super-Convergence:\n",
    "    Very Fast Training of Neural Networks Using Large Learning Rates`_.\n",
    "\n",
    "    The 1cycle learning rate policy changes the learning rate after every batch.\n",
    "    `step` should be called after a batch has been used for training.\n",
    "\n",
    "    This scheduler is not chainable.\n",
    "\n",
    "    Note also that the total number of steps in the cycle can be determined in one\n",
    "    of two ways (listed in order of precedence):\n",
    "\n",
    "    #. A value for total_steps is explicitly provided.\n",
    "    #. A number of epochs (epochs) and a number of steps per epoch\n",
    "       (steps_per_epoch) are provided.\n",
    "       In this case, the number of total steps is inferred by\n",
    "       total_steps = epochs * steps_per_epoch\n",
    "\n",
    "    You must either provide a value for total_steps or provide a value for both\n",
    "    epochs and steps_per_epoch.\n",
    "\n",
    "    The default behaviour of this scheduler follows the fastai implementation of 1cycle, which\n",
    "    claims that \"unpublished work has shown even better results by using only two phases\". To\n",
    "    mimic the behaviour of the original paper instead, set ``three_phase=True``.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        max_lr (float or list): Upper learning rate boundaries in the cycle\n",
    "            for each parameter group.\n",
    "        total_steps (int): The total number of steps in the cycle. Note that\n",
    "            if a value is not provided here, then it must be inferred by providing\n",
    "            a value for epochs and steps_per_epoch.\n",
    "            Default: None\n",
    "        epochs (int): The number of epochs to train for. This is used along\n",
    "            with steps_per_epoch in order to infer the total number of steps in the cycle\n",
    "            if a value for total_steps is not provided.\n",
    "            Default: None\n",
    "        steps_per_epoch (int): The number of steps per epoch to train for. This is\n",
    "            used along with epochs in order to infer the total number of steps in the\n",
    "            cycle if a value for total_steps is not provided.\n",
    "            Default: None\n",
    "        pct_start (float): The percentage of the cycle (in number of steps) spent\n",
    "            increasing the learning rate.\n",
    "            Default: 0.3\n",
    "        anneal_strategy (str): {'cos', 'linear'}\n",
    "            Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for\n",
    "            linear annealing.\n",
    "            Default: 'cos'\n",
    "        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
    "            to learning rate between 'base_momentum' and 'max_momentum'.\n",
    "            Default: True\n",
    "        base_momentum (float or list): Lower momentum boundaries in the cycle\n",
    "            for each parameter group. Note that momentum is cycled inversely\n",
    "            to learning rate; at the peak of a cycle, momentum is\n",
    "            'base_momentum' and learning rate is 'max_lr'.\n",
    "            Default: 0.85\n",
    "        max_momentum (float or list): Upper momentum boundaries in the cycle\n",
    "            for each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_momentum - base_momentum).\n",
    "            Note that momentum is cycled inversely\n",
    "            to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
    "            and learning rate is 'base_lr'\n",
    "            Default: 0.95\n",
    "        div_factor (float): Determines the initial learning rate via\n",
    "            initial_lr = max_lr/div_factor\n",
    "            Default: 25\n",
    "        final_div_factor (float): Determines the minimum learning rate via\n",
    "            min_lr = initial_lr/final_div_factor\n",
    "            Default: 1e4\n",
    "        three_phase (bool): If ``True``, use a third phase of the schedule to annihilate the\n",
    "            learning rate according to 'final_div_factor' instead of modifying the second\n",
    "            phase (the first two phases will be symmetrical about the step indicated by\n",
    "            'pct_start').\n",
    "        last_epoch (int): The index of the last batch. This parameter is used when\n",
    "            resuming a training job. Since `step()` should be invoked after each\n",
    "            batch instead of after each epoch, this number represents the total\n",
    "            number of *batches* computed, not the total number of epochs computed.\n",
    "            When last_epoch=-1, the schedule is started from the beginning.\n",
    "            Default: -1\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         train_batch(...)\n",
    "        >>>         scheduler.step()\n",
    "\n",
    "\n",
    "    .. _Super-Convergence\\: Very Fast Training of Neural Networks Using Large Learning Rates:\n",
    "        https://arxiv.org/abs/1708.07120\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 optimizer,\n",
    "                 max_lr,\n",
    "                 total_steps=None,\n",
    "                 epochs=None,\n",
    "                 steps_per_epoch=None,\n",
    "                 pct_start=0.3,\n",
    "                 anneal_strategy='cos',\n",
    "                 cycle_momentum=True,\n",
    "                 base_momentum=0.85,\n",
    "                 max_momentum=0.95,\n",
    "                 div_factor=25.,\n",
    "                 final_div_factor=1e4,\n",
    "                 three_phase=False,\n",
    "                 last_epoch=-1,\n",
    "                 verbose=False):\n",
    "\n",
    "        # Validate optimizer\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Validate total_steps\n",
    "        if total_steps is None and epochs is None and steps_per_epoch is None:\n",
    "            raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n",
    "        elif total_steps is not None:\n",
    "            if total_steps <= 0 or not isinstance(total_steps, int):\n",
    "                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n",
    "            self.total_steps = total_steps\n",
    "        else:\n",
    "            if epochs <= 0 or not isinstance(epochs, int):\n",
    "                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n",
    "            if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n",
    "                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n",
    "            self.total_steps = epochs * steps_per_epoch\n",
    "\n",
    "        if three_phase:\n",
    "            self._schedule_phases = [\n",
    "                {\n",
    "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
    "                    'start_lr': 'initial_lr',\n",
    "                    'end_lr': 'max_lr',\n",
    "                    'start_momentum': 'max_momentum',\n",
    "                    'end_momentum': 'base_momentum',\n",
    "                },\n",
    "                {\n",
    "                    'end_step': float(2 * pct_start * self.total_steps) - 2,\n",
    "                    'start_lr': 'max_lr',\n",
    "                    'end_lr': 'initial_lr',\n",
    "                    'start_momentum': 'base_momentum',\n",
    "                    'end_momentum': 'max_momentum',\n",
    "                },\n",
    "                {\n",
    "                    'end_step': self.total_steps - 1,\n",
    "                    'start_lr': 'initial_lr',\n",
    "                    'end_lr': 'min_lr',\n",
    "                    'start_momentum': 'max_momentum',\n",
    "                    'end_momentum': 'max_momentum',\n",
    "                },\n",
    "            ]\n",
    "        else:\n",
    "            self._schedule_phases = [\n",
    "                {\n",
    "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
    "                    'start_lr': 'initial_lr',\n",
    "                    'end_lr': 'max_lr',\n",
    "                    'start_momentum': 'max_momentum',\n",
    "                    'end_momentum': 'base_momentum',\n",
    "                },\n",
    "                {\n",
    "                    'end_step': self.total_steps - 1,\n",
    "                    'start_lr': 'max_lr',\n",
    "                    'end_lr': 'min_lr',\n",
    "                    'start_momentum': 'base_momentum',\n",
    "                    'end_momentum': 'max_momentum',\n",
    "                },\n",
    "            ]\n",
    "\n",
    "        # Validate pct_start\n",
    "        if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n",
    "            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n",
    "\n",
    "        # Validate anneal_strategy\n",
    "        if anneal_strategy not in ['cos', 'linear']:\n",
    "            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n",
    "        elif anneal_strategy == 'cos':\n",
    "            self.anneal_func = self._annealing_cos\n",
    "        elif anneal_strategy == 'linear':\n",
    "            self.anneal_func = self._annealing_linear\n",
    "\n",
    "        # Initialize learning rate variables\n",
    "        max_lrs = self._format_param('max_lr', self.optimizer, max_lr)\n",
    "        if last_epoch == -1:\n",
    "            for idx, group in enumerate(self.optimizer.param_groups):\n",
    "                group['initial_lr'] = max_lrs[idx] / div_factor\n",
    "                group['max_lr'] = max_lrs[idx]\n",
    "                group['min_lr'] = group['initial_lr'] / final_div_factor\n",
    "\n",
    "        # Initialize momentum variables\n",
    "        self.cycle_momentum = cycle_momentum\n",
    "        if self.cycle_momentum:\n",
    "            if 'momentum' not in self.optimizer.defaults and 'betas' not in self.optimizer.defaults:\n",
    "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
    "            self.use_beta1 = 'betas' in self.optimizer.defaults\n",
    "            max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
    "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
    "            if last_epoch == -1:\n",
    "                for m_momentum, b_momentum, group in zip(max_momentums, base_momentums, optimizer.param_groups):\n",
    "                    if self.use_beta1:\n",
    "                        _, beta2 = group['betas']\n",
    "                        group['betas'] = (m_momentum, beta2)\n",
    "                    else:\n",
    "                        group['momentum'] = m_momentum\n",
    "                    group['max_momentum'] = m_momentum\n",
    "                    group['base_momentum'] = b_momentum\n",
    "\n",
    "        super(OneCycleLR, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def _format_param(self, name, optimizer, param):\n",
    "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
    "        if isinstance(param, (list, tuple)):\n",
    "            if len(param) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
    "                    len(optimizer.param_groups), name, len(param)))\n",
    "            return param\n",
    "        else:\n",
    "            return [param] * len(optimizer.param_groups)\n",
    "\n",
    "    def _annealing_cos(self, start, end, pct):\n",
    "        \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "        cos_out = math.cos(math.pi * pct) + 1\n",
    "        return end + (start - end) / 2.0 * cos_out\n",
    "\n",
    "    def _annealing_linear(self, start, end, pct):\n",
    "        \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "        return (end - start) * pct + start\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        lrs = []\n",
    "        step_num = self.last_epoch\n",
    "\n",
    "        if step_num > self.total_steps:\n",
    "            raise ValueError(\"Tried to step {} times. The specified number of total steps is {}\"\n",
    "                             .format(step_num + 1, self.total_steps))\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            start_step = 0\n",
    "            for i, phase in enumerate(self._schedule_phases):\n",
    "                end_step = phase['end_step']\n",
    "                if step_num <= end_step or i == len(self._schedule_phases) - 1:\n",
    "                    pct = (step_num - start_step) / (end_step - start_step)\n",
    "                    computed_lr = self.anneal_func(group[phase['start_lr']], group[phase['end_lr']], pct)\n",
    "                    if self.cycle_momentum:\n",
    "                        computed_momentum = self.anneal_func(group[phase['start_momentum']], group[phase['end_momentum']], pct)\n",
    "                    break\n",
    "                start_step = phase['end_step']\n",
    "\n",
    "            lrs.append(computed_lr)\n",
    "            if self.cycle_momentum:\n",
    "                if self.use_beta1:\n",
    "                    _, beta2 = group['betas']\n",
    "                    group['betas'] = (computed_momentum, beta2)\n",
    "                else:\n",
    "                    group['momentum'] = computed_momentum\n",
    "\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## WarmupLRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def linear_warmup_decay(warmup_steps, total_steps, cosine=True, linear=False):\n",
    "    \"\"\"Linear warmup for warmup_steps, optionally with cosine annealing or linear decay to 0 at total_steps.\"\"\"\n",
    "    assert not (linear and cosine)\n",
    "\n",
    "    def fn(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "\n",
    "        if not (cosine or linear):\n",
    "            # no decay\n",
    "            return 1.0\n",
    "\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        if cosine:\n",
    "            # cosine decay\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "        # linear decay\n",
    "        return 1.0 - progress\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "\n",
    "class DelayedCosineAnnealingWarmRestarts(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "    def __init__(self,\n",
    "                 optimizer, \n",
    "                 T_0,\n",
    "                 T_mult=2,\n",
    "                 eta_min=0,\n",
    "                 start_epoch=0):\n",
    "        self.start_epoch = start_epoch\n",
    "        super().__init__(optimizer=optimizer, \n",
    "                         T_0=T_0,\n",
    "                         T_mult=T_mult,\n",
    "                         eta_min=eta_min)\n",
    "        \n",
    "        \n",
    "    def step(self, epoch=0):\n",
    "        if epoch >= self.start_epoch:\n",
    "            super().step(epoch=epoch-self.start_epoch)\n",
    "\n",
    "\n",
    "# lr_sched = linear_warmup_decay(warmup_steps, total_steps, cosine=True, linear=False)\n",
    "# lr_sched = linear_warmup_decay(warmup_steps, total_steps, cosine=False, linear=True)\n",
    "# plt.plot(range(40), list(map(lr_sched, range(40))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TestModel(BoringModel):\n",
    "    def __init__(self):\n",
    "            \"\"\"Testing PL Module.\n",
    "            Use as follows:\n",
    "            - subclass\n",
    "            - modify the behavior for what you want\n",
    "            class TestModel(BaseTestModel):\n",
    "                def training_step(...):\n",
    "                    # do your own thing\n",
    "            or:\n",
    "            model = BaseTestModel()\n",
    "            model.training_epoch_end = None\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.backbone = torch.nn.Linear(32, 32)\n",
    "            self.head = torch.nn.Linear(32, 2)\n",
    "            self.layer = torch.nn.Sequential(OrderedDict({\"backbone\":self.backbone,\n",
    "                                                          \"head\":self.head}))\n",
    "            \n",
    "            self.lr_hist = {\"backbone\":[],\n",
    "                            \"head\":[]}\n",
    "\n",
    "\n",
    "    def training_step_end(self, training_step_outputs):\n",
    "#         print(dir(self.trainer))\n",
    "        lr_groups = [g['lr'] for g in self.trainer.optimizers[0].param_groups]\n",
    "        self.lr_hist[\"backbone\"].append(lr_groups) #optimizer.param_groups[0][\"lr\"])\n",
    "        self.lr_hist[\"head\"].append(lr_groups) #optimizer.param_groups[1][\"lr\"])\n",
    "\n",
    "        return training_step_outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\n",
    "        optim_lr = 0.1\n",
    "        optimizer = torch.optim.SGD([{\"params\":self.backbone.parameters(), \"lr\":optim_lr*0.1, \"weight_decay\": 0.01},\n",
    "                                     {\"params\":self.head.parameters(), \"lr\":optim_lr, \"weight_decay\": 0.01}])\n",
    "        warmup_steps = 5\n",
    "        total_steps = 20\n",
    "        lr_lambda = linear_warmup_decay(warmup_steps, total_steps, cosine=True, linear=False)\n",
    "        warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "        return [optimizer], [{\"scheduler\":warmup_scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         min_lr = optim_lr*0.1*0.1\n",
    "#         decay_scheduler = DelayedCosineAnnealingWarmRestarts(optimizer, \n",
    "#                                                              T_0=1,\n",
    "#                                                              T_mult=2,\n",
    "#                                                              eta_min=min_lr,\n",
    "#                                                              start_epoch=warmup_steps)\n",
    "#         return [optimizer], [{\"scheduler\":warmup_scheduler, \"interval\": \"epoch\"},\n",
    "#                              {\"scheduler\":decay_scheduler, \"interval\": \"epoch\"}]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def configure_schedulers(optimizer,\n",
    "                         config):\n",
    "    \n",
    "    if config.scheduler_type == \"linear_warmup_cosine_decay\":\n",
    "        warmup_steps = config.get(\"warmup_steps\", 5)\n",
    "        total_steps = config.get(\"total_steps\", 20)\n",
    "        lr_lambda = linear_warmup_decay(warmup_steps, total_steps, cosine=True, linear=False)\n",
    "        warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "        return [{\"scheduler\":warmup_scheduler, \"interval\": \"epoch\"}]\n",
    "    elif config.scheduler_type == \"linear_warmup_cosine_decay_w_warm_restarts\":\n",
    "        warmup_steps = config.get(\"warmup_steps\", 5)\n",
    "        total_steps = config.get(\"total_steps\", 20)\n",
    "        lr_lambda = linear_warmup_decay(warmup_steps, total_steps, cosine=True, linear=False)\n",
    "        warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "        min_lr = config.get(\"min_lr\", )\n",
    "        decay_scheduler = DelayedCosineAnnealingWarmRestarts(optimizer,\n",
    "                                                             T_0=config.get(\"T_0\",1),\n",
    "                                                             T_mult=config.get(\"T_mult\", 2),\n",
    "                                                             eta_min=config.get(\"eta_min\",0),\n",
    "                                                             start_epoch=config.get(\"start_epoch\", warmup_steps))\n",
    "        return [optimizer], [{\"scheduler\":warmup_scheduler, \"interval\": \"epoch\"},\n",
    "                             {\"scheduler\":decay_scheduler, \"interval\": \"epoch\"}]\n",
    "    else:\n",
    "        raise ConfigurationError(f\"Misconfigured Scheduler config:{config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_lr = 0.3\n",
    "# base_lr = 0.1\n",
    "num_epochs = 40\n",
    "\n",
    "model = TestModel()\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    limit_train_batches=1,\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set_style(\"talk\")\n",
    "sns.set_theme(context=\"talk\")\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(16,12))\n",
    "ax.plot(t, model.lr_hist[\"backbone\"], label=\"backbone\", alpha=0.5)\n",
    "ax.plot(t, model.lr_hist[\"head\"], label=\"head\", alpha=0.5)\n",
    "ax.set_xlabel(\"epochs\")\n",
    "ax.set_ylabel(\"lr\")\n",
    "plt.suptitle(\"LR Warmup -> CosineAnnealing w/ WarmRestarts\")\n",
    "\n",
    "plt.savefig(\"LR Warmup-CosineAnnealing w WarmRestarts.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler_1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.1*optim_lr*0.1)\n",
    "\n",
    "# lr_scheduler_1 = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "#                                                                       T_0=5,\n",
    "#                                                                       T_mult=2,\n",
    "#                                                                       eta_min=0.1*optim_lr*0.1)\n",
    "\n",
    "warmup_lambda = \n",
    "\n",
    "# lr_scheduler_1 = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
    "\n",
    "\n",
    "\n",
    "# lr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(\n",
    "#     optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=1, step_size_down=3\n",
    "# )\n",
    "\n",
    "\n",
    "t = list(range(num_epochs))\n",
    "lr_hist = {\"backbone\":[],\n",
    "           \"head\":[]}\n",
    "\n",
    "for i in t:\n",
    "#     if i <= lr_scheduler_1.T_max:\n",
    "#     if i <= lr_scheduler_1.T_0:\n",
    "    lr_scheduler_1.step()\n",
    "#     else:\n",
    "#         lr_scheduler_2.step()\n",
    "    lr_hist[\"backbone\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "    lr_hist[\"head\"].append(optimizer.param_groups[1][\"lr\"])\n",
    "#     print(f\"last_lr={last_lr}\")\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(1,1,figsize=(12,12))\n",
    "ax.plot(t, lr_hist[\"backbone\"], label=\"backbone\", alpha=0.5)\n",
    "ax.plot(t, lr_hist[\"head\"], label=\"head\", alpha=0.5)\n",
    "# plt.xaxis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in optimizer.param_groups:\n",
    "    group.setdefault('initial_lr', group['lr'])\n",
    "    print({k:v for k,v in group.items() if k!=\"params\"})\n",
    "base_lrs = [group['initial_lr'] for group in optimizer.param_groups]\n",
    "\n",
    "print(base_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    for k,v in g.items():\n",
    "        if k != \"params\":\n",
    "            print(k,f\"{v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.setdefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pp\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.Logger(__name__)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import glob\n",
    "import hydra\n",
    "from collections import OrderedDict\n",
    "from typing import *\n",
    "\n",
    "from lightning_hydra_classifiers.models.transfer import *\n",
    "from rich import print as pp\n",
    "from lightning_hydra_classifiers.utils.model_utils import count_parameters, collect_results\n",
    "from lightning_hydra_classifiers.utils.metric_utils import get_per_class_metrics, get_scalar_metrics\n",
    "from lightning_hydra_classifiers.models.backbones.backbone import build_model\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "from lightning_hydra_classifiers.scripts.multitask.train import MultiTaskDataModule, LitMultiTaskModule, ImagePredictionLogger, train_task,  CIFAR10DataModule, run_multitask_test, load_data_and_model, load_data, resolve_config, configure_callbacks, configure_loggers, configure_trainer\n",
    "from lightning_hydra_classifiers.data.datasets.common import toPIL\n",
    "from lightning_hydra_classifiers.utils.etl_utils import ETL\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from lightning_hydra_classifiers.scripts.pretrain import lr_tuner\n",
    "\n",
    "from lightning_hydra_classifiers.scripts.multitask.train import configure_callbacks, configure_loggers#, configure_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Developing Early Stopping multi-stage subclass for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### trying out built in lightning tests for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(Path(pl.__file__).parent)\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%writefile first_test.py\n",
    "\n",
    "# # See the License for the specific language governing permissions and\n",
    "# # limitations under the License.\n",
    "# from collections import OrderedDict\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import pytest\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.optim import Optimizer, SGD\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "# from pytorch_lightning.callbacks import BackboneFinetuning, BaseFinetuning, ModelCheckpoint\n",
    "# from pytorch_lightning.callbacks.base import Callback\n",
    "# from tests.helpers import BoringModel, RandomDataset\n",
    "\n",
    "# from typing import *\n",
    "\n",
    "# # class RandomDataset(torch.utils.data.Dataset):\n",
    "# #     def __init__(self, num_samples=2000, shape=(3,64,64)):\n",
    "# #         self.num_samples = num_samples\n",
    "# #         self.shape = shape\n",
    "# #         self.data = torch.randn(num_samples, *shape)\n",
    "\n",
    "# #     def __getitem__(self, index):\n",
    "# #         return self.data[index]\n",
    "\n",
    "# #     def __len__(self):\n",
    "# #         return self.num_samples\n",
    "\n",
    "# # class RandomTupleSupervisedDataset(RandomDataset):\n",
    "    \n",
    "# #     def __init__(self, num_classes=1000, num_samples=2000, shape=(3,64,64)):\n",
    "# #         super().__init__(num_samples, shape)\n",
    "# #         self.num_classes = num_classes\n",
    "        \n",
    "# #         self.targets = torch.randperm(num_classes)[:num_samples]\n",
    "        \n",
    "# #     def __getitem__(self, index):\n",
    "# #         return self.data[index], self.targets[index]\n",
    "        \n",
    "\n",
    "# # dataset = RandomTupleSupervisedDataset(1000, 200, (3,128,128))\n",
    "# # dataset\n",
    "# # dataset.data.shape\n",
    "\n",
    "# # class TestBackboneFinetuningCallback(BackboneFinetuning):\n",
    "# #     def on_train_epoch_start(self, trainer, pl_module):\n",
    "# #         super().on_train_epoch_start(trainer, pl_module)\n",
    "# #         epoch = trainer.current_epoch\n",
    "# #         if self.unfreeze_backbone_at_epoch <= epoch:\n",
    "# #             optimizer = trainer.optimizers[0]\n",
    "# #             current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "# #             backbone_lr = self.previous_backbone_lr\n",
    "# #             if epoch < 6:\n",
    "# #                 assert backbone_lr <= current_lr\n",
    "# #             else:\n",
    "# #                 assert backbone_lr == current_lr\n",
    "\n",
    "# ###############################################################\n",
    "\n",
    "# # class BackboneFinetuningCallback(pl.callbacks.Callback):\n",
    "\n",
    "# #         def __init__(self,\n",
    "    \n",
    "\n",
    "# print(\"current dir:\", os.getcwd())\n",
    "\n",
    "# import os\n",
    "# os.path.abspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile finetuning_callback_test.py\n",
    "\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pytest\n",
    "# from typing import *\n",
    "# import pytorch_lightning as pl\n",
    "# import torch\n",
    "# from lightning_hydra_classifiers.models.transfer import *\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from torch import nn\n",
    "# # from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "# import logging\n",
    "# import json\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.Logger(__name__)\n",
    "# logger.setLevel('INFO')\n",
    "# pylog = logging.getLogger()\n",
    "\n",
    "\n",
    "# BN_TYPE = (torch.nn.modules.batchnorm._BatchNorm,)\n",
    "\n",
    "# def is_bn(layer: nn.Module) -> bool:\n",
    "#     \"\"\" Return True if layer's type is one of the batch norms.\"\"\"\n",
    "#     return isinstance(layer, BN_TYPE)\n",
    "\n",
    "# def grad_check(tensor: torch.Tensor) -> bool:\n",
    "#     \"\"\" Returns True if tensor.requires_grad==True, else False.\"\"\"\n",
    "#     return tensor.requires_grad == True\n",
    "\n",
    "\n",
    "# # os.chdir(\"/media/data/jacob/GitHub/lightning-hydra-classifiers\")#/tests\")\n",
    "\n",
    "# class RandomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, num_samples=2000, shape=(3,64,64)):\n",
    "#         self.num_samples = num_samples\n",
    "#         self.shape = shape\n",
    "#         self.data = torch.randn(num_samples, *shape)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data[index]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_samples\n",
    "\n",
    "# class RandomTupleSupervisedDataset(RandomDataset):\n",
    "    \n",
    "#     def __init__(self, num_classes=1000, num_samples=2000, shape=(3,64,64)):\n",
    "#         super().__init__(num_samples, shape)\n",
    "#         self.num_classes = num_classes\n",
    "        \n",
    "#         self.targets = torch.randperm(num_classes)[:num_samples]\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data[index], self.targets[index]\n",
    "\n",
    "##############################################\n",
    "\n",
    "    \n",
    "    \n",
    "# class FinetuningLightningCallback(pl.callbacks.Callback):\n",
    "    \n",
    "# # class FinetuningLightningPlugin:\n",
    "#     mode_dict = {\"min\": torch.lt, \"max\": torch.gt}\n",
    "#     order_dict = {\"min\": \"<\", \"max\": \">\"}\n",
    "    \n",
    "    \n",
    "#     def __init__(self,\n",
    "#                  monitor: str=\"val_loss\",\n",
    "#                  mode: str=\"min\",\n",
    "#                  patience: int=4):\n",
    "        \n",
    "# #         if pl_module.hparams.finetuning_strategy == \"finetuning_unfreeze_layers_on_plateau\":\n",
    "#         self.monitor = monitor\n",
    "#         self.mode = mode\n",
    "#         self.patience = patience\n",
    "# #         self.best_metric = 0\n",
    "#         self.milestone_index = 0\n",
    "        \n",
    "# #         self.min_delta *= 1 if self.monitor_op == torch.gt else -1\n",
    "#         torch_inf = torch.tensor(np.Inf)\n",
    "#         self.best_score = torch_inf if self.monitor_op == torch.lt else -torch_inf\n",
    "    \n",
    "#         self.milestone_logs = []\n",
    "        \n",
    "#     def on_fit_start(self,\n",
    "#                      trainer,\n",
    "#                      pl_module):\n",
    "#         self.milestones = pl_module.finetuning_milestones\n",
    "#         print(f\"Setting milestones: {pl_module.finetuning_milestones}\")\n",
    "#         self._finished = False\n",
    "    \n",
    "    \n",
    "#     def finetuning_pretrained_strategy(self,\n",
    "#                                        trainer: \"pl.Trainer\",\n",
    "#                                        pl_module):\n",
    "#         \"\"\"\n",
    "        \n",
    "        \n",
    "#         \"\"\"\n",
    "#         epoch = trainer.current_epoch\n",
    "#         logs = trainer.callback_metrics\n",
    "#         current = logs.get(self.monitor)\n",
    "        \n",
    "#         if self.mode == \"min\":\n",
    "#             new_best = current < self.best_score\n",
    "#         elif self.mode == \"max\":\n",
    "#             new_best = current > self.best_score\n",
    "        \n",
    "#         if self._finished:\n",
    "#             return\n",
    "        \n",
    "#         if new_best:\n",
    "#             self.best_score = current\n",
    "#             self.wait_epochs = 0\n",
    "#             print(f\"New best score: {self.monitor}={self.best_score}.\")\n",
    "#         elif self.wait_epochs >= self.patience:\n",
    "            \n",
    "#             next_to_unfreeze = self.milestones[self.milestone_index]\n",
    "#             print(f\"Patience of {self.patience} surpassed at epoch: {epoch} unfreezing down to: {next_to_unfreeze}\")\n",
    "            \n",
    "#             pl_module.unfreeze_backbone_top_layers(unfreeze_down_to=next_to_unfreeze)\n",
    "#             self.wait_epochs = 0\n",
    "#             self.milestone_index += 1\n",
    "#             self.milestone_logs.append({\"epoch\":epoch,\n",
    "#                                         \"unfreeze_at_layer\":next_to_unfreeze,\n",
    "#                                         \"trainable_params\":pl_module.get_trainable_parameters(count_params=True),\n",
    "#                                         \"nontrainable_params\":pl_module.get_nontrainable_parameters(count_params=True)})\n",
    "#             if self.milestone_index >= len(self.milestones):\n",
    "#                 self._finished = True\n",
    "#         else:\n",
    "#             self.wait_epochs += 1\n",
    "    \n",
    "#     @property\n",
    "#     def monitor_op(self) -> Callable:\n",
    "#         return self.mode_dict[self.mode]\n",
    "    \n",
    "#     def on_epoch_end(self, trainer, pl_module):\n",
    "#         \"\"\"Called when the epoch ends.\"\"\"\n",
    "\n",
    "#         self.finetuning_pretrained_strategy(trainer=trainer, pl_module=pl_module)\n",
    "#         try:\n",
    "#             pl_module.log(\"nontrainable_params\", pl_module.get_nontrainable_parameters(count_params=True))\n",
    "#             pl_module.log(\"trainable_params\", pl_module.get_trainable_parameters(count_params=True))\n",
    "# #             pl_module.logger.summary[\"milestones\"] = self.milestone_logs[-1]\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             print(f\"logging to wandb didnt work bro\")\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "########################\n",
    "\n",
    "\n",
    "from lightning_hydra_classifiers.callbacks.finetuning_callbacks import FinetuningLightningCallback\n",
    "\n",
    "class TestLightningClassifier(LightningClassifier):\n",
    "\n",
    "    def __init__(self,\n",
    "                 backbone_name='resnet50',\n",
    "                 pretrained: Union[bool, str]=True,\n",
    "                 num_classes: int=1000,\n",
    "                 finetuning_strategy: str=\"feature_extractor\",\n",
    "                 seed: int=None,\n",
    "                 **kwargs):\n",
    "\n",
    "        super().__init__(backbone_name=backbone_name,\n",
    "                         pretrained=pretrained,\n",
    "                         num_classes=num_classes,\n",
    "                         pool_type=\"avgdrop\",\n",
    "                         head_type=\"linear\",\n",
    "                         hidden_size=None, lr=0.01, backbone_lr_mult=0.1,\n",
    "                         weight_decay=0.01,\n",
    "                         finetuning_strategy=finetuning_strategy,\n",
    "                         seed=42,\n",
    "                        **kwargs)\n",
    "        self._verbose=True\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.log(\"train_loss\",1)\n",
    "        return {\"loss\": torch.ones(1, requires_grad=True)}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.log(\"val_loss\",1)\n",
    "        return {\"loss\": torch.ones(1, requires_grad=True)}\n",
    "    \n",
    "    \n",
    "#         output = super().training_step(batch, batch_idx)\n",
    "#         self._verbose=False\n",
    "#         return output\n",
    "\n",
    "    def training_step_end(self, outputs):\n",
    "        super().training_step_end(outputs)\n",
    "\n",
    "    def print(self, *args):\n",
    "        if self._verbose:\n",
    "            print(*args)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(RandomTupleSupervisedDataset(num_classes=1000, num_samples=50, shape=(3,64,64)), batch_size=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(RandomTupleSupervisedDataset(num_classes=1000, num_samples=50, shape=(3,64,64)), batch_size=2)\n",
    "\n",
    "\n",
    "def save_log(log, fp):\n",
    "    with open(fp, \"w\") as fp:\n",
    "        json.dump(log, fp, indent=4, sort_keys=False)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "###############################\n",
    "###############################\n",
    "\n",
    "\n",
    "# @pytest.mark.parametrize(\"finetuning_strategy\",\n",
    "#                         [(\"feature_extractor\",)\n",
    "#                          \"feature_extractor_+_bn.eval()\",\n",
    "#                          \"feature_extractor_+_except_bn\"])\n",
    "\n",
    "# @pytest.mark.parametrize(\"finetuning_strategy, expected_layer_counts\",\n",
    "#     [\n",
    "#         (\"feature_extractor\",\n",
    "#             {\"is_training\":{'True': 53, 'False': 0, 'Total': 53}, \n",
    "#              \"requires_grad\":{'True': 0, 'False': 53, 'Total': 53}}\n",
    "#         ),\n",
    "#         (\"feature_extractor_+_bn.eval()\",\n",
    "#             {\"is_training\":{'True': 0, 'False': 53, 'Total': 53}, \n",
    "#              \"requires_grad\":{'True': 0, 'False': 53, 'Total': 53}}\n",
    "#         ),\n",
    "#         (\"feature_extractor_+_except_bn\",\n",
    "#             {\"is_training\":{'True': 53, 'False': 0, 'Total': 53}, \n",
    "#              \"requires_grad\":{'True': 53, 'False': 0, 'Total': 53}}\n",
    "#         )\n",
    "#     ]\n",
    "#                         )\n",
    "# @pytest.mark.parametrize()\n",
    "def test_finetuning_callback(tmpdir):#, finetuning_strategy: str, expected_layer_counts: Dict[str,Dict[str,int]]):#, expectations: Dict[str,Any]):\n",
    "    \"\"\"Test finetuning strategy works as expected.\"\"\"\n",
    "\n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    callbacks = [FinetuningLightningCallback(monitor=\"val_loss\",\n",
    "                                             mode=\"min\",\n",
    "                                             patience=4)]\n",
    "\n",
    "    model = TestLightningClassifier(finetuning_strategy=\"finetuning_unfreeze_layers_on_plateau\")\n",
    "#     callback = TestBackboneFinetuningCallback(unfreeze_backbone_at_epoch=3, verbose=False)\n",
    "\n",
    "    trainer = pl.Trainer(limit_train_batches=2,\n",
    "                         limit_val_batches=2,\n",
    "                         default_root_dir=\"/home/jrose3\",\n",
    "                         log_every_n_steps=1,\n",
    "                         callbacks=callbacks,\n",
    "                         max_epochs=25)\n",
    "    trainer.fit(model)\n",
    "    \n",
    "    \n",
    "#     pylog.info(f\"strategy: {finetuning_strategy}\")\n",
    "    model._verbose = True\n",
    "    layer_counts = model.count_trainable_batchnorm_layers()\n",
    "\n",
    "\n",
    "\n",
    "    from rich import print as pp\n",
    "    print(\"milestone_logs:\")\n",
    "    pp(callbacks[0].milestone_logs)\n",
    "\n",
    "#     pylog.info(f\"strategy: {finetuning_strategy}\")\n",
    "#     pylog.info(f\"Expected layer counts: {expected_layer_counts}\")\n",
    "    pylog.info(f\"count trainable batchnorm layers`: {model.count_trainable_batchnorm_layers()}\")\n",
    "    pylog.info(f\"count trainable layers: {model.get_trainable_parameters(count_layers=True)}\")\n",
    "    pylog.info(f\"count nontrainable layers: {model.get_nontrainable_parameters(count_layers=True)}\")\n",
    "    pylog.info(f\"count trainable params: {model.get_trainable_parameters(count_params=True)}\")\n",
    "    pylog.info(f\"count nontrainable params: {model.get_nontrainable_parameters(count_params=True)}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     assert expected_layer_counts[\"is_training\"][\"True\"] == layer_counts[0][\"True\"]\n",
    "#     assert expected_layer_counts[\"is_training\"][\"False\"] == layer_counts[0][\"False\"]\n",
    "\n",
    "#     assert expected_layer_counts[\"requires_grad\"][\"True\"] == layer_counts[1][\"True\"]\n",
    "#     assert expected_layer_counts[\"requires_grad\"][\"False\"] == layer_counts[1][\"False\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TestLightningClassifier(finetuning_strategy=\"feature_extractor_+_except_bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestLightningClassifier(LightningClassifier):\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  backbone_name='resnet50',\n",
    "#                  pretrained: Union[bool, str]=True,\n",
    "#                  num_classes: int=1000,\n",
    "#                  finetuning_strategy: str=\"feature_extractor\",\n",
    "#                  seed: int=None,\n",
    "#                  **kwargs):\n",
    "\n",
    "#         super().__init__(backbone_name=backbone_name,\n",
    "#                          pretrained=pretrained,\n",
    "#                          num_classes=num_classes,\n",
    "#                          pool_type=\"avgdrop\",\n",
    "#                          head_type=\"linear\",\n",
    "#                          hidden_size=None, lr=0.01, backbone_lr_mult=0.1,\n",
    "#                          weight_decay=0.01,\n",
    "#                          finetuning_strategy=finetuning_strategy,\n",
    "#                          seed=42,\n",
    "#                         **kwargs)\n",
    "#         self._verbose=True\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         output = super().training_step(batch, batch_idx)\n",
    "# #             self.print(f\"During: self.training_step\")\n",
    "# #             self.count_trainable_batchnorm_layers()\n",
    "#         self._verbose=False\n",
    "#         return output\n",
    "\n",
    "#     def training_step_end(self, outputs):\n",
    "#         output = super().training_step_end(outputs)\n",
    "\n",
    "#     def print(self, *args):\n",
    "#         if self._verbose:\n",
    "#             print(*args)\n",
    "\n",
    "# #         def configure_optimizers(self):\n",
    "# #             optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\n",
    "# #             lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "# #             return [optimizer], [lr_scheduler]\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(RandomTupleSupervisedDataset(num_classes=1000, num_samples=50, shape=(3,64,64)), batch_size=2)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(RandomTupleSupervisedDataset(num_classes=1000, num_samples=50, shape=(3,64,64)), batch_size=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # finetuning_strategy = \"feature_extractor\"\n",
    "# fixtures = [\"feature_extractor\",\n",
    "#             \"feature_extractor_+_bn.eval()\",\n",
    "#             \"feature_extractor_+_except_bn\"]\n",
    "\n",
    "# for finetuning_strategy in fixtures:\n",
    "#     print(f\"strategy: {finetuning_strategy}\")\n",
    "\n",
    "#     model = TestLightningClassifier(finetuning_strategy=finetuning_strategy)\n",
    "#     training_batch_stats, params_require_grads = model.count_trainable_batchnorm_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich.console import Console\n",
    "# from rich.markdown import Markdown\n",
    "# console = Console()\n",
    "# markdown = Markdown(\"$$\\delta \\pi = 3.14159265358979323$$\")\n",
    "# console.print(markdown)\n",
    "# from IPython.display import display, Math, Latex\n",
    "# display(Math(r\"$$mean=E[x^k],\" + \"\\n\" +\"Var = Var[x^k]\"))\n",
    "# from IPython.display import display, Math, Latex\n",
    "# display(Math(r\"$$\\delta \\pi = 3.14159265358979323$$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"count trainable layers: \", model.count_trainable_layers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# class ThresholdBasedFinetuning(pl.callbacks.EarlyStopping):\n",
    "    \n",
    "#     mode_dict = {\"min\": torch.lt, \"max\": torch.gt}\n",
    "\n",
    "#     order_dict = {\"min\": \"<\", \"max\": \">\"}\n",
    "    \n",
    "#     def __init__(self,\n",
    "#                  monitor: Optional[str] = None,\n",
    "#                  min_delta: float = 0.0,\n",
    "#                  patience: int = 3,\n",
    "#                  verbose: bool = False,\n",
    "#                  mode: str = \"min\",\n",
    "#                  strict: bool = True,\n",
    "#                  check_finite: bool = True,\n",
    "#                  stopping_threshold: Optional[float] = None,\n",
    "#                  divergence_threshold: Optional[float] = None,\n",
    "#                  check_on_train_epoch_end: Optional[bool] = None):\n",
    "#         super().__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "#     def on_save_checkpoint(\n",
    "#         self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: Dict[str, Any]\n",
    "#     ) -> Dict[str, Any]:\n",
    "#         return {\n",
    "#             \"wait_count\": self.wait_count,\n",
    "#             \"stopped_epoch\": self.stopped_epoch,\n",
    "#             \"best_score\": self.best_score,\n",
    "#             \"patience\": self.patience,\n",
    "#         }\n",
    "\n",
    "#     def on_load_checkpoint(\n",
    "#         self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", callback_state: Dict[str, Any]\n",
    "#     ) -> None:\n",
    "#         self.wait_count = callback_state[\"wait_count\"]\n",
    "#         self.stopped_epoch = callback_state[\"stopped_epoch\"]\n",
    "#         self.best_score = callback_state[\"best_score\"]\n",
    "#         self.patience = callback_state[\"patience\"]\n",
    "\n",
    "        \n",
    "#     def _should_skip_check(self, trainer: \"pl.Trainer\") -> bool:\n",
    "#         from pytorch_lightning.trainer.states import TrainerFn\n",
    "\n",
    "#         return trainer.state.fn != TrainerFn.FITTING or trainer.sanity_checking\n",
    "\n",
    "#     def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "#         if self._should_skip_check(trainer):\n",
    "#             return\n",
    "#         self._run_early_stopping_check(trainer)\n",
    "\n",
    "        \n",
    "#     def _run_early_stopping_check(self, trainer: \"pl.Trainer\") -> None:\n",
    "#         \"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\n",
    "#         logs = trainer.callback_metrics\n",
    "\n",
    "#         if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\n",
    "#             logs\n",
    "#         ):  # short circuit if metric not present\n",
    "#             return\n",
    "\n",
    "#         current = logs.get(self.monitor)\n",
    "#         should_stop, reason = self._evaluate_stopping_criteria(current)\n",
    "\n",
    "#         # stop every ddp process if any world process decides to stop\n",
    "#         should_stop = trainer.training_type_plugin.reduce_boolean_decision(should_stop)\n",
    "#         trainer.should_stop = trainer.should_stop or should_stop\n",
    "#         if should_stop:\n",
    "#             self.stopped_epoch = trainer.current_epoch\n",
    "#         if reason and self.verbose:\n",
    "#             self._log_info(trainer, reason)\n",
    "\n",
    "#     def _evaluate_stopping_criteria(self, current: torch.Tensor) -> Tuple[bool, Optional[str]]:\n",
    "#         should_stop = False\n",
    "#         reason = None\n",
    "#         if self.check_finite and not torch.isfinite(current):\n",
    "#             should_stop = True\n",
    "#             reason = (\n",
    "#                 f\"Monitored metric {self.monitor} = {current} is not finite.\"\n",
    "#                 f\" Previous best value was {self.best_score:.3f}. Signaling Trainer to stop.\"\n",
    "#             )\n",
    "#         elif self.stopping_threshold is not None and self.monitor_op(current, self.stopping_threshold):\n",
    "#             should_stop = True\n",
    "#             reason = (\n",
    "#                 \"Stopping threshold reached:\"\n",
    "#                 f\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.stopping_threshold}.\"\n",
    "#                 \" Signaling Trainer to stop.\"\n",
    "#             )\n",
    "#         elif self.divergence_threshold is not None and self.monitor_op(-current, -self.divergence_threshold):\n",
    "#             should_stop = True\n",
    "#             reason = (\n",
    "#                 \"Divergence threshold reached:\"\n",
    "#                 f\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"\n",
    "#                 \" Signaling Trainer to stop.\"\n",
    "#             )\n",
    "#         elif self.monitor_op(current - self.min_delta, self.best_score.to(current.device)):\n",
    "#             should_stop = False\n",
    "#             reason = self._improvement_message(current)\n",
    "#             self.best_score = current\n",
    "#             self.wait_count = 0\n",
    "#         else:\n",
    "#             self.wait_count += 1\n",
    "#             if self.wait_count >= self.patience:\n",
    "#                 should_stop = True\n",
    "#                 reason = (\n",
    "#                     f\"Monitored metric {self.monitor} did not improve in the last {self.wait_count} records.\"\n",
    "#                     f\" Best score: {self.best_score:.3f}. Signaling Trainer to stop.\"\n",
    "#                 )\n",
    "\n",
    "#         return should_stop, reason"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "class TestLightningClassifier(LightningClassifier):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 backbone_name='resnet50',\n",
    "                 pretrained: Union[bool, str]=True,\n",
    "                 num_classes: int=1000,\n",
    "                 pool_size: int=1,\n",
    "                 pool_type: str='avg',\n",
    "                 head_type: str='linear',\n",
    "                 hidden_size: Optional[int]=512,\n",
    "                 lr: float=2e-03,\n",
    "                 backbone_lr_mult: bool=0.1,\n",
    "                 weight_decay: float=0.01,\n",
    "                 finetuning_strategy: str=\"finetuning_unfreeze_layers_on_plateau\", #\"feature_extractor\",\n",
    "                 seed: int=None,\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(backbone_name=backbone_name,\n",
    "                         pretrained=pretrained,\n",
    "                         num_classes=num_classes,\n",
    "                         pool_size=pool_size,\n",
    "                         pool_type=pool_type,\n",
    "                         head_type=head_type,\n",
    "                         hidden_size=hidden_size,\n",
    "                         lr=lr,\n",
    "                         backbone_lr_mult=backbone_lr_mult,\n",
    "                         weight_decay=weight_decay,\n",
    "                         finetuning_strategy=finetuning_strategy,\n",
    "                         seed=seed,\n",
    "                        **kwargs)\n",
    "        self._verbose=True\n",
    "    \n",
    "    def print(self, *args):\n",
    "        if self._verbose:\n",
    "            print(*args)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "#         self.print(f\"Before: self.training_step\")\n",
    "#         self.count_trainable_batchnorm_layers()\n",
    "        output = super().training_step(batch, batch_idx)\n",
    "        self.print(f\"During: self.training_step\")\n",
    "        self.count_trainable_batchnorm_layers()\n",
    "#         if self.eval_bn:\n",
    "#             if not self.freeze_bn:\n",
    "#                 self.unfreeze(self.model,\n",
    "#                               filter_pattern=\"bn\")\n",
    "\n",
    "#         self.print(f\"After: self.training_step\")\n",
    "#         self.count_trainable_batchnorm_layers()\n",
    "        self._verbose=False\n",
    "        return output\n",
    "    \n",
    "    def training_step_end(self, outputs): #batch, batch_idx):\n",
    "#         self.print(f\"Before: self.training_step_end\")\n",
    "#         self.count_trainable_batchnorm_layers()\n",
    "        output = super().training_step_end(outputs) #batch, batch_idx)\n",
    "#         self.print(f\"After: self.training_step_end\")\n",
    "#         self.count_trainable_batchnorm_layers()\n",
    "        \n",
    "#         return output\n",
    "    \n",
    "#     def training_epoch_end(self, outputs):\n",
    "#         self.print(f\"Before: self.training_epoch_end\")\n",
    "        self.count_trainable_batchnorm_layers()\n",
    "#         super().training_epoch_end(outputs)\n",
    "#         self.print(f\"After: self.training_epoch_end\")\n",
    "#         self.count_trainable_batchnorm_layers()\n",
    "#         self._verbose=False\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html\n",
    "from lightning_hydra_classifiers.callbacks.finetuning_callbacks import FinetuningLightningCallback\n",
    "\n",
    "\n",
    "def test_model_freeze_strategy(config, datamodule, **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    config.model.finetuning_strategy = \"finetuning_unfreeze_layers_on_plateau\"\n",
    "    \n",
    "    group=f'{config.model.backbone.backbone_name}_{config.data.experiment.experiment_name}'#_task_{task_id}'\n",
    "    config.logger.wandb.group = group\n",
    "    config.callbacks.log_per_class_metrics_to_wandb.class_names = datamodule.classes\n",
    "\n",
    "    callbacks = configure_callbacks(config)\n",
    "    \n",
    "    callbacks.append(FinetuningLightningCallback(monitor=\"val_loss\",\n",
    "                                                 mode=\"min\",\n",
    "                                                 patience=5))\n",
    "    logger = configure_loggers(config)\n",
    "    \n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=config.checkpoint_dir, #config.experiment_dir,\n",
    "                         gpus=1,\n",
    "                         max_epochs=config.trainer.max_epochs,\n",
    "                         callbacks=callbacks,\n",
    "#                                     ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "#                                     LearningRateMonitor(\"epoch\")],\n",
    "                         logger=logger,\n",
    "                         resume_from_checkpoint=config.trainer.resume_from_checkpoint,\n",
    "                         progress_bar_refresh_rate=1)\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = config.trainer.resume_from_checkpoint #config.checkpoint_dir\n",
    "    if os.path.isfile(str(pretrained_filename)):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = TestLightningClassifier.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(config.model.seed)\n",
    "        model = TestLightningClassifier(**config.model, **kwargs)\n",
    "        model.label_encoder = datamodule.label_encoder\n",
    "        \n",
    "\n",
    "\n",
    "        if config.trainer.auto_lr_find:\n",
    "\n",
    "            lr_tune_output = lr_tuner.run_lr_tuner(trainer=trainer,\n",
    "                                                   model=model,\n",
    "                                                   datamodule=datamodule,\n",
    "                                                   config=config,\n",
    "                                                   results_dir=config.lr_tuner_dir,\n",
    "                                                   group=\"bn_eval_trials\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "        model = TestLightningClassifier.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "        print(f\"Best checkpoint saved to: {trainer.checkpoint_callback.best_model_path}\")\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, test_dataloaders=datamodule.val_dataloader(), verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=datamodule.test_dataloader(), verbose=False)\n",
    "    \n",
    "    try:\n",
    "        result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = {\"test_acc\": test_result, \"val_acc\": val_result}\n",
    "        \n",
    "    result[\"ckpt_path\"] = trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.scripts.multitask.train import MultiTaskDataModule, LitMultiTaskModule, ImagePredictionLogger, train_task,  CIFAR10DataModule, run_multitask_test, load_data_and_model, load_data, resolve_config, configure_callbacks, configure_loggers, configure_trainer\n",
    "\n",
    "\n",
    "\n",
    "# from lightning_hydra_classifiers.scripts.finetune_demo import *\n",
    "# # from lightning_hydra_classifiers.data.datasets.common import toPIL\n",
    "# from lightning_hydra_classifiers.utils.etl_utils import ETL\n",
    "# from omegaconf import OmegaConf\n",
    "# import os\n",
    "\n",
    "def get_config_and_load_data(overrides = None,\n",
    "                             task_id: int = 1,\n",
    "                             pool_type='avgdrop',\n",
    "                             finetuning_strategy=\"feature_extractor_+_bn.eval()\",\n",
    "                             lr=2e-03,\n",
    "                             dropout_p: float=0.3,\n",
    "                             max_epochs: int=5):\n",
    "    overrides = overrides or []    \n",
    "    config = ETL.load_hydra_config(config_name = \"finetune_config\",\n",
    "                                   config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\",\n",
    "                                   overrides=overrides)\n",
    "    OmegaConf.set_struct(config, False)\n",
    "    \n",
    "\n",
    "    datamodule = load_data(config,\n",
    "                           task_id=task_id)\n",
    "\n",
    "    model_config = OmegaConf.create(dict(\n",
    "                                    backbone={\"backbone_name\":config.model.backbone.backbone_name},\n",
    "                                    backbone_name=config.model.backbone.backbone_name,\n",
    "                                    pretrained=True,\n",
    "                                    num_classes=datamodule.num_classes,\n",
    "                                    pool_type=pool_type,\n",
    "                                    head_type='linear',\n",
    "                                    hidden_size=None,\n",
    "                                    dropout_p=dropout_p,\n",
    "                                    lr=2e-03,\n",
    "                                    backbone_lr_mult=0.1,\n",
    "                                    finetuning_strategy=finetuning_strategy,\n",
    "                                    weight_decay=0.01,\n",
    "                                    seed=98))\n",
    "    config.model = model_config\n",
    "#     config.trainer.max_epochs = max_epochs\n",
    "#     config.trainer.auto_lr_find = False\n",
    "#     config.experiment_name = f\"{config.model.finetuning_strategy}-PNAS-{datamodule.num_classes}_classes-res_{config.data.image_size}-bsz_{config.data.batch_size}-{config.model.backbone_name}-pretrained_{config.model.pretrained}-pool_{config.model.pool_type}\"\n",
    "    \n",
    "#     config.root_dir = os.path.join(os.getcwd(), \"bn_unit_test_logs\", config.model.pool_type)\n",
    "#     config.lr_tuner_dir = os.path.join(config.results_dir, f\"task_{task_id}\", \"lr_tuner\")\n",
    "    \n",
    "    config = OmegaConf.create(OmegaConf.to_container(config, resolve=True))\n",
    "    \n",
    "#     os.makedirs(config.results_dir, exist_ok=True)\n",
    "#     os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "#     os.makedirs(config.lr_tuner_dir, exist_ok=True)\n",
    "    return config, datamodule\n",
    "\n",
    "\n",
    "# model = LightningClassifier(**config.model)\n",
    "# model = TestLightningClassifier(**config.model)\n",
    "# model.label_encoder = datamodule.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# config = OmegaConf.load(\"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/finetune_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OmegaConf.to_container(config, resolve=True)\n",
    "\n",
    "# from lightning_hydra_classifiers.scripts.finetune_demo import *\n",
    "# # from lightning_hydra_classifiers.data.datasets.common import toPIL\n",
    "# from lightning_hydra_classifiers.utils.etl_utils import ETL\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "from lightning_hydra_classifiers.scripts.finetune_demo import *\n",
    "\n",
    "config = get_config(overrides=[\"task_id=1\"])\n",
    "\n",
    "config, datamodule = get_config_and_load_data(config)#overrides = None, task_id = 1, finetuning_strategy=\"finetuning_unfreeze_layers_on_plateau\")\n",
    "\n",
    "datamodule.train_dataset[3].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/finetune_trials/Extant-to-PNAS-512-transfer_benchmark-resnet50-Extant-PNAS_to_PNAS-92_classes-res_512-bsz_32-pretrained_imagenet-pool_avgdrop/replicate_1/checkpoints/epoch=09-val_loss=1.097-val_acc=0.621.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning_hydra_classifiers.scripts.finetune_demo import *\n",
    "\n",
    "# kwargs = {\"backbone_name\":\"resnet50\",\n",
    "#           \"num_classes\":91}\n",
    "\n",
    "model = LightningClassifier.init_pretrained_backbone_w_new_classifier(ckpt_path=ckpt_path,\n",
    "                                                                      new_num_classes=19,\n",
    "                                                                      **config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model.model.head.classifier.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.load_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "def plot_class_counts(df,\n",
    "                      ax=None,\n",
    "                      figsize=(25,10),\n",
    "                      alpha=0.8,\n",
    "                      ticklabel_rotation=40,\n",
    "                      title: str=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1, figsize=figsize)\n",
    "    else:\n",
    "        fig = plt.gcf()\n",
    "    sns.barplot(df.index, df.values, alpha=alpha, ax=ax)\n",
    "\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), \n",
    "                       rotation = ticklabel_rotation,\n",
    "                       ha=\"right\", fontsize=\"xx-small\")\n",
    "    if isinstance(title, str):\n",
    "        ax.set_title(title)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"family\"\n",
    "\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "\n",
    "    df = getattr(datamodule, f\"{subset}_dataset\").samples_df.value_counts(y_col)\n",
    "\n",
    "    fig, ax = plot_class_counts(df,\n",
    "                                ax=None,\n",
    "                                figsize=(25,10),\n",
    "                                alpha=0.8,\n",
    "                                ticklabel_rotation=40,\n",
    "                                title=subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "valid_strategies = (\"finetuning_unfreeze_layers_on_plateau\",)\n",
    "# pool_types = (\"avg\", \"avgdrop\")#, \"avgmax\", \"max\", \"avgmaxdrop\")\n",
    "\n",
    "# finetuning_strategy=\"feature_extractor\"\n",
    "# finetuning_strategy=\"feature_extractor_+_bn.eval()\"\n",
    "\n",
    "# pool_type='avgdrop'\n",
    "# pool_type='avgmaxdrop'\n",
    "pool_type=\"avg\"\n",
    "dropout_p = 0.3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for strategy in valid_strategies:\n",
    "\n",
    "    print(f\"BEGINNING STRATEGY: {strategy}\")\n",
    "    overrides = ['model/backbone=resnet50',\n",
    "                 \"data=extant_to_pnas\",\n",
    "                 \"trainer.max_epochs=1\",\n",
    "                 \"trainer.auto_lr_find=true\",\n",
    "                 \"trainer.precision=16\",\n",
    "                 \"trainer.gpus=[0]\",\n",
    "                 \"trainer.resume_from_checkpoint=null\",\n",
    "                 \"data.batch_size=16\",\n",
    "                 \"logger.wandb.project=finetuning_on_plateau\"]\n",
    "\n",
    "    config, datamodule = get_config_and_load_data(overrides = overrides,\n",
    "                                                  task_id=1,\n",
    "                                                  pool_type=pool_type,\n",
    "                                                  finetuning_strategy=strategy, #\"feature_extractor_+_bn.eval()\",\n",
    "                                                  lr=2e-03,\n",
    "                                                  dropout_p=dropout_p)#,\n",
    "#                                                   max_epochs=config.trainer.max_epochs)\n",
    "    ckpt_paths = os.listdir(os.path.join(config.checkpoint_dir))\n",
    "    if len(ckpt_paths) and os.path.exists(ckpt_paths[-1]):\n",
    "        print(f\"Found {ckpt_paths[-1]}\")\n",
    "        config.resume_from_checkpoint = ckpt_paths[-1]\n",
    "\n",
    "\n",
    "    model, results = test_model_freeze_strategy(config, datamodule)\n",
    "    model.cpu()\n",
    "    del model\n",
    "\n",
    "    results['model_config'] = OmegaConf.to_container(config.model, resolve=True)\n",
    "    results['data_config'] = OmegaConf.to_container(config.data, resolve=True)\n",
    "    \n",
    "    ETL.config2yaml(results, os.path.join(config.results_dir, \"results.yaml\"))\n",
    "    print(f\"[SAVED TRIAL RESULTS] Location: {os.path.join(config.results_dir, 'results.yaml')}\")\n",
    "    pp(results)\n",
    "    \n",
    "    all_results[strategy] = results\n",
    "\n",
    "print(f\"ALL FINISHED!!! RESULTS:\")\n",
    "pp(all_results)\n",
    "\n",
    "\n",
    "ETL.config2yaml(all_results, os.path.join(config.root_dir, \"results.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logs/avg/finetuning_unfreeze_layers_on_plateau-PNAS-19_classes-res_512-bsz_16-resnet50-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ckpt_path = '/media/data/jacob/GitHub/lightning-hydra-classifiers/notebooks/bn_unit_test_logs/avg/finetuning_unfreeze_layers_on_plateau-PNAS-19_classes-res_512-bsz_16-resnet50-pretrained_True-pool_avg/replicate_1/results/checkpoints/epoch=00-val_loss=1.102-val_acc=0.475.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.utils.common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_counts_df.T.plot(kind='bar', figsize=(16,9), multiple='stack')\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "# sns.displot(data=subset_counts_df.T, kind='bar', figsize=(16,9), multiple='stack')\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "available_palettes = ['Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'crest', 'crest_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'flare', 'flare_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', 'inferno_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r']\n",
    "\n",
    "for p in available_palettes:\n",
    "    print(p)\n",
    "    display(sns.color_palette(p))\n",
    "\n",
    "datamodule.current_task\n",
    "\n",
    "data_splits=datamodule.current_task\n",
    "\n",
    "train_key = [\"train\"]\n",
    "sort_by = compute_class_counts(data_splits[train_key[0]].targets,\n",
    "                               sort_by=\"count\")\n",
    "\n",
    "subset_counts_df = {}\n",
    "for subset, values in data_splits.items():\n",
    "    print(subset)\n",
    "    values = compute_class_counts(data_splits[subset].targets,\n",
    "                                  sort_by=sort_by)\n",
    "    subset_counts_df[subset] = values\n",
    "\n",
    "subset_counts_df = pd.DataFrame.from_dict(subset_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subset_counts_df.T.stack().reset_index().rename(columns={0:\"counts\"})#.set_index(\"subset\")\n",
    "\n",
    "# df.index.name = \"subset\"\n",
    "# df.columns.name = \"target\"\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "subset_counts_df.T#.unstack(\"target\")\n",
    "\n",
    "df_like = next(iter(df.groupby(\"subset\")))[1]\n",
    "\n",
    "totals = df_like[[\"target\", \"counts\"]].assign(counts=df_like[\"counts\"]*0).set_index(\"target\")\n",
    "\n",
    "# totals = df_like.set_index(\"subset\").assign(count=df[\"count\"]*0)\n",
    "previous_totals = totals.copy()\n",
    "\n",
    "previous_totals.counts + data_subset.set_index(\"target\").counts\n",
    "\n",
    "totals#.set_index(\"target\")\n",
    "previous_totals\n",
    "# data_subset.set_index(\"target\")\n",
    "\n",
    "#     totals[\"count\"] = previous_totals[\"count\"] + data_subset[\"count\"]\n",
    "\n",
    "# totals[\"count\"]\n",
    "# previous_totals[\"count\"] + \n",
    "data_subset[\"count\"]\n",
    "\n",
    "colors\n",
    "\n",
    "df\n",
    "\n",
    "df.stack(\"subset\")\n",
    "\n",
    "# gb = df.reset_index().groupby(\"subset\").unstack()\n",
    "\n",
    "df = df.reset_index().set_index(\"target\")#, \"target\"))\n",
    "# gb = df.unstack((\"subset\",\"target\"))\n",
    "df.plot(y=\"counts\", hue=\"subset\", kind=\"bar\", stacked=True)\n",
    "\n",
    "# gb.columns\n",
    "\n",
    "gb#.set_index(keys=(\"subset\", \"target\"))\n",
    "\n",
    "help(gb.plot)\n",
    "\n",
    "final_sum = df.groupby(\"target\").agg(sum).sort_values(\"counts\")\n",
    "\n",
    "final_sum\n",
    "\n",
    "data = totals.counts / final_sum\n",
    "\n",
    "totals\n",
    "\n",
    "# data\n",
    "final_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"Set2\")\n",
    "i=0\n",
    "\n",
    "for subset_name, data_subset in df.groupby(\"subset\"):\n",
    "    print(subset_name)\n",
    "    \n",
    "    totals.counts = previous_totals.counts + data_subset.set_index(\"target\").counts\n",
    "    previous_totals.counts = totals.counts\n",
    "    \n",
    "    data = totals.counts / final_sum\n",
    "#     display(totals)\n",
    "    bar = sns.barplot(data=totals.reset_index(), y=\"counts\",x=\"target\", label=subset_name, color=colors[i], alpha=0.3)#, kind='bar', palette=\"tab10_r\")\n",
    "#     bar = sns.barplot(data=totals, y=\"count\",x=\"target\", hue=\"subset\", kind='bar', palette=\"tab10_r\")\n",
    "    i+=1\n",
    "plt.legend()\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(16,9))\n",
    "# bar = sns.catplot(data=df, x=\"target\", y=\"count\", hue=\"subset\", kind='bar', ax=ax, palette=\"tab10_r\")#, multiple='stack')\n",
    "# bar = sns.catplot(data=df, x=\"target\", y=\"count\", hue=\"subset\", kind='bar', figsize=(16,9), palette=\"tab10_r\")\n",
    "\n",
    "# for c in colors:\n",
    "# data_bar_totals = pd.DataFrame.\n",
    "# dir(pd.DataFrame)\n",
    "\n",
    "\n",
    "for subset_name, data_subset in df.groupby(\"subset\"):\n",
    "    print(subset_name, data_subset)\n",
    "    bar = sns.barplot(data=data_subset, y=\"count\",x=\"target\", hue=\"subset\", kind='bar', palette=\"tab10_r\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cat = sns.catplot(data=df, y=\"count\",x=\"target\", hue=\"subset\", kind='bar', palette=\"tab10_r\", height=5, aspect=3, multiple='stack')\n",
    "# ax = plt.gca()\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), fontsize=14, rotation=30, ha=\"right\");\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.catplot(data=df, multiple='stack',  kind='bar')#, figsize=(16,9), multiple='stack')\n",
    "\n",
    "# a = df.set_index(df.columns.tolist())\n",
    "a = df.set_index([\"subset\", \"target\"])\n",
    "\n",
    "a.index\n",
    "\n",
    "# df.T.index.name\n",
    "\n",
    "\n",
    "df.index#columns\n",
    "\n",
    "df.T.reset_index()\n",
    "\n",
    "df.melt(id_vars=[\"subset\", \"target\"])\n",
    "\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "sns.displot(penguins, x=\"flipper_length_mm\")\n",
    "\n",
    "penguins\n",
    "\n",
    "# # pd.DataFrame({\"labels\": [() for subset, values in data_splits.items()})\n",
    "\n",
    "# data_splits_cat = []\n",
    "\n",
    "# for subset, values in data_splits.items():\n",
    "#     print(subset)\n",
    "#     data_splits_cat.extend([(subset, v) for v in values])\n",
    "    \n",
    "    \n",
    "# data_splits_cat = pd.DataFrame.from_records(data_splits_cat, columns=[\"subset\",\"target\"])\n",
    "\n",
    "# data_splits_cat\n",
    "\n",
    "\n",
    "\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "fig, ax = plot_split_distributions(data_splits=datamodule.current_task,\n",
    "                                   use_one_axis=True,\n",
    "                                   hist_kwargs={\"alpha\":0.4,\n",
    "                                                \"multiple\":\"fill\"})\n",
    "plt.legend()\n",
    "\n",
    "display(ax[0])\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "dir(datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overrides = ['model/backbone=efficientnet_b3',\"data=extant_to_fossil\", \"trainer.max_epochs=2\", \"data.batch_size=16\", \"trainer.precision=16\"]\n",
    "# overrides = ['model/backbone=resnet50',\"data=extant_to_pnas\", \"trainer.max_epochs=20\", \"data.batch_size=32\", \"trainer.precision=16\", \"trainer.gpus=[7]\"]\n",
    "# config = ETL.load_hydra_config(config_name = \"config\",\n",
    "#                               config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\",\n",
    "#                               overrides=overrides)\n",
    "\n",
    "# valid_strategies : Tuple[str] = (\"feature_extractor\",\n",
    "#                              \"feature_extractor_+_bn.eval()\",\n",
    "#                              \"feature_extractor_+_except_bn\")\n",
    "            \n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Evaluate multiple strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "valid_strategies = (\"feature_extractor\",\n",
    "                    \"feature_extractor_+_bn.eval()\",\n",
    "                    \"feature_extractor_+_except_bn\")\n",
    "pool_types = (\"avg\", \"avgdrop\", \"avgmax\", \"max\", \"avgmaxdrop\")\n",
    "\n",
    "# finetuning_strategy=\"feature_extractor\"\n",
    "# finetuning_strategy=\"feature_extractor_+_bn.eval()\"\n",
    "\n",
    "# pool_type='avgdrop'\n",
    "# pool_type='avgmaxdrop'\n",
    "pool_type=\"avg\"\n",
    "dropout_p = 0.3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for strategy in valid_strategies:\n",
    "\n",
    "    print(f\"BEGINNING STRATEGY: {strategy}\")\n",
    "    overrides = ['model/backbone=resnet50',\n",
    "                 \"data=extant_to_pnas\",\n",
    "                 \"trainer.max_epochs=10\",\n",
    "                 \"trainer.auto_lr_find=true\",\n",
    "                 \"trainer.precision=16\",\n",
    "                 \"trainer.gpus=[7]\",\n",
    "                 \"trainer.resume_from_checkpoint=null\",\n",
    "                 \"logger.wandb.project=bn_global_pool_trials\"]\n",
    "    if strategy in [\"feature_extractor_+_except_bn\"]:\n",
    "        overrides.append(\"data.batch_size=16\")\n",
    "    else:\n",
    "        overrides.append(\"data.batch_size=32\")\n",
    "\n",
    "\n",
    "    config, datamodule = get_config_and_load_data(overrides = overrides,\n",
    "                                                  task_id=1,\n",
    "                                                  pool_type=pool_type,\n",
    "                                                  finetuning_strategy=strategy, #\"feature_extractor_+_bn.eval()\",\n",
    "                                                  lr=2e-03,\n",
    "                                                  dropout_p=dropout_p)#,\n",
    "#                                                   max_epochs=config.trainer.max_epochs)\n",
    "    ckpt_paths = os.listdir(os.path.join(config.checkpoint_dir))\n",
    "    if len(ckpt_paths) and os.path.exists(ckpt_paths[-1]):\n",
    "        print(f\"Found {ckpt_paths[-1]}\")\n",
    "        config.resume_from_checkpoint = ckpt_paths[-1]\n",
    "\n",
    "\n",
    "    model, results = test_model_freeze_strategy(config, datamodule)\n",
    "    model.cpu()\n",
    "    del model\n",
    "\n",
    "    results['model_config'] = OmegaConf.to_container(config.model, resolve=True)\n",
    "    results['data_config'] = OmegaConf.to_container(config.data, resolve=True)\n",
    "    \n",
    "    ETL.config2yaml(results, os.path.join(config.results_dir, \"results.yaml\"))\n",
    "    print(f\"[SAVED TRIAL RESULTS] Location: {os.path.join(config.results_dir, 'results.yaml')}\")\n",
    "    pp(results)\n",
    "    \n",
    "    all_results[strategy] = results\n",
    "\n",
    "print(f\"ALL FINISHED!!! RESULTS:\")\n",
    "pp(all_results)\n",
    "\n",
    "\n",
    "ETL.config2yaml(all_results, os.path.join(config.root_dir, \"results.yaml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### plot strategy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_x = []\n",
    "val_accs = []\n",
    "test_accs = []\n",
    "for k,v in all_results.items():\n",
    "    strategies_x.append(k)\n",
    "    try:\n",
    "        val_accs.append(v['val_acc'])\n",
    "        test_accs.append(v['test_acc'])\n",
    "    except:\n",
    "        val_accs.append(v['val'])\n",
    "        test_accs.append(v['test'])\n",
    "        \n",
    "        \n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame({\"strategy\":strategies_x,\n",
    "                           \"val_acc\":val_accs,\n",
    "                           \"test_acc\":test_accs})\n",
    "results_df\n",
    "\n",
    "barWidth = 0.25\n",
    "\n",
    "r1 = np.arange(len(strategies_x))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.bar(r1, val_accs, color='#7f6d5f', width=barWidth, edgecolor='white', label='val')\n",
    "plt.bar(r2, test_accs, color='#557f2d', width=barWidth, edgecolor='white', label='test')\n",
    "\n",
    " \n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('finetuning strategy', fontweight='bold')\n",
    "plt.ylabel(\"Macro avg acc\")\n",
    "plt.xticks([r + barWidth for r in range(len(strategies_x))], strategies_x, rotation=10)\n",
    "plt.suptitle(f\"{config.model.backbone_name} with global_pool={config.model.pool_type}.\" + \"\\n Classifier head trained on PNAS for <=10 epochs\")\n",
    "plt.tight_layout(rect=[0.0, 0.05, 1.0, 0.95])\n",
    "plt.savefig(os.path.join(config.root_dir, \"final_results_plot.png\"))\n",
    "print(f\"Final results saved to\", os.path.join(config.root_dir, \"final_results_plot.png\"))\n",
    "# sns.barplot(data=results_df, x='strategy', y='val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next: Add specific tests for different freeze strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Training script development\n",
    "(10-17-21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html\n",
    "\n",
    "def train_model(model_name, save_name=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    if save_name is None:\n",
    "        save_name = model_name\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),                          # Where to save models\n",
    "                         gpus=1 if str(device)==\"cuda:0\" else 0,                                             # We run on a single GPU (if possible)\n",
    "                         max_epochs=180,                                                                     # How many epochs to train for if no patience is set\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "                                    LearningRateMonitor(\"epoch\")],                                           # Log learning rate every epoch\n",
    "                         progress_bar_refresh_rate=1)                                                        # In case your notebook crashes due to the progress bar, consider increasing the refresh rate\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = CIFARModule.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = CIFARModule(model_name=model_name, **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = CIFARModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class LightningClassifier(BaseLightningModule):\n",
    "#     def __init__(self,\n",
    "#                  backbone_name='gluon_seresnext50_32x4d',\n",
    "#                  pretrained: Union[bool, str]=True,\n",
    "#                  num_classes: int=1000,\n",
    "#                  pool_size: int=1,\n",
    "#                  pool_type: str='avg',\n",
    "#                  head_type: str='linear',\n",
    "#                  hidden_size: Optional[int]=512,\n",
    "#                  lr: float=2e-03,\n",
    "#                  weight_decay: float=0.01,\n",
    "#                  seed: int=None):\n",
    "#         super().__init__(seed=seed)\n",
    "#         self.save_hyperparameters()\n",
    "        \n",
    "#         self.model = build_model(backbone_name=backbone_name,\n",
    "#                                       pretrained=pretrained,\n",
    "#                                       num_classes=num_classes,\n",
    "#                                       pool_size=pool_size,\n",
    "#                                       pool_type=pool_type,\n",
    "#                                       head_type=head_type,\n",
    "#                                       hidden_size=hidden_size)\n",
    "    \n",
    "#         self.criterion = nn.CrossEntropyLoss()\n",
    "#         self.metrics = self.init_metrics(stage='all')\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         return self.model(x)\n",
    "    \n",
    "    \n",
    "#     def get_lr(self, group: str=None):\n",
    "#         if group is None:\n",
    "#             return self.hparams.lr\n",
    "#         if group == \"backbone\":\n",
    "#             return self.hparams.lr * 0.1\n",
    "#         if group == \"head\":\n",
    "#             return self.hparams.lr\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         print(f\"self.hparams={self.hparams}\")\n",
    "#         self.optimizer = torch.optim.AdamW([{\"params\":self.model.backbone.parameters(), \"lr\":self.get_lr(\"backbone\"), \"weight_decay\": self.hparams.weight_decay},\n",
    "#                                             {\"params\":self.model.head.parameters(), \"lr\":self.get_lr(\"head\"), \"weight_decay\": self.hparams.weight_decay}])\n",
    "# #         self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.config.t_max, eta_min=self.config.min_lr)\n",
    "\n",
    "#         return {'optimizer': self.optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BnFreeze(Callback):\n",
    "# source: https://github.com/fastai/fastai/blob/master/fastai/callback/training.py#L55\n",
    "#     run_after=TrainEvalCallback\n",
    "#     \"Freeze moving average statistics in all non-trainable batchnorm layers.\"\n",
    "#     def before_train(self):\n",
    "#         set_bn_eval(self.model)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Experiments\n",
    "\n",
    "For each architecture, 3 different experimental training methods will be evaluated\n",
    "\n",
    "Training methods:  \n",
    "1. Feature Extractor  \n",
    "2. Feature Extractor + set batchnorm to eval()  \n",
    "3. Freeze backbone except for all batchnorm layers  \n",
    "\n",
    "\n",
    "\n",
    "Baseline:  \n",
    "   * Architecture 1. simple pretrained backbone -> `avg_pool` -> linear_classifier  \n",
    "    \n",
    "Next Comparisons:  \n",
    "* Architecture 2. simple pretrained backbone -> `max_pool` -> linear_classifier\n",
    "* Architecture 3. simple pretrained backbone -> `avgmax_pool` -> linear_classifier  \n",
    "\n",
    "Later:  \n",
    "* Architecture 4. simple pretrained backbone -> `avgdrop_pool` -> linear_classifier  \n",
    "* Architecture 5. simple pretrained backbone -> `maxdrop_pool` -> linear_classifier  \n",
    "* Architecture 6. simple pretrained backbone -> `avgmaxdrop_pool` -> linear_classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = build_model(backbone_name='gluon_seresnext50_32x4d',\n",
    "#                     pretrained=True,\n",
    "#                     num_classes=19,\n",
    "#                     pool_size=1,\n",
    "#                     pool_type='avgmax',\n",
    "#                     head_type='linear',\n",
    "#                     hidden_size=None)\n",
    "# from torchinfo import summary\n",
    "\n",
    "\n",
    "#left off here 11 pm\n",
    "\n",
    "# model = LightningClassifier(backbone_name='gluon_seresnext50_32x4d',\n",
    "#                             pretrained=True,\n",
    "#                             num_classes=19,\n",
    "#                             pool_size=1,\n",
    "#                             pool_type='avgmax',\n",
    "#                             head_type='linear',\n",
    "#                             hidden_size=None,\n",
    "#                             lr=2e-03,\n",
    "#                             weight_decay=0.01,\n",
    "#                             seed=98)\n",
    "\n",
    "# pp(list(model.get_batchnorm_modules()))\n",
    "# # pp(list(model.get_conv_modules()))\n",
    "# pp(list(model.get_linear_modules()))\n",
    "# # pp(list(model.get_conv_modules()))\n",
    "# pp(list(model.get_named_modules()))\n",
    "\n",
    "# model.freeze_backbone(freeze_bn=False)\n",
    "# model.freeze_backbone(freeze_bn=True)\n",
    "# summary(model.model)\n",
    "\n",
    "# print(f\"trainable: {len(list(model.get_trainable_parameters()))}\")\n",
    "# print(f\"non-trainable: {len(list(model.get_nontrainable_parameters()))}\")\n",
    "\n",
    "# bn = {n:p.requires_grad for n, p in model.get_named_parameters(\"bn\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini test: Wrap all model hooks & display + verify for each parameter group the proper status of module.training & weight_tensors.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_backbone(freeze_bn=True) #False)\n",
    "# model.set_bn_eval()\n",
    "# summary(model.model)\n",
    "model.count_trainable_batchnorm_layers()\n",
    "count_parameters(model.model, verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2. Feature extractor + BN set to Eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_backbone(freeze_bn=True)\n",
    "model.set_bn_eval(model)\n",
    "# summary(model.model)\n",
    "model.count_trainable_batchnorm_layers()\n",
    "count_parameters(model.model, verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Freeze backbone except for BN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.unfreeze(model.model)\n",
    "model.freeze_backbone(freeze_bn=False)\n",
    "\n",
    "model.unfreeze(model.model,\n",
    "               filter_pattern=\"bn\")\n",
    "# summary(model.model)\n",
    "model.count_trainable_batchnorm_layers()\n",
    "count_parameters(model.model, verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data, model, trainer, callbacks, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.scripts.multitask.train import MultiTaskDataModule, LitMultiTaskModule, ImagePredictionLogger, train_task,  CIFAR10DataModule, run_multitask_test, load_data_and_model, load_data, resolve_config, configure_callbacks, configure_loggers, configure_trainer\n",
    "from lightning_hydra_classifiers.data.datasets.common import toPIL\n",
    "from lightning_hydra_classifiers.utils.etl_utils import ETL\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "# overrides = ['model/backbone=efficientnet_b3',\"data=extant_to_fossil\", \"trainer.max_epochs=2\", \"data.batch_size=16\", \"trainer.precision=16\"]\n",
    "overrides = ['model/backbone=resnet50',\"data=extant_to_pnas\", \"trainer.max_epochs=20\", \"data.batch_size=32\", \"trainer.precision=16\", \"trainer.gpus=[7]\"]\n",
    "config = ETL.load_hydra_config(config_name = \"config\",\n",
    "                              config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\",\n",
    "                              overrides=overrides)\n",
    "\n",
    "task_id = 1\n",
    "pp(config.data)\n",
    "datamodule = load_data(config,\n",
    "                       task_id=task_id)\n",
    "\n",
    "\n",
    "        \n",
    "# model_config = OmegaConf.create(dict(\n",
    "#                                 backbone_name=config.model.backbone.backbone_name, #'gluon_seresnext50_32x4d',\n",
    "#                                 pretrained=True,\n",
    "#                                 num_classes=datamodule.num_classes,\n",
    "#                                 pool_type='avg',\n",
    "#                                 head_type='linear',\n",
    "#                                 hidden_size=None,\n",
    "#                                 lr=2e-03,\n",
    "#                                 weight_decay=0.01,\n",
    "#                                 seed=98))\n",
    "\n",
    "# model_config = OmegaConf.create(dict(\n",
    "#                                 backbone_name=config.model.backbone.backbone_name, #'gluon_seresnext50_32x4d',\n",
    "#                                 pretrained=True,\n",
    "#                                 num_classes=datamodule.num_classes,\n",
    "#                                 pool_type='max',\n",
    "#                                 head_type='linear',\n",
    "#                                 hidden_size=None,\n",
    "#                                 lr=2e-03,\n",
    "#                                 weight_decay=0.01,\n",
    "#                                 seed=98))\n",
    "\n",
    "model_config = OmegaConf.create(dict(\n",
    "                                backbone_name=config.model.backbone.backbone_name, #'gluon_seresnext50_32x4d',\n",
    "                                pretrained=True,\n",
    "                                num_classes=datamodule.num_classes,\n",
    "                                pool_type='avgdrop',\n",
    "                                head_type='linear',\n",
    "                                hidden_size=None,\n",
    "                                lr=2e-03,\n",
    "                                backbone_lr_mult=0.1,\n",
    "                                finetuning_strategy=\"feature_extractor\",\n",
    "                                weight_decay=0.01,\n",
    "                                seed=98))\n",
    "\n",
    "\n",
    "\n",
    "config.model = model_config\n",
    "\n",
    "algorithm_name = \"feature_extractor\"\n",
    "config.experiment_name = f\"{algorithm_name}-PNAS-{datamodule.num_classes}_classes-res_{config.data.image_size}-bsz_{config.data.batch_size}-{config.model.backbone_name}-pretrained_{config.model.pretrained}-pool_{config.model.pool_type}\"\n",
    "\n",
    "\n",
    "experiment_dir = config.experiment_dir\n",
    "results_dir = config.results_dir\n",
    "results_dir\n",
    "\n",
    "model = LightningClassifier(**config.model)\n",
    "model.label_encoder = datamodule.label_encoder\n",
    "\n",
    "\n",
    "group = f'{config.model.backbone_name}__PNAS__experiment_0__feature_extractor'\n",
    "config.logger.wandb.group = group\n",
    "config.callbacks.log_per_class_metrics_to_wandb.class_names = datamodule.classes\n",
    "\n",
    "\n",
    "callbacks = configure_callbacks(config)\n",
    "logger = configure_loggers(config)\n",
    "\n",
    "trainer: pl.Trainer = configure_trainer(config, callbacks=callbacks, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Run lr_tune->fit->test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.scripts.pretrain import lr_tuner\n",
    "\n",
    "\n",
    "lr_tuner_results_dir = os.path.join(results_dir, f\"task_{task_id}\", \"lr_tuner\")\n",
    "lr_tune_output = lr_tuner.run_lr_tuner(trainer=trainer,\n",
    "                                       model=model,\n",
    "                                       datamodule=datamodule,\n",
    "                                       config=config,\n",
    "                                       results_dir=lr_tuner_results_dir,\n",
    "                                       group=group)\n",
    "\n",
    "## model.fit\n",
    "\n",
    "hist = trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "## model.test\n",
    "\n",
    "test_result = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report\n",
    "\n",
    "\n",
    "# def predict_step(batch, batch_idx=None):\n",
    "#     out = self.step(batch, batch_idx)\n",
    "#     if hasattr(batch, \"metadata\"):\n",
    "#         if \"path\" in batch.metadata:\n",
    "#             out = [*out, batch.metadata[\"path\"]]\n",
    "#     return out\n",
    "\n",
    "# self=model\n",
    "# model.predict_step = predict_step\n",
    "# test_results = trainer.predict(dataloaders=datamodule.test_dataloader(), return_predictions=True)\n",
    "# results = collect_results(prediction_results)\n",
    "prediction_results = test_results\n",
    "results = collect_results(prediction_results)\n",
    "len(prediction_results[0])\n",
    "# len(results)\n",
    "\n",
    "def tensors2np(t: Union[torch.Tensor, list]) -> np.ndarray:\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        t = t.cpu().numpy()\n",
    "    elif isinstance(t, list):\n",
    "        t = list(map(tensors2np, t))\n",
    "    if isinstance(t, np.ndarray):\n",
    "        return t\n",
    "    else:\n",
    "        raise TypeError(f\"type(t)={type(t)} is invalid for function tensors2np\" + '\\n' + 'tensors2np(t: Union[torch.Tensor, list]) -> np.ndarray:')\n",
    "        \n",
    "rows = []\n",
    "for result in list(prediction_results):\n",
    "    \n",
    "    y_logit.append(result[0])\n",
    "    y_true.append(result[1])\n",
    "    y_pred.append(result[2])\n",
    "    paths.extend(result[3])\n",
    "    \n",
    "y_logit = torch.cat(y_logit).cpu().numpy()\n",
    "y_true = torch.cat(y_true).cpu().numpy()\n",
    "y_pred = torch.cat(y_pred).cpu().numpy()\n",
    "# paths = torch.cat(paths).cpu().numpy()\n",
    "\n",
    "# [(r[0].device, r[0].shape, r[1].shape, r[2].shape) for r in test_results]\n",
    "\n",
    "print(y_logit.shape, y_true.shape, y_pred.shape, len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = model.label_encoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F\n",
    "\n",
    "\n",
    "\n",
    "# https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwji_bKG6cPzAhXJT98KHU0eCsg4HhAWegQIAhAB&url=https%3A%2F%2Fclear.ml%2Fdocs%2Flatest%2Fdocs%2Fguides%2Freporting%2Fexplicit_reporting%2F&usg=AOvVaw3tvUYT7fU3QHIwunDpE800\n",
    "labels = model.label_encoder.classes\n",
    "\n",
    "test_predictions_filepath = os.path.join(results_dir, f\"task_{task_id}\", \"test_predictions.csv\")\n",
    "\n",
    "\n",
    "class ImageInterpretation:\n",
    "    \n",
    "    def __init__(self, model, datamodule, trainer, y_col: str='family'):\n",
    "        self.model = model\n",
    "        self.dm = datamodule\n",
    "        self.trainer = trainer\n",
    "        self.y_col = y_col\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def decoder(self):\n",
    "        return self.dm.label_encoder.idx2class\n",
    "        \n",
    "    def decode_label(y: int): #, labels: Union[Dict[int, str], List[str]]=None):\n",
    "        try:\n",
    "            return self.decoder[y]\n",
    "        except:\n",
    "            return y\n",
    "        \n",
    "    def log_image_predictions(self,\n",
    "                              results_path: str=None,\n",
    "                              sort_by_losses: bool=True,\n",
    "                              ascending: bool=True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Save table of model predictions as csv\n",
    "        \n",
    "        |losses\t|y_true\t|y_pred\t|paths \t|per-class logits|\n",
    "        |---\t|---\t|---\t|---\t| ---\t |---\t |\n",
    "        |   \t|   \t|   \t|   \t|   \t |   \t |\n",
    "        |   \t|   \t|   \t|   \t|   \t |   \t |\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        pred_results = trainer.predict(dataloaders=datamodule.test_dataloader(), return_predictions=True)\n",
    "\n",
    "        results = collect_results(pred_results)\n",
    "\n",
    "        labels = list(self.decoder.values())\n",
    "        columns = [\"xEnt_loss\", f\"{self.y_col}_true\", f\"{self.y_col}_pred\", \"paths\", *[f\"{l}_logit\" for l in labels]]\n",
    "\n",
    "        y_logits = torch.from_numpy(results[0].astype(\"float32\"))\n",
    "        y_true = torch.from_numpy(results[1])\n",
    "        xEnt_loss = F.cross_entropy(y_logits, y_true, reduction=\"none\")\n",
    "\n",
    "        losses = xEnt_loss\n",
    "        y_true = results[1]\n",
    "        y_pred = results[2]\n",
    "        paths = results[3]\n",
    "        per_class_y_logits = np.hsplit(results[0], results[0].shape[1])\n",
    "\n",
    "        num_results = len(results[0])\n",
    "        rows = []\n",
    "        for i in range(num_results):\n",
    "            rows.append({k:v for k, v in zip(columns,\n",
    "                                             [losses,\n",
    "                                              self.decode_label(y=y_true[i]),\n",
    "                                              self.decode_label(y=y_pred[i]),\n",
    "                                              paths[i],\n",
    "                                              *(y[i].item() for y in per_class_y_logits)]\n",
    "                                            )\n",
    "                        })\n",
    "\n",
    "        data_df = pd.DataFrame.from_records(rows)\n",
    "        \n",
    "        if sort_by_losses:\n",
    "            data_df = data_df.sort_values(\"xEnt_loss\", ascending=ascending)\n",
    "        \n",
    "        ETL.df2csv(data_df, results_spath)\n",
    "        \n",
    "        return data_df\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#         data = {\"xEnt_loss\":xEnt_loss,\n",
    "#                 f\"{self.y_col}_true\":results[1],\n",
    "#                 f\"{self.y_col}_pred\":results[2],\n",
    "#                 \"path\":results[3],\n",
    "#                 \"y_logits\":np.hsplit(results[0], results[0].shape[1])}    \n",
    "    \n",
    "    \n",
    "    \n",
    "#         rows = []\n",
    "#         for i in range(num_results):\n",
    "#             rows.append({k:v for k, v in zip(columns,\n",
    "#                                              [data[\"xEnt_loss\"][i], \n",
    "#                                               self.decode_label(y=data[f\"{self.y_col}_true\"][i]),\n",
    "#                                               self.decode_label(y=data[f\"{self.y_col}_pred\"][i]),\n",
    "#                                               data[\"path\"][i],\n",
    "#                                               *(y[i].item() for y in data[\"y_logits\"])]\n",
    "#                                             )\n",
    "#                         })\n",
    "\n",
    "#         data_df = pd.DataFrame.from_records(rows)\n",
    "#         ETL.df2csv(data_df, test_predictions_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.sort_values(\"Anacardiaceae_logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df\n",
    "test_predictions_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.utils.report_utils.pandas_embed_images import df_embed_paths2imgs\n",
    "\n",
    "\n",
    "\n",
    "df_embed_paths2imgs(df: pd.DataFrame,\n",
    "                        file_path: str, \n",
    "                        path_col: str=\"path\",\n",
    "                        display: bool=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paths)\n",
    "\n",
    "y_logit#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(y_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_report(y_pred,\n",
    "                    y_true, \n",
    "                    labels=None,\n",
    "                    results_dir: str=None):\n",
    "    \"\"\"Create a performance report for the current experiment and \n",
    "    consolidate the information to a general report of all runs \n",
    "    Parameters\n",
    "    ----------\n",
    "    opt : sklearn.model_selection.Object\n",
    "        A hyperparameter \n",
    "    X_test: numpy array or pandas Dataframe \n",
    "        Input test data\n",
    "    y_test: numpy array or pandas Dataframe \n",
    "        Target test data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    logger.info(\"Generating Evaluation Report:\")\n",
    "    \n",
    "\n",
    "    res = classification_report(y_true, y_pred, labels=labels, output_dict=True)\n",
    "    res = pd.DataFrame(res)\n",
    "\n",
    "    logger.info(\"Test report:\")\n",
    "    logger.info('\\n \\t'+ res.to_string().replace('\\n', '\\n\\t'))\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, labels=labels, average='macro')\n",
    "    \n",
    "    steps= [*pipeline.named_steps]\n",
    "\n",
    "    cv_mean ,cv_std = opt.best_score_,opt.cv_results_['std_test_score'][opt.best_index_]\n",
    "\n",
    "    tmp= pd.DataFrame({\"Scaling\":[steps[0]],\n",
    "                        \"Model\":[steps[1]],\n",
    "                        \"params\":[opt.best_params_],\n",
    "                        'CV Mean':[cv_mean],\n",
    "                        'CV Std':[cv_std],\n",
    "                        'Test dataset':f1,\n",
    "                        })\n",
    "\n",
    "    if os.path.exists(path+\"/results.csv\"):\n",
    "        current_csv =pd.read_csv(path+\"/results.csv\")\n",
    "        pd.concat([current_csv, tmp], \n",
    "                   ignore_index=True\n",
    "                 ).to_csv(path+\"/results.csv\",\n",
    "                          index=False)    \n",
    "    else:\n",
    "        tmp.to_csv(path+\"/results.csv\",\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = LightningClassifier(backbone_name='gluon_seresnext50_32x4d',\n",
    "                            pretrained=True,\n",
    "                            num_classes=19,\n",
    "                            pool_size=1,\n",
    "                            pool_type='avgmax',\n",
    "                            head_type='linear',\n",
    "                            hidden_size=None,\n",
    "                            lr=2e-03,\n",
    "                            weight_decay=0.01,\n",
    "                            seed=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bqplot.pyplot as plt\n",
    "# from dataclasses import dataclass\n",
    "# @dataclass\n",
    "# class LRTunerConfig:\n",
    "    \n",
    "#     min_lr: float = 1e-08\n",
    "#     max_lr: float = 1.0\n",
    "#     num_training: int = 50\n",
    "#     mode: str = 'exponential'\n",
    "#     early_stop_threshold: float = 4.0\n",
    "\n",
    "# cfg = OmegaConf.structured(LRTunerConfig())\n",
    "\n",
    "# lr_tuner = trainer.tuner.lr_find(model,\n",
    "#                                  data,\n",
    "#                                  **cfg)\n",
    "# lr_tuner_results = lr_tuner.results\n",
    "# best_lr = lr_tuner.suggestion()\n",
    "\n",
    "# suggestion = {\"lr\": best_lr,\n",
    "#               \"loss\":lr_tuner_results['loss'][lr_tuner._optimal_idx]}\n",
    "\n",
    "# plt.figure()\n",
    "# fig = lr_tuner.plot(suggest=True)\n",
    "# lr_tuner_results_dir = os.path.join(results_dir, f\"task_{task_id}\", \"lr_tuner\")\n",
    "\n",
    "# plot_fname = 'lr_tuner_results_loss-vs-lr.png'\n",
    "# plot_path = Path(lr_tuner_results_dir) / plot_fname\n",
    "# plt.title(f\"Suggested lr={best_lr:.4e} |\\n| Searched {lr_tuner.num_training} lr values $\\in$ [{lr_tuner.lr_min},{lr_tuner.lr_max}] |\\n| bsz = {config.data.batch_size}\", style={\"fontsize\":'small'})\n",
    "# fig.save_png(filename=str(plot_path))\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.plot(x=lr_tuner.results['lr'],\n",
    "#          y=lr_tuner.results['loss'],\n",
    "#         figure=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Display available global pool types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aside: Verify task_0 and task_1 label maps all agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_0_labels = data.label_encoder\n",
    "\n",
    "data.setup(stage='fit', task_id=1)\n",
    "\n",
    "task_1_labels = data.label_encoder\n",
    "\n",
    "task_0_labels\n",
    "task_1_labels\n",
    "\n",
    "task_0_labels\n",
    "print(f\"label|task_0_idx|task_1_idx\")\n",
    "for label, idx in task_1_labels.class2idx.items():\n",
    "    print(f\"{label}|{task_0_labels.class2idx[label]}|{idx}\")\n",
    "    \n",
    "    assert task_0_labels.class2idx[label] == idx\n",
    "    \n",
    "print(f\"Success, all labels in task_1 have identical integer mappings to their corresponding values in task_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2idx = data.label_encoder.class2idx\n",
    "family_counts = df.value_counts(\"family\").to_dict()\n",
    "\n",
    "df = df.assign(class_idx=df.family.apply(lambda x: class2idx[x]),\n",
    "               score = df.family.apply(lambda x: family_counts[x]))\n",
    "\n",
    "df#.clear_intent()\n",
    "\n",
    "df.groupby('class_idx').mean()\n",
    "\n",
    "# df.exported.keys()\n",
    "\n",
    "df.exported['Distribution']\n",
    "\n",
    "df.compute_metadata()\n",
    "\n",
    "df\n",
    "\n",
    "df.clear_intent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_name='gluon_seresnext50_32x4d'\n",
    "pretrained=True\n",
    "num_classes=1000\n",
    "\n",
    "head_type='linear'\n",
    "hidden_size=0\n",
    "\n",
    "pool_types = [\"avg\", \"max\", \"avgmax\"]\n",
    "\n",
    "models = OrderedDict({})\n",
    "\n",
    "for pool_type in pool_types:\n",
    "    models[pool_type] = build_model(backbone_name=backbone_name,\n",
    "                                    pretrained=pretrained,\n",
    "                                    num_classes=num_classes,\n",
    "                                    pool_size=1,\n",
    "                                    pool_type=pool_type,\n",
    "                                    head_type=head_type,\n",
    "                                    hidden_size=hidden_size)\n",
    "print(f\"backbone={backbone_name}|pretrained={pretrained}|num_classes={num_classes}|head_type={head_type}|hidden_size={hidden_size}\")\n",
    "for pool_type, model in models.items():\n",
    "    print(f\"pool_type={pool_type}\")\n",
    "    pp({k: v.shape for k,v in model.head.named_parameters()})\n",
    "#     pp(list(dict(model.head.named_parameters()).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Display available head types (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_name='gluon_seresnext50_32x4d'\n",
    "pretrained=True\n",
    "num_classes=1000\n",
    "\n",
    "head_types=['linear', 'custom']\n",
    "hidden_size=0\n",
    "\n",
    "pool_types = [\"avg\", \"max\", \"avgmax\"]\n",
    "\n",
    "models = OrderedDict({})\n",
    "\n",
    "for pool_type in pool_types:\n",
    "    models[pool_type] = build_model(backbone_name=backbone_name,\n",
    "                                    pretrained=pretrained,\n",
    "                                    num_classes=num_classes,\n",
    "                                    pool_size=1,\n",
    "                                    pool_type=pool_type,\n",
    "                                    head_type=head_type,\n",
    "                                    hidden_size=hidden_size)\n",
    "print(f\"backbone={backbone_name}|pretrained={pretrained}|num_classes={num_classes}|head_type={head_type}|hidden_size={hidden_size}\")\n",
    "for pool_type, model in models.items():\n",
    "    print(f\"pool_type={pool_type}\")\n",
    "    pp({k: v.shape for k,v in model.head.named_parameters()})\n",
    "#     pp(list(dict(model.head.named_parameters()).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borrowed from fastai2 library\n",
    "\n",
    "bn_types = (torch.nn.modules.batchnorm.BatchNorm1d,torch.nn.modules.batchnorm.BatchNorm2d,torch.nn.modules.batchnorm.BatchNorm3d)\n",
    " \n",
    "def set_bn_eval(m:nn.Module)->None:\n",
    "    \"Set bn layers in eval mode for all recursive children of `m`.\"\n",
    "    for l in m.children():\n",
    "        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n",
    "            l.eval()\n",
    "        set_bn_eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,model,train_dl,valid_dl,loss_fn,opt,device=None,bn_eval=False):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write(['epoch','train_loss','valid_loss','trn_acc','val_acc'],table=True)\n",
    "    model.to(device)\n",
    "\n",
    "    for i in mb:    \n",
    "        trn_loss,val_loss = 0.0,0.0\n",
    "        trn_acc,val_acc = 0,0\n",
    "        trn_n,val_n = len(train_dl.dataset),len(valid_dl.dataset)\n",
    "        model.train()\n",
    "        if bn_eval:set_bn_eval(model)\n",
    "        for xb,yb in progress_bar(train_dl,parent=mb):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(out,yb)\n",
    "            _,pred = torch.max(out.data, 1)\n",
    "            trn_acc += (pred == yb).sum().item()\n",
    "            trn_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        trn_loss /= mb.child.total\n",
    "        trn_acc /= trn_n\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in progress_bar(valid_dl,parent=mb):\n",
    "                xb,yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = loss_fn(out,yb)\n",
    "                val_loss += loss.item()\n",
    "                _,pred = torch.max(out.data, 1)\n",
    "                val_acc += (pred == yb).sum().item()\n",
    "        val_loss /= mb.child.total\n",
    "        val_acc /= val_n\n",
    "\n",
    "        mb.write([i,f'{trn_loss:.6f}',f'{val_loss:.6f}',f'{trn_acc:.6f}',f'{val_acc:.6f}'],table=True)        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(model,bn_freeze=True):\n",
    "    for name,param in model.named_parameters():\n",
    "        if bn_freeze:\n",
    "            param.requires_grad = False\n",
    "        elif name.find('bn') == -1:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def unfreeze(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def get_model(lrs=[1e-3,1e-3],bn_freeze=True):\n",
    "    model = MyResNet()\n",
    "    freeze(model.body,bn_freeze=bn_freeze)\n",
    "    opt = optim.Adam([{'params': model.body.parameters(), 'lr':lrs[0]},\n",
    "                {'params': model.head.parameters(), 'lr': lrs[1]}])\n",
    "    return model,opt\n",
    "\n",
    "def update_lr(lr,opt):\n",
    "    opt.param_groups[0]['lr'] = lr/100\n",
    "    opt.param_groups[1]['lr'] = lr\n",
    "    \n",
    "\n",
    "### Freeze the complete resnet body\n",
    "\n",
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=True)\n",
    "fit(2,model,trn_dl,valid_dl,loss_fn,opt)\n",
    "\n",
    "### Freeze the complete resnet body and place BN layers in eval mode.\n",
    "\n",
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=True)\n",
    "fit(2,model,trn_dl,valid_dl,loss_fn,opt,bn_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Freeze the complete resnet body and place BN layers in eval mode and train the body at a lesser learning rate for the second epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=True)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt,bn_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_lr(lr/2,opt)\n",
    "unfreeze(model)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Freeze the resnet body except fot BN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model(lrs=[1e-3,1e-3],bn_freeze=False)\n",
    "fit(2,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the resnet body except fot BN layers and try smaller leraning rate for the resnet body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=False)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)\n",
    "update_lr(lr/2,opt)\n",
    "unfreeze(model)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try adjusting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveConcatPooling(nn.Module):\n",
    "    def forward(self,x):\n",
    "        avg_pool = F.adaptive_avg_pool2d(x,1)\n",
    "        max_pool = F.adaptive_max_pool2d(x,1)\n",
    "        return torch.cat([avg_pool,max_pool],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.body = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.head = nn.Sequential(AdaptiveConcatPooling(),Flatten(),nn.Linear(512*2,2))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.body(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=False)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)\n",
    "update_lr(lr/2,opt)\n",
    "unfreeze(model)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the complexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        nf = 512*2\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.body = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.head = nn.Sequential(AdaptiveConcatPooling(),Flatten(),nn.BatchNorm1d(nf),nn.Dropout(p=0.25),\n",
    "                      nn.Linear(nf,nf//2,bias=False),nn.ReLU(inplace=True),nn.BatchNorm1d(nf//2),nn.Dropout(p=0.75),\n",
    "                      nn.Linear(nf//2,2,bias=False))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.body(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model,opt = get_model(lrs=[lr,lr],bn_freeze=False)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)\n",
    "update_lr(lr/2,opt)\n",
    "unfreeze(model)\n",
    "fit(1,model,trn_dl,valid_dl,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "                \n",
    "\n",
    "# ## WIP: display_layer_status\n",
    "#     @classmethod\n",
    "#     def display_layer_status(cls,\n",
    "#                              model: nn.Module,\n",
    "#                              max_depth: int=3):\n",
    "#         \"\"\"\n",
    "#         Return a formatted display of model's layers alongside relevant training status info.\n",
    "#         \"\"\"\n",
    "#         modules = []\n",
    "        \n",
    "# #         for name, module in model.named_modules():\n",
    "#         for name, module in model.named_children():\n",
    "# #             print(name, 'max_depth:', max_depth)\n",
    "#             if name==\"\": continue\n",
    "# #             if (max_depth>0) and (len(list(module.named_modules())) > 0):\n",
    "#             if (max_depth>0) and (len(list(module.named_children())) > 0):\n",
    "#                 modules.extend(cls.display_layer_status(module, max_depth=max_depth-1))\n",
    "#                 continue\n",
    "#             module_out = {\"name\":name,\n",
    "#                           \"training\":module.training,\n",
    "#                           \"type\":type(module),\n",
    "#                           \"params\":[]}\n",
    "#             for param_name, param in module.named_parameters():\n",
    "#                 module_out[\"params\"].append({\n",
    "#                     \"name\":param_name,\n",
    "#                     \"type\":type(param),\n",
    "#                     \"requires_grad\":param.requires_grad,\n",
    "#                     \"shape\":param.shape\n",
    "#                 })\n",
    "#             print(name)\n",
    "#             pp(module_out)\n",
    "#             modules.append(module_out)\n",
    "#         return modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for unfreeze_down_to in reversed(range(0,-8,-1)):\n",
    "# model.freeze_backbone(freeze_bn=False)\n",
    "# model.freeze_backbone(freeze_bn=True)\n",
    "# summary(model.model)\n",
    "# count_trainable_batchnorm_layers(model)\n",
    "\n",
    "# for unfreeze_down_to in range(0,-9,-1):\n",
    "#     print(unfreeze_down_to)\n",
    "#     print(f\"Unfreezing backbone down to layer: {unfreeze_down_to}\")\n",
    "#     model.unfreeze_backbone_top_layers(unfreeze_down_to=unfreeze_down_to)\n",
    "# #     summary(model.model)\n",
    "#     model.count_trainable_batchnorm_layers()\n",
    "# #     count_trainable_batchnorm_layers(model)\n",
    "\n",
    "#     print(f\"trainable parameters: {len(list(model.get_trainable_parameters()))}\")\n",
    "#     print(f\"non-trainable parameters: {len(list(model.get_nontrainable_parameters()))}\")\n",
    "\n",
    "# unfreeze_down_to = -2\n",
    "\n",
    "# model.unfreeze_backbone_top_layers(unfreeze_down_to=unfreeze_down_to)\n",
    "# summary(model.model)\n",
    "# count_trainable_batchnorm_layers(model)\n",
    "\n",
    "# print(f\"trainable parameters: {len(list(model.get_trainable_parameters()))}\")\n",
    "# print(f\"non-trainable parameters: {len(list(model.get_nontrainable_parameters()))}\")\n",
    "\n",
    "# unfreeze_down_to = -3\n",
    "\n",
    "# model.unfreeze_backbone_top_layers(unfreeze_down_to=unfreeze_down_to)\n",
    "# summary(model.model)\n",
    "# print(f\"trainable: {len(list(model.get_trainable_parameters()))}\")\n",
    "# print(f\"non-trainable: {len(list(model.get_nontrainable_parameters()))}\")\n",
    "\n",
    "# unfreeze_down_to = -4\n",
    "\n",
    "# model.unfreeze_backbone_top_layers(unfreeze_down_to=unfreeze_down_to)\n",
    "# summary(model.model)\n",
    "# print(f\"trainable: {len(list(model.get_trainable_parameters()))}\")\n",
    "# print(f\"non-trainable: {len(list(model.get_nontrainable_parameters()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     model = timm.create_model(model_name=backbone_name, num_classes=1000, pretrained=pretrained)\n",
    "#     if isinstance(pretrained, str) and pretrained != \"imagenet\":\n",
    "#         model = load_model_checkpoint(model, ckpt_path=pretrained)\n",
    "# #         ckpt_pth = glob.glob(hydra.utils.to_absolute_path(pretrained))\n",
    "# #         model = model.load_state_dict(torch.load(ckpt_pth[0]))\n",
    "        \n",
    "#     body = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "    \n",
    "#     feature_size = model.fc.in_features\n",
    "    \n",
    "#     head = OrderedDict()\n",
    "#     global_pool, feature_size = build_global_pool(pool_type=pool_type,\n",
    "#                                                   pool_size=pool_size,\n",
    "#                                                   feature_size=feature_size)\n",
    "#     head[\"global_pool\"] = global_pool\n",
    "#     head[\"flatten\"] = Flatten()\n",
    "    \n",
    "#     classifier_input_feature_size = feature_size*(pool_size*2)        \n",
    "#     if head_type=='linear':\n",
    "#         head[\"classifier\"] = nn.Linear(classifier_input_feature_size, num_classes)\n",
    "#     elif head_type=='custom':\n",
    "#         head[\"classifier\"] = nn.Sequential(nn.Linear(classifier_input_feature_size, hidden_size),\n",
    "#                                 nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False),\n",
    "#                                 nn.BatchNorm1d(hidden_size),\n",
    "#                                 nn.Linear(hidden_size, num_classes))\n",
    "        \n",
    "#     head = nn.Sequential(head)\n",
    "\n",
    "\n",
    "#     model = nn.Sequential(OrderedDict({\n",
    "#         \"body\":body,\n",
    "#         \"head\":head\n",
    "#     }))\n",
    "#     return model\n",
    "\n",
    "# def build_model(backbone_name='gluon_seresnext50_32x4d',\n",
    "#                 pretrained: Union[bool, str]=True,\n",
    "#                 num_classes: int=1000,\n",
    "#                 pool_size: int=1,\n",
    "#                 pool_type: str='avg',\n",
    "#                 head_type: str='linear',\n",
    "#                 hidden_size: Optional[int]=512):\n",
    "    \n",
    "#     try:\n",
    "#         model = build_timm_custom(backbone_name=backbone_name,\n",
    "#                                   pretrained=pretrained,\n",
    "#                                   num_classes=num_classes,\n",
    "#                                   pool_size=pool_size,\n",
    "#                                   pool_type=pool_type,\n",
    "#                                   head_type=hidden_size,\n",
    "#                                   hidden_size=hidden_size)\n",
    "\n",
    "#     except:\n",
    "#         print\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
