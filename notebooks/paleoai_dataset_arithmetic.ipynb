{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1d1425",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PaleoAI Dataset Arithmetic\n",
    "\n",
    "`paleoai_dataset_arithmetic.ipynb`\n",
    "\n",
    "This notebook is for using pandas dataframes for elegantly adding or subtracting sets of data samples while maintaining unique-specimen constraints based on enforcing uniqueness for a user-specified id column. Using simple operator overloading in Python class definitions, we can perform complex queries with minimal boilerplate.\n",
    "\n",
    "Author: Jacob A Rose  \n",
    "Created on: Monday July 19th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a80d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3ecc41",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "won\n"
     ]
    }
   ],
   "source": [
    "# !conda list\n",
    "# !pip list\n",
    "test = \"won\"\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810a80cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def left_union(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\", suffixes=(\"_x\", \"_y\")) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Return a new dataframe containing all rows from `data_df`, concatenated with any rows that only exist in `other_df`. Any rows that are shared between the 2 default to only including the values from `data_df`.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     return data_df.merge(other_df, how='outer', on=id_col, suffixes=suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9dcadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def intersection(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\", suffixes=(\"_x\", \"_y\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a new dataframe containing only rows that share the same values for `id_col` between `data_df` and `other_df`\n",
    "    \n",
    "    Equivalent to an AND join between sets\n",
    "    \"\"\"\n",
    "    return data_df.merge(other_df, how='inner', on=id_col, suffixes=suffixes)\n",
    "\n",
    "\n",
    "def left_exclusive(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a new dataframe containing only rows from `data_df` that do not share an `id_col` value with any row from `other_df`.\n",
    "    \n",
    "    Equivalent to subtracting the set of `id_col` values in `other_df` from `data_df`\n",
    "    \"\"\"\n",
    "    omit = list(other_df[id_col].values)\n",
    "    \n",
    "    return data_df[data_df[id_col].apply(lambda x: x not in omit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf03f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extant_df = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Extant-dataset_leavesdb-v0_3.csv\", index_col=0)\n",
    "# pnas_train = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100/train.csv\")\n",
    "# pnas_test = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100/test.csv\")\n",
    "# pnas_df = pd.concat([pnas_train, pnas_test])\n",
    "\n",
    "# extant_in_pnas = intersection(data_df=extant_df,\n",
    "#                               other_df=pnas_df,\n",
    "#                               id_col=\"catalog_number\",\n",
    "#                               suffixes=(\"_extant\", \"_pnas\"))\n",
    "\n",
    "# # In order to only keep original columns\n",
    "# suffixes=(\"_extant\", \"_pnas\")\n",
    "# extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\n",
    "# extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ed6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extant_minus_pnas -> rows exclusive to extant dataset\n",
    "# extant_minus_pnas = left_exclusive(data_df=extant_df,\n",
    "#                                    other_df=pnas_df,\n",
    "#                                    id_col=\"catalog_number\",\n",
    "#                                    suffixes=(\"_extant\", \"_pnas\"))\n",
    "\n",
    "# # pnas_minus_extant -> rows exclusive to pnas dataset\n",
    "# pnas_minus_extant = left_exclusive(data_df=pnas_df,\n",
    "#                                    other_df=extant_df,\n",
    "#                                    id_col=\"catalog_number\",\n",
    "#                                    suffixes=(\"_pnas\", \"_extant\"))\n",
    "\n",
    "# extant_minus_pnas = left_exclusive(data_df=extant_df, other_df=pnas_df, id_col=\"catalog_number\", suffixes=(\"_extant\", \"_pnas\"))\n",
    "\n",
    "# pnas_minus_extant = left_exclusive(data_df=pnas_df, other_df=extant_df, id_col=\"catalog_number\", suffixes=(\"_pnas\", \"_extant\"))\n",
    "\n",
    "# extant_and_pnas = left_union(data_df=extant_df, other_df=pnas_df, id_col=\"catalog_number\", suffixes=(\"_extant\", \"_pnas\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785feb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ab1601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/pytorch_lightning/metrics/__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os.path\n",
    " \n",
    "def initialize_logger():\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "     \n",
    "    # create console handler and set level to info\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "initialize_logger()\n",
    "    \n",
    "import torchdata\n",
    "from typing import Union, List, Any, Tuple\n",
    "# from collections import Counter\n",
    "from lightning_hydra_classifiers.utils import template_utils\n",
    "from lightning_hydra_classifiers.utils.common_utils import trainvaltest_split\n",
    "import collections\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from lightning_hydra_classifiers.data.common import CommonDataSelect, CommonDataset, LeavesLightningDataModule\n",
    "from lightning_hydra_classifiers.data import fossil, extant, pnas\n",
    "from rich import print as pp\n",
    "import os\n",
    "\n",
    "from typing import *\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "log = template_utils.get_logger(__name__, level=logging.DEBUG)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2249a1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd = /media/data/jacob/GitHub/lightning-hydra-classifiers/configs\n"
     ]
    }
   ],
   "source": [
    "config_dir = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\"\n",
    "\n",
    "from hydra.experimental import compose, initialize, initialize_config_dir\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "os.chdir(config_dir)\n",
    "print(f\"cwd = {os.getcwd()}\")\n",
    "\n",
    "def initialize_config(config_dir: str,\n",
    "                      overrides=None):\n",
    "    with initialize_config_dir(config_dir=config_dir, job_name=\"multi-gpu_experiment\"):\n",
    "\n",
    "        cfg = compose(config_name=\"multi-gpu\", overrides=overrides)\n",
    "        OmegaConf.set_struct(cfg, False)\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a86fdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.utils.common_utils import LabelEncoder, trainval_split\n",
    "from lightning_hydra_classifiers.data.common import CommonDataset, LeavesLightningDataModule, plot_split_distributions\n",
    "# default_config = initialize_config(config_dir=config_dir,\n",
    "#                                    overrides=[\"datamodule=default_datamodule\"])\n",
    "# config = DictConfig({\"datamodule.dataset.name\":\"Extant_family_10_minus_PNAS_family_100_512\"})\n",
    "# default_config = DictConfig({'datamodule':LeavesLightningDataModule.default_config()})\n",
    "\n",
    "# pp(OmegaConf.to_container(datamodule.datamodule_config, resolve=True))\n",
    "# default_config = DictConfig({'datamodule':LeavesLightningDataModule.default_config()})\n",
    "# user_config = DictConfig({\"datamodule\":\n",
    "#                               {\"dataset\":\n",
    "#                                    {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\n",
    "#                               }\n",
    "#                          })\n",
    "# pp(OmegaConf.to_container(OmegaConf.merge(default_config, user_config), resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce9da355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-22 17:50:20,667 lightning_hydra_classifiers.utils.common_utils INFO     LabelEncoder replacing 1 class encodings with that other an another class\n",
      "INFO - LabelEncoder replacing 1 class encodings with that other an another class\n",
      "2021-07-22 17:50:20,670 lightning_hydra_classifiers.utils.common_utils INFO     Replacing: {'Nothofagaceae': 'Fagaceae'}\n",
      "INFO - Replacing: {'Nothofagaceae': 'Fagaceae'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'config'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'config'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'return_signature'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'subset_key'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'class_type'\u001b[0m,\n",
       "    \u001b[32m'dataset_dirs'\u001b[0m,\n",
       "    \u001b[32m'id_col'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m,\n",
       "    \u001b[32m'num_samples'\u001b[0m,\n",
       "    \u001b[32m'path_schema'\u001b[0m,\n",
       "    \u001b[32m'return_signature'\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m,\n",
       "    \u001b[32m'subset_key'\u001b[0m,\n",
       "    \u001b[32m'test_split'\u001b[0m,\n",
       "    \u001b[32m'threshold'\u001b[0m,\n",
       "    \u001b[32m'val_train_split'\u001b[0m,\n",
       "    \u001b[32m'x_col'\u001b[0m,\n",
       "    \u001b[32m'y_col'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'config'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'config'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'return_signature'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'subset_key'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'class_type'\u001b[0m,\n",
       "    \u001b[32m'dataset_dirs'\u001b[0m,\n",
       "    \u001b[32m'id_col'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m,\n",
       "    \u001b[32m'num_samples'\u001b[0m,\n",
       "    \u001b[32m'path_schema'\u001b[0m,\n",
       "    \u001b[32m'return_signature'\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m,\n",
       "    \u001b[32m'subset_key'\u001b[0m,\n",
       "    \u001b[32m'test_split'\u001b[0m,\n",
       "    \u001b[32m'threshold'\u001b[0m,\n",
       "    \u001b[32m'val_train_split'\u001b[0m,\n",
       "    \u001b[32m'x_col'\u001b[0m,\n",
       "    \u001b[32m'y_col'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'config'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'config'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'return_signature'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'subset_key'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'class_type'\u001b[0m,\n",
       "    \u001b[32m'dataset_dirs'\u001b[0m,\n",
       "    \u001b[32m'id_col'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m,\n",
       "    \u001b[32m'num_samples'\u001b[0m,\n",
       "    \u001b[32m'path_schema'\u001b[0m,\n",
       "    \u001b[32m'return_signature'\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m,\n",
       "    \u001b[32m'subset_key'\u001b[0m,\n",
       "    \u001b[32m'test_split'\u001b[0m,\n",
       "    \u001b[32m'threshold'\u001b[0m,\n",
       "    \u001b[32m'val_train_split'\u001b[0m,\n",
       "    \u001b[32m'x_col'\u001b[0m,\n",
       "    \u001b[32m'y_col'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d9555517fdb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#                             })\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# config.dataset.config.name = \"Extant_family_10_1024_in_PNAS_family_100_1024\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m datamodule = LeavesLightningDataModule(config=None, #config, #default_config,\n\u001b[0m\u001b[1;32m      8\u001b[0m                                        data_dir=output_dir)\n\u001b[1;32m      9\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, data_dir)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetConstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_dataset_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, files, return_signature, subset_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"name must be a string if files is None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             files = self.select_dataset_by_name(name=self.name,\n\u001b[0m\u001b[1;32m    495\u001b[0m                                                 config=self.config)\n\u001b[1;32m    496\u001b[0m \u001b[0;31m#             assert len(files) > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/common.py\u001b[0m in \u001b[0;36mselect_dataset_by_name\u001b[0;34m(cls, name, config)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'available_datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must provide available dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mdataset_dirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mdataset_dirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset_dirs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_1024_minus_PNAS_family_100_1024\"\n",
    "\n",
    "#         config = DictConfig({\"dataset\":\n",
    "#                                        {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\n",
    "#                             })\n",
    "# config.dataset.config.name = \"Extant_family_10_1024_in_PNAS_family_100_1024\"\n",
    "datamodule = LeavesLightningDataModule(config=None, #config, #default_config,\n",
    "                                       data_dir=output_dir)\n",
    "config.hparams.classes = datamodule.classes\n",
    "config.hparams.num_classes = len(config.hparams.classes)\n",
    "config.dataset.config.classes = datamodule.classes\n",
    "config.dataset.config.num_classes = len(config.hparams.classes)\n",
    "\n",
    "\n",
    "data_loader = datamodule.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cd3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b68a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978bc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d1744a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'trainer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.Trainer'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'gpus'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'auto_select_gpus'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'min_epochs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'max_epochs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'weights_summary'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'top'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'progress_bar_refresh_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'profiler'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'simple'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'log_every_n_steps'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'terminate_on_nan'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'fast_dev_run'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'limit_train_batches'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'limit_val_batches'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accelerator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'ddp'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'amp_backend'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'native'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'amp_level'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'02'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'benchmark'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.models.transfer.TransferLearningModel'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'classifier_kwargs'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'backbone_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.backbone}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.num_classes}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'pretrained'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.pretrained}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'finetune'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'train_bn'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${callbacks.finetuning.train_bn}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'milestones'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${callbacks.finetuning.milestones}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.batch_size}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'optimizer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Adam'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.lr}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'lr_scheduler_gamma'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${log_dir}/model'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'datamodule'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.data.common.LeavesLightningDataModule'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'config'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${.dataset.name}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'normalize'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'image_size'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'grayscale'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'channels'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9877</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'augment'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${.dataset.num_classes}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'predict_on_split'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'debug'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${dataset.config}'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.data.common.CommonDataset'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'config'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'PNAS_family_100_1024'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">987485</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'family'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'path'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${.class_type}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'catalog_number'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'{family}_{genus}_{species}_{catalog_number}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'callbacks'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finetuning'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.models.transfer.MilestonesFinetuning'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'milestones'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.finetune_milestones}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'train_bn'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'checkpoint'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.callbacks.ModelCheckpoint'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'monitor'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'val/loss'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'dirpath'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${model.model_dir}/checkpoints'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'auto_insert_metric_name'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'best-epoch_{epoch:02d}-val_loss-{val/loss:.2f}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'save_top_k'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'min'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'uploadaheckpointsasartifact'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.utils.logging_utils.UploadCheckpointsAsArtifact'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ckpt_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${..checkpoint.dirpath}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'upload_best_only'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logconfusionmatrix'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.utils.logging_utils.LogConfusionMatrix'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'class_names'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.classes}'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'log_per_class_metrics_to_wandb'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'lightning_hydra_classifiers.callbacks.wandb_callbacks.LogPerClassMetricsToWandb'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'class_names'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.classes}'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'module_data_monitor'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pl_bolts.callbacks.ModuleDataMonitor'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'early_stopping'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.callbacks.early_stopping.EarlyStopping'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'monitor'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${optimized_metric}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'patience'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'verbose'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'min'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'logger'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'wandb'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.loggers.wandb.WandbLogger'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${oc.env:WANDB_ENTITY}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stage_0'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'csv'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.loggers.csv_logs.CSVLogger'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${log_dir}/logs'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'csv/'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'tensorboard'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_lightning.loggers.tensorboard.TensorBoardLogger'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${log_dir}/logs/tensorboard/'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.backbone}'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.dataset}_train-test'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'root_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${oc.env:WANDB_CACHE_DIR}'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'work_dir'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'job_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${work_dir}/${job_type}'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'log_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${job_dir}/7-15'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'results_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${model.model_dir}/results'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'optimized_metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'val/loss'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_only'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95893</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'debug'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'print_config'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'disable_warnings'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hparams_log_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${job_dir}/hparams/best_hparams.yaml'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hparams'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'classes'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'family'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'image_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${dataset.config.name}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'backbone'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'resnet50'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pretrained'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'unfreeze_init'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finetune_milestones'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'unfreeze_curriculum'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'layer4'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'layer3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'layer 2'</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'wandb'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'entity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${oc.env:WANDB_ENTITY}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'image_classification'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'job_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train_supervised'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'group'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'run_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'${root_dir}'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.dataset}'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'${hparams.backbone}'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'supervised'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'init'</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'reinit'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'trainer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.Trainer'\u001b[0m,\n",
       "        \u001b[32m'gpus'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'auto_select_gpus'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'min_epochs'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "        \u001b[32m'max_epochs'\u001b[0m: \u001b[1;36m40\u001b[0m,\n",
       "        \u001b[32m'weights_summary'\u001b[0m: \u001b[32m'top'\u001b[0m,\n",
       "        \u001b[32m'progress_bar_refresh_rate'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "        \u001b[32m'profiler'\u001b[0m: \u001b[32m'simple'\u001b[0m,\n",
       "        \u001b[32m'log_every_n_steps'\u001b[0m: \u001b[1;36m50\u001b[0m,\n",
       "        \u001b[32m'terminate_on_nan'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'fast_dev_run'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'limit_train_batches'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "        \u001b[32m'limit_val_batches'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "        \u001b[32m'accelerator'\u001b[0m: \u001b[32m'ddp'\u001b[0m,\n",
       "        \u001b[32m'amp_backend'\u001b[0m: \u001b[32m'native'\u001b[0m,\n",
       "        \u001b[32m'amp_level'\u001b[0m: \u001b[32m'02'\u001b[0m,\n",
       "        \u001b[32m'precision'\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "        \u001b[32m'benchmark'\u001b[0m: \u001b[3;92mTrue\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'lightning_hydra_classifiers.models.transfer.TransferLearningModel'\u001b[0m,\n",
       "        \u001b[32m'classifier_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'backbone_name'\u001b[0m: \u001b[32m'${hparams.backbone}'\u001b[0m,\n",
       "            \u001b[32m'num_classes'\u001b[0m: \u001b[32m'${hparams.num_classes}'\u001b[0m,\n",
       "            \u001b[32m'pretrained'\u001b[0m: \u001b[32m'${hparams.pretrained}'\u001b[0m,\n",
       "            \u001b[32m'finetune'\u001b[0m: \u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'train_bn'\u001b[0m: \u001b[32m'${callbacks.finetuning.train_bn}'\u001b[0m,\n",
       "        \u001b[32m'milestones'\u001b[0m: \u001b[32m'${callbacks.finetuning.milestones}'\u001b[0m,\n",
       "        \u001b[32m'batch_size'\u001b[0m: \u001b[32m'${hparams.batch_size}'\u001b[0m,\n",
       "        \u001b[32m'optimizer'\u001b[0m: \u001b[32m'Adam'\u001b[0m,\n",
       "        \u001b[32m'lr'\u001b[0m: \u001b[32m'${hparams.lr}'\u001b[0m,\n",
       "        \u001b[32m'lr_scheduler_gamma'\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "        \u001b[32m'model_dir'\u001b[0m: \u001b[32m'${log_dir}/model'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'datamodule'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'lightning_hydra_classifiers.data.common.LeavesLightningDataModule'\u001b[0m,\n",
       "        \u001b[32m'config'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'${.dataset.name}'\u001b[0m,\n",
       "            \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m128\u001b[0m,\n",
       "            \u001b[32m'normalize'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'image_size'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'grayscale'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'channels'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "            \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'pin_memory'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "            \u001b[32m'seed'\u001b[0m: \u001b[1;36m9877\u001b[0m,\n",
       "            \u001b[32m'augment'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'shuffle'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'num_classes'\u001b[0m: \u001b[32m'${.dataset.num_classes}'\u001b[0m,\n",
       "            \u001b[32m'predict_on_split'\u001b[0m: \u001b[32m'test'\u001b[0m,\n",
       "            \u001b[32m'debug'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "            \u001b[32m'dataset'\u001b[0m: \u001b[32m'${dataset.config}'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'dataset'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'_target_'\u001b[0m: \u001b[32m'lightning_hydra_classifiers.data.common.CommonDataset'\u001b[0m,\n",
       "        \u001b[32m'config'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'PNAS_family_100_1024'\u001b[0m,\n",
       "            \u001b[32m'val_train_split'\u001b[0m: \u001b[1;36m0.2\u001b[0m,\n",
       "            \u001b[32m'test_split'\u001b[0m: \u001b[32m'test'\u001b[0m,\n",
       "            \u001b[32m'threshold'\u001b[0m: \u001b[1;36m100\u001b[0m,\n",
       "            \u001b[32m'seed'\u001b[0m: \u001b[1;36m987485\u001b[0m,\n",
       "            \u001b[32m'class_type'\u001b[0m: \u001b[32m'family'\u001b[0m,\n",
       "            \u001b[32m'x_col'\u001b[0m: \u001b[32m'path'\u001b[0m,\n",
       "            \u001b[32m'y_col'\u001b[0m: \u001b[32m'${.class_type}'\u001b[0m,\n",
       "            \u001b[32m'id_col'\u001b[0m: \u001b[32m'catalog_number'\u001b[0m,\n",
       "            \u001b[32m'path_schema'\u001b[0m: \u001b[32m'{family}_{genus}_{species}_{catalog_number}'\u001b[0m,\n",
       "            \u001b[32m'num_classes'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'num_samples'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'dataset_dirs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'callbacks'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'finetuning'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'lightning_hydra_classifiers.models.transfer.MilestonesFinetuning'\u001b[0m,\n",
       "            \u001b[32m'milestones'\u001b[0m: \u001b[32m'${hparams.finetune_milestones}'\u001b[0m,\n",
       "            \u001b[32m'train_bn'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'checkpoint'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.callbacks.ModelCheckpoint'\u001b[0m,\n",
       "            \u001b[32m'monitor'\u001b[0m: \u001b[32m'val/loss'\u001b[0m,\n",
       "            \u001b[32m'dirpath'\u001b[0m: \u001b[32m'${model.model_dir}/checkpoints'\u001b[0m,\n",
       "            \u001b[32m'auto_insert_metric_name'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "            \u001b[32m'filename'\u001b[0m: \u001b[32m'best-epoch_{epoch:02d}-val_loss-{val/loss:.2f}'\u001b[0m,\n",
       "            \u001b[32m'save_top_k'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "            \u001b[32m'mode'\u001b[0m: \u001b[32m'min'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'uploadaheckpointsasartifact'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \n",
       "\u001b[32m'lightning_hydra_classifiers.utils.logging_utils.UploadCheckpointsAsArtifact'\u001b[0m,\n",
       "            \u001b[32m'ckpt_dir'\u001b[0m: \u001b[32m'${..checkpoint.dirpath}'\u001b[0m,\n",
       "            \u001b[32m'upload_best_only'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'logconfusionmatrix'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'lightning_hydra_classifiers.utils.logging_utils.LogConfusionMatrix'\u001b[0m,\n",
       "            \u001b[32m'class_names'\u001b[0m: \u001b[32m'${hparams.classes}'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'log_per_class_metrics_to_wandb'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \n",
       "\u001b[32m'lightning_hydra_classifiers.callbacks.wandb_callbacks.LogPerClassMetricsToWandb'\u001b[0m,\n",
       "            \u001b[32m'class_names'\u001b[0m: \u001b[32m'${hparams.classes}'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'module_data_monitor'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'pl_bolts.callbacks.ModuleDataMonitor'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'early_stopping'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.callbacks.early_stopping.EarlyStopping'\u001b[0m,\n",
       "            \u001b[32m'monitor'\u001b[0m: \u001b[32m'${optimized_metric}'\u001b[0m,\n",
       "            \u001b[32m'patience'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "            \u001b[32m'verbose'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "            \u001b[32m'mode'\u001b[0m: \u001b[32m'min'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'logger'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'wandb'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.loggers.wandb.WandbLogger'\u001b[0m,\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'${oc.env:WANDB_ENTITY}'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification'\u001b[0m,\n",
       "            \u001b[32m'job_type'\u001b[0m: \u001b[32m'train'\u001b[0m,\n",
       "            \u001b[32m'group'\u001b[0m: \u001b[32m'stage_0'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'csv'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.loggers.csv_logs.CSVLogger'\u001b[0m,\n",
       "            \u001b[32m'save_dir'\u001b[0m: \u001b[32m'${log_dir}/logs'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'csv/'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'tensorboard'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'_target_'\u001b[0m: \u001b[32m'pytorch_lightning.loggers.tensorboard.TensorBoardLogger'\u001b[0m,\n",
       "            \u001b[32m'save_dir'\u001b[0m: \u001b[32m'${log_dir}/logs/tensorboard/'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'${hparams.backbone}'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'job_type'\u001b[0m: \u001b[32m'${hparams.dataset}_train-test'\u001b[0m,\n",
       "    \u001b[32m'root_dir'\u001b[0m: \u001b[32m'${oc.env:WANDB_CACHE_DIR}'\u001b[0m,\n",
       "    \u001b[32m'work_dir'\u001b[0m: \n",
       "\u001b[32m'/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021'\u001b[0m,\n",
       "    \u001b[32m'job_dir'\u001b[0m: \u001b[32m'${work_dir}/${job_type}'\u001b[0m,\n",
       "    \u001b[32m'log_dir'\u001b[0m: \u001b[32m'${job_dir}/7-15'\u001b[0m,\n",
       "    \u001b[32m'results_dir'\u001b[0m: \u001b[32m'${model.model_dir}/results'\u001b[0m,\n",
       "    \u001b[32m'optimized_metric'\u001b[0m: \u001b[32m'val/loss'\u001b[0m,\n",
       "    \u001b[32m'test_only'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m: \u001b[1;36m95893\u001b[0m,\n",
       "    \u001b[32m'debug'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'print_config'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'disable_warnings'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'hparams_log_path'\u001b[0m: \u001b[32m'${job_dir}/hparams/best_hparams.yaml'\u001b[0m,\n",
       "    \u001b[32m'hparams'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m128\u001b[0m,\n",
       "        \u001b[32m'lr'\u001b[0m: \u001b[1;36m0.002\u001b[0m,\n",
       "        \u001b[32m'num_classes'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'classes'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'class_type'\u001b[0m: \u001b[32m'family'\u001b[0m,\n",
       "        \u001b[32m'image_size'\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "        \u001b[32m'dataset'\u001b[0m: \u001b[32m'${dataset.config.name}'\u001b[0m,\n",
       "        \u001b[32m'backbone'\u001b[0m: \u001b[32m'resnet50'\u001b[0m,\n",
       "        \u001b[32m'pretrained'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'unfreeze_init'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'finetune_milestones'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'unfreeze_curriculum'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'layer4'\u001b[0m, \u001b[32m'layer3'\u001b[0m, \u001b[32m'layer 2'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'wandb'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'init'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'entity'\u001b[0m: \u001b[32m'${oc.env:WANDB_ENTITY}'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'image_classification'\u001b[0m,\n",
       "            \u001b[32m'job_type'\u001b[0m: \u001b[32m'train_supervised'\u001b[0m,\n",
       "            \u001b[32m'group'\u001b[0m: \u001b[32m'train'\u001b[0m,\n",
       "            \u001b[32m'run_dir'\u001b[0m: \u001b[32m'${root_dir}'\u001b[0m,\n",
       "            \u001b[32m'tags'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                \u001b[32m'${hparams.dataset}'\u001b[0m,\n",
       "                \u001b[32m'${hparams.backbone}'\u001b[0m,\n",
       "                \u001b[32m'train'\u001b[0m,\n",
       "                \u001b[32m'supervised'\u001b[0m,\n",
       "                \u001b[32m'init'\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'reinit'\u001b[0m: \u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-22 09:54:41,710 lightning_hydra_classifiers.data.common INFO     [SELECT DATASET] (name=PNAS_family_100_1024, num_files=5311), \n",
      "dataset_dirs=\n",
      "    /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_1024\n",
      "INFO - [SELECT DATASET] (name=PNAS_family_100_1024, num_files=5311), \n",
      "dataset_dirs=\n",
      "    /media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_1024\n",
      "2021-07-22 09:54:43,058 lightning_hydra_classifiers.data.common INFO     [RUNNING] datamodule.setup(None)\n",
      "INFO - [RUNNING] datamodule.setup(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2124\n",
      "val 531\n",
      "train 2124\n",
      "val 531\n",
      "train 2124\n",
      "val 531\n",
      "train 2124\n",
      "val 531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-22 09:54:46,061 lightning_hydra_classifiers.data.common INFO     [SELECT DATASET] (name=Extant_family_10_1024, num_files=25496), \n",
      "dataset_dirs=\n",
      "    /media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/catalog_files/extant_family_10/1024\n",
      "INFO - [SELECT DATASET] (name=Extant_family_10_1024, num_files=25496), \n",
      "dataset_dirs=\n",
      "    /media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/catalog_files/extant_family_10/1024\n",
      "2021-07-22 09:54:50,559 lightning_hydra_classifiers.data.common INFO     [RUNNING] datamodule.setup(None)\n",
      "INFO - [RUNNING] datamodule.setup(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 11815\n",
      "val 2954\n",
      "train 11815\n",
      "val 2954\n",
      "train 11815\n",
      "val 2954\n",
      "train 11815\n",
      "val 2954\n"
     ]
    }
   ],
   "source": [
    "y_col = 'family'\n",
    "seed = 5687\n",
    "val_train_split = 0.2\n",
    "\n",
    "# pnas_name = \"PNAS_family_100_512\"\n",
    "# extant_name = \"Extant_family_10_512\"\n",
    "\n",
    "pnas_name = \"PNAS_family_100_1024\"#512\"\n",
    "extant_name = \"Extant_family_10_1024\" #512\"\n",
    "\n",
    "\n",
    "## Load primary Extant and PNAS datamodules\n",
    "pnas_cfg = initialize_config(config_dir=config_dir,\n",
    "                        overrides=[\"dataset=pnas_dataset\",\n",
    "                                  \"datamodule=standalone_datamodule\"])\n",
    "# pnas_cfg[f\"dataset.config.name\"] = pnas_name\n",
    "\n",
    "pnas_cfg.datamodule.config.dataset.name = pnas_name\n",
    "pp(OmegaConf.to_container(pnas_cfg, resolve=False))\n",
    "\n",
    "pnas_datamodule = LeavesLightningDataModule(pnas_cfg)#.datamodule.config)\n",
    "\n",
    "extant_cfg = initialize_config(config_dir=config_dir,\n",
    "                        overrides=[\"dataset=extant_dataset\",\n",
    "                                   \"datamodule=standalone_datamodule\"])\n",
    "# pnas_cfg[f\"dataset.config.name\"] = pnas_name\n",
    "\n",
    "extant_cfg.datamodule.config.dataset.name = extant_name\n",
    "extant_datamodule = LeavesLightningDataModule(extant_cfg) #.datamodule.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfdb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'config'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'config'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'return_signature'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'subset_key'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'class_type'\u001b[0m,\n",
       "    \u001b[32m'dataset_dirs'\u001b[0m,\n",
       "    \u001b[32m'id_col'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m,\n",
       "    \u001b[32m'num_samples'\u001b[0m,\n",
       "    \u001b[32m'path_schema'\u001b[0m,\n",
       "    \u001b[32m'return_signature'\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m,\n",
       "    \u001b[32m'subset_key'\u001b[0m,\n",
       "    \u001b[32m'test_split'\u001b[0m,\n",
       "    \u001b[32m'threshold'\u001b[0m,\n",
       "    \u001b[32m'val_train_split'\u001b[0m,\n",
       "    \u001b[32m'x_col'\u001b[0m,\n",
       "    \u001b[32m'y_col'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'config'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'config'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'return_signature'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'subset_key'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'class_type'\u001b[0m,\n",
       "    \u001b[32m'dataset_dirs'\u001b[0m,\n",
       "    \u001b[32m'id_col'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m,\n",
       "    \u001b[32m'num_samples'\u001b[0m,\n",
       "    \u001b[32m'path_schema'\u001b[0m,\n",
       "    \u001b[32m'return_signature'\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m,\n",
       "    \u001b[32m'subset_key'\u001b[0m,\n",
       "    \u001b[32m'test_split'\u001b[0m,\n",
       "    \u001b[32m'threshold'\u001b[0m,\n",
       "    \u001b[32m'val_train_split'\u001b[0m,\n",
       "    \u001b[32m'x_col'\u001b[0m,\n",
       "    \u001b[32m'y_col'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'config'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'config'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'return_signature'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'subset_key'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'class_type'\u001b[0m,\n",
       "    \u001b[32m'dataset_dirs'\u001b[0m,\n",
       "    \u001b[32m'id_col'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m,\n",
       "    \u001b[32m'num_samples'\u001b[0m,\n",
       "    \u001b[32m'path_schema'\u001b[0m,\n",
       "    \u001b[32m'return_signature'\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m,\n",
       "    \u001b[32m'subset_key'\u001b[0m,\n",
       "    \u001b[32m'test_split'\u001b[0m,\n",
       "    \u001b[32m'threshold'\u001b[0m,\n",
       "    \u001b[32m'val_train_split'\u001b[0m,\n",
       "    \u001b[32m'x_col'\u001b[0m,\n",
       "    \u001b[32m'y_col'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################\n",
    "\n",
    "dataset_name = f\"{extant_name}_minus_{pnas_name}\" # Extant_family_10_512_minus_PNAS_family_100_512\n",
    "test_dataset_name = f\"{extant_name}_in_{pnas_name}\"\n",
    "output_dir = f\"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/{dataset_name}\"\n",
    "\n",
    "#########################################\n",
    "\n",
    "extant_dataset = extant_datamodule.dataset\n",
    "pnas_dataset = pnas_datamodule.dataset\n",
    "extant_df = extant_dataset.samples_df\n",
    "pnas_df = pnas_dataset.samples_df\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "# extant_minus_pnas -> rows exclusive to extant dataset\n",
    "extant_minus_pnas = left_exclusive(data_df=extant_df,\n",
    "                                   other_df=pnas_df,\n",
    "                                   id_col=\"catalog_number\")\n",
    "# extant_in_pnas -> rows from extant dataset that share a catalog_number with PNAS\n",
    "extant_in_pnas = intersection(data_df=extant_df,\n",
    "                              other_df=pnas_df,\n",
    "                              id_col=\"catalog_number\",\n",
    "                              suffixes=(\"_extant\", \"_pnas\"))\n",
    "\n",
    "suffixes=(\"_extant\", \"_pnas\")\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})\n",
    "\n",
    "#########################################\n",
    "### GENERATE TRAIN VAL SPLIT INDICES\n",
    "#########################################\n",
    "\n",
    "y = extant_minus_pnas[y_col]\n",
    "data_splits = trainval_split(x=None,\n",
    "                             y=y,\n",
    "                             val_train_split=val_train_split,\n",
    "                             random_state=seed,\n",
    "                             stratify=True\n",
    "                             )\n",
    "train_idx, val_idx = data_splits['train'][0], data_splits['val'][0]\n",
    "\n",
    "#########################################\n",
    "### CREATE COMMONDATASETS FROM DATAFRAMES, USING THE SPLIT INDICES\n",
    "#########################################\n",
    "\n",
    "\n",
    "train_df = extant_minus_pnas.iloc[train_idx,:]\n",
    "val_df = extant_minus_pnas.iloc[val_idx,:]\n",
    "\n",
    "train_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\n",
    "                                                               sample_df=train_df,\n",
    "                                                               config=None,\n",
    "                                                               return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                               subset_key=\"train\")\n",
    "\n",
    "val_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\n",
    "                                                             sample_df=val_df,\n",
    "                                                             config=None,\n",
    "                                                             return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                             subset_key=\"val\")\n",
    "\n",
    "test_dataset_extant_in_pnas = CommonDataset.from_dataframe(sample_df=extant_in_pnas,\n",
    "                                                            config=None,\n",
    "                                                            return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                            subset_key=\"test\")\n",
    "\n",
    "data_splits= {'train':train_dataset_extant_minus_pnas,\n",
    "              'val': val_dataset_extant_minus_pnas,\n",
    "              'test':test_dataset_extant_in_pnas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34087e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits['train'].config.name = dataset_name\n",
    "data_splits['train'].config.subset_key = \"train\"\n",
    "\n",
    "data_splits['val'].config.name = dataset_name\n",
    "data_splits['val'].config.subset_key = \"val\"\n",
    "\n",
    "data_splits['test'].config.name = test_dataset_name\n",
    "data_splits['test'].config.subset_key = \"test\"\n",
    "\n",
    "#########################################\n",
    "### LABEL ENCODER\n",
    "#########################################\n",
    "\n",
    "replace = {\"Nothofagaceae\": \"Fagaceae\"}\n",
    "label_encoder = LabelEncoder(replace=replace) # class2idx)\n",
    "label_encoder.fit(data_splits[\"test\"].targets)\n",
    "label_encoder.fit(data_splits[\"train\"].targets)\n",
    "for d in list(data_splits.values()):\n",
    "    d.label_encoder = label_encoder\n",
    "\n",
    "    \n",
    "#########################################\n",
    "### EXPORT DATASET CONFIGURATION TO A COMBO OF CSV, JSON, YAML, AND JPG FILES.\n",
    "#########################################\n",
    "from lightning_hydra_classifiers.data.common import export_dataset_to_csv, import_dataset_from_csv\n",
    "# output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_512_minus_PNAS_family_100_512\"\n",
    "export_dataset_to_csv(data_splits=data_splits,\n",
    "                          label_encoder=label_encoder,\n",
    "                          output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe380e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-22 17:43:22,848 lightning_hydra_classifiers.utils.common_utils INFO     LabelEncoder replacing 1 class encodings with that other an another class\n",
      "INFO - LabelEncoder replacing 1 class encodings with that other an another class\n",
      "2021-07-22 17:43:22,851 lightning_hydra_classifiers.utils.common_utils INFO     Replacing: {'Nothofagaceae': 'Fagaceae'}\n",
      "INFO - Replacing: {'Nothofagaceae': 'Fagaceae'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'config'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'config'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'return_signature'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'subset_key'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'class_type'\u001b[0m,\n",
       "    \u001b[32m'dataset_dirs'\u001b[0m,\n",
       "    \u001b[32m'id_col'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m,\n",
       "    \u001b[32m'num_samples'\u001b[0m,\n",
       "    \u001b[32m'path_schema'\u001b[0m,\n",
       "    \u001b[32m'return_signature'\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m,\n",
       "    \u001b[32m'subset_key'\u001b[0m,\n",
       "    \u001b[32m'test_split'\u001b[0m,\n",
       "    \u001b[32m'threshold'\u001b[0m,\n",
       "    \u001b[32m'val_train_split'\u001b[0m,\n",
       "    \u001b[32m'x_col'\u001b[0m,\n",
       "    \u001b[32m'y_col'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'config'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'config'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'return_signature'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'subset_key'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'class_type'\u001b[0m,\n",
       "    \u001b[32m'dataset_dirs'\u001b[0m,\n",
       "    \u001b[32m'id_col'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m,\n",
       "    \u001b[32m'num_samples'\u001b[0m,\n",
       "    \u001b[32m'path_schema'\u001b[0m,\n",
       "    \u001b[32m'return_signature'\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m,\n",
       "    \u001b[32m'subset_key'\u001b[0m,\n",
       "    \u001b[32m'test_split'\u001b[0m,\n",
       "    \u001b[32m'threshold'\u001b[0m,\n",
       "    \u001b[32m'val_train_split'\u001b[0m,\n",
       "    \u001b[32m'x_col'\u001b[0m,\n",
       "    \u001b[32m'y_col'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'config'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'config'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'class_type'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_dirs'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_classes'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_samples'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'path_schema'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'return_signature'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'subset_key'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'val_train_split'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x_col'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_col'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'class_type'\u001b[0m,\n",
       "    \u001b[32m'dataset_dirs'\u001b[0m,\n",
       "    \u001b[32m'id_col'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m,\n",
       "    \u001b[32m'num_classes'\u001b[0m,\n",
       "    \u001b[32m'num_samples'\u001b[0m,\n",
       "    \u001b[32m'path_schema'\u001b[0m,\n",
       "    \u001b[32m'return_signature'\u001b[0m,\n",
       "    \u001b[32m'seed'\u001b[0m,\n",
       "    \u001b[32m'subset_key'\u001b[0m,\n",
       "    \u001b[32m'test_split'\u001b[0m,\n",
       "    \u001b[32m'threshold'\u001b[0m,\n",
       "    \u001b[32m'val_train_split'\u001b[0m,\n",
       "    \u001b[32m'x_col'\u001b[0m,\n",
       "    \u001b[32m'y_col'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConfigAttributeError",
     "evalue": "Missing key dataset\n    full_key: dataset\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-eef48ad43a28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#                             })\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# config.dataset.config.name = \"Extant_family_10_1024_in_PNAS_family_100_1024\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m datamodule = LeavesLightningDataModule(config=None, #config, #default_config,\n\u001b[0m\u001b[1;32m     14\u001b[0m                                        data_dir=output_dir)\n\u001b[1;32m     15\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, data_dir)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetConstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_DEFAULT_MARKER_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mConfigKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             self._format_and_raise(\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfigAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             )\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/omegaconf/base.py\u001b[0m in \u001b[0;36m_format_and_raise\u001b[0;34m(self, key, value, cause, type_override)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_override\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     ) -> None:\n\u001b[0;32m--> 189\u001b[0;31m         format_and_raise(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/omegaconf/_utils.py\u001b[0m in \u001b[0;36mformat_and_raise\u001b[0;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_type_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref_type_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0m_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/omegaconf/_utils.py\u001b[0m in \u001b[0;36m_raise\u001b[0;34m(ex, cause)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# set end OC_CAUSE=1 for full backtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_DEFAULT_MARKER_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mConfigKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             self._format_and_raise(\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m_get_impl\u001b[0;34m(self, key, default_value)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDictKeyType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthrow_on_missing_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConfigAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdefault_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_DEFAULT_MARKER_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jrose3/envs/sequoia/lib/python3.8/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m_get_node\u001b[0;34m(self, key, validate_access, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mthrow_on_missing_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConfigKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing key {key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mthrow_on_missing_value\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingMandatoryValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing mandatory value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigAttributeError\u001b[0m: Missing key dataset\n    full_key: dataset\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "# loaded_data_splits, conf = import_dataset_from_csv(data_catalog_dir = output_dir)\n",
    "# print(conf)\n",
    "# for k,v in loaded_data_splits.items():\n",
    "#     print(k, repr(v))\n",
    "\n",
    "\n",
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_1024_minus_PNAS_family_100_1024\"\n",
    "\n",
    "#         config = DictConfig({\"dataset\":\n",
    "#                                        {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\n",
    "#                             })\n",
    "# config.dataset.config.name = \"Extant_family_10_1024_in_PNAS_family_100_1024\"\n",
    "datamodule = LeavesLightningDataModule(config=None, #config, #default_config,\n",
    "                                       data_dir=output_dir)\n",
    "config.hparams.classes = datamodule.classes\n",
    "config.hparams.num_classes = len(config.hparams.classes)\n",
    "config.dataset.config.classes = datamodule.classes\n",
    "config.dataset.config.num_classes = len(config.hparams.classes)\n",
    "\n",
    "\n",
    "data_loader = datamodule.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0fc33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5141d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_1024_minus_PNAS_family_100_1024\"\n",
    "\n",
    "config = DictConfig({\"dataset\":\n",
    "                               {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\n",
    "                    })\n",
    "datamodule = LeavesLightningDataModule(config=config, #default_config,\n",
    "                                       data_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034b1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def left_exclusive(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\") -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Return a new dataframe containing only rows from `data_df` that do not share an `id_col` value with any row from `other_df`.\n",
    "    \n",
    "#     Equivalent to subtracting the set of `id_col` values in `other_df` from `data_df`\n",
    "#     \"\"\"\n",
    "#     omit = list(other_df[id_col].values)\n",
    "    \n",
    "#     return data_df[data_df[id_col].apply(lambda x: x not in omit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a601eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnas_cfg = initialize_config(config_dir=config_dir,\n",
    "                        overrides=[\"dataset=pnas_dataset\"])\n",
    "pnas_data = LeavesLightningDataModule(pnas_cfg.datamodule.config)\n",
    "\n",
    "extant_cfg = initialize_config(config_dir=config_dir,\n",
    "                        overrides=[\"dataset=extant_dataset\"])\n",
    "extant_data = LeavesLightningDataModule(extant_cfg.datamodule.config)\n",
    "\n",
    "extant_dataset = extant_data.dataset\n",
    "pnas_dataset = pnas_data.dataset\n",
    "\n",
    "print(len(extant_dataset), len(pnas_dataset))\n",
    "\n",
    "extant_df = extant_dataset.samples_df\n",
    "pnas_df = pnas_dataset.samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8096f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extant_minus_pnas -> rows exclusive to extant dataset\n",
    "extant_minus_pnas = left_exclusive(data_df=extant_df,\n",
    "                                   other_df=pnas_df,\n",
    "                                   id_col=\"catalog_number\")\n",
    "\n",
    "# pnas_minus_extant -> rows exclusive to pnas dataset\n",
    "# pnas_minus_extant = left_exclusive(data_df=pnas_df,\n",
    "#                                    other_df=extant_df,\n",
    "#                                    id_col=\"catalog_number\")\n",
    "\n",
    "\n",
    "# print(extant_minus_pnas.shape, pnas_minus_extant.shape)\n",
    "\n",
    "# pnas_in_extant = intersection(data_df=pnas_df,\n",
    "#                               other_df=extant_df,\n",
    "#                               id_col=\"catalog_number\",\n",
    "#                               suffixes=(\"_pnas\", \"_extant\"))\n",
    "\n",
    "extant_in_pnas = intersection(data_df=extant_df,\n",
    "                              other_df=pnas_df,\n",
    "                              id_col=\"catalog_number\",\n",
    "                              suffixes=(\"_extant\", \"_pnas\"))\n",
    "\n",
    "\n",
    "# print(extant_in_pnas.shape, pnas_in_extant.shape)\n",
    "\n",
    "suffixes=(\"_extant\", \"_pnas\")\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})\n",
    "\n",
    "extant_in_pnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'family'\n",
    "seed = 5687\n",
    "val_train_split = 0.2\n",
    "\n",
    "y = extant_minus_pnas[y_col]\n",
    "\n",
    "data_splits = trainval_split(x=None,\n",
    "                             y=y,\n",
    "                               val_train_split=val_train_split,\n",
    "                               random_state=seed,\n",
    "                               stratify=True\n",
    "                               )\n",
    "\n",
    "data_splits['train'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = data_splits['train'][0], data_splits['val'][0]\n",
    "\n",
    "train_df = extant_minus_pnas.iloc[train_idx,:]\n",
    "val_df = extant_minus_pnas.iloc[val_idx,:]\n",
    "\n",
    "train_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\n",
    "                                                               sample_df=train_df,\n",
    "                                                               config=None,\n",
    "                                                               return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                               subset_key=\"train\")\n",
    "#                                                                subset_key=None) #\"train\")\n",
    "\n",
    "\n",
    "val_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\n",
    "                                                             sample_df=val_df,\n",
    "                                                             config=None,\n",
    "                                                             return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                             subset_key=\"val\")\n",
    "\n",
    "\n",
    "test_dataset_extant_in_pnas = CommonDataset.from_dataframe(sample_df=extant_in_pnas,\n",
    "                                                            config=None,\n",
    "                                                            return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                            subset_key=\"test\")\n",
    "\n",
    "data_splits= {'train':train_dataset_extant_minus_pnas,\n",
    "              'val': val_dataset_extant_minus_pnas,\n",
    "              'test':test_dataset_extant_in_pnas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f65a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in data_splits.items():    \n",
    "#     print(k, v.__repr__())\n",
    "\n",
    "# import_dataset_from_csv(self, data_dir: str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527bc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455885e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ffbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OmegaConf.to_container(test_dataset_extant_in_pnas.config, resolve=True)\n",
    "# label_encoder = LabelEncoder() # class2idx)\n",
    "# label_encoder.fit(data_splits[\"train\"].targets)\n",
    "\n",
    "# for d in list(data_splits.values()):\n",
    "#     d.label_encoder = label_encoder\n",
    "# test_dataset_extant_in_pnas.label_encoder\n",
    "# label_encoder = LabelEncoder() # class2idx)\n",
    "# label_encoder.fit(data_splits[\"test\"].targets)\n",
    "\n",
    "\n",
    "\n",
    "# test_dataset_extant_in_pnas.label_encoder\n",
    "# label_encoder\n",
    "\n",
    "# test_df = data_splits[\"test\"].samples_df\n",
    "\n",
    "# replace = {\"Nothofagaceae\": \"Fagaceae\"}\n",
    "# label_encoder = LabelEncoder(replace=replace) # class2idx)\n",
    "# label_encoder.fit(data_splits[\"test\"].targets)\n",
    "\n",
    "# label_encoder.fit(data_splits[\"train\"].targets)\n",
    "\n",
    "\n",
    "\n",
    "# for d in list(data_splits.values()):\n",
    "#     d.label_encoder = label_encoder\n",
    "    \n",
    "# label_encoder\n",
    "# test_df[test_df.family==\"Nothofagaceae\"].replace(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c758d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import *\n",
    "# from omegaconf import DictConfig\n",
    "# from pathlib import Path\n",
    "# import matplotlib.pyplot as plt\n",
    "# pnas_df[pnas_df.catalog_number==\"Wolfe_8535\"]\n",
    "\n",
    "# set(data_splits[\"test\"].samples_df.family.astype(pd.CategoricalDtype()).cat.categories) - set(pnas_df.family.astype(pd.CategoricalDtype()).cat.categories)\n",
    "\n",
    "# test_dataset_extant_in_pnas.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b03fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(config: DictConfig, config_path: str):\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(OmegaConf.to_yaml(config, resolve=True))\n",
    "\n",
    "def load_config(config_path: str) -> DictConfig:    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        loaded = OmegaConf.load(f)\n",
    "    return loaded\n",
    "\n",
    "\n",
    "def export_image_data_diagnostics(data_splits: Dict[str,CommonDataset],\n",
    "                                  output_dir: str='.',\n",
    "                                  max_samples: int = 64,\n",
    "                                  export_sample_images: bool=True,\n",
    "                                  export_class_distribution_plots: bool=True) -> Dict[str,str]:\n",
    "    image_paths = {\"images\": {},\n",
    "                   \"class_distribution_plots\":{}}\n",
    "    \n",
    "    image_dir = os.path.join(output_dir, \"images\")\n",
    "    plot_dir = os.path.join(output_dir, \"plots\")\n",
    "    os.makedirs(image_dir, exist_ok = True)\n",
    "    os.makedirs(plot_dir, exist_ok = True)\n",
    "\n",
    "    if export_sample_images:\n",
    "#         subsets = ['train', 'val', 'test']\n",
    "        for subset in data_splits.keys():\n",
    "            fig, ax = data_splits[subset].show_batch(indices=max_samples, include_colorbar=False,\n",
    "                                                     suptitle = f\"subset: {subset}, {max_samples} random images\")\n",
    "            img_path = os.path.join(image_dir, f\"subset: {subset}, {max_samples} random images.jpg\")\n",
    "            image_paths[\"images\"][subset] = img_path\n",
    "            plt.savefig(img_path)\n",
    "\n",
    "    if export_class_distribution_plots:\n",
    "        fig, ax = plot_split_distributions(data_splits=data_splits)\n",
    "        class_distribution_plot_path = os.path.join(plot_dir, f\"class_distribution_plots_{[subset for subset in data_splits.keys()]}\")\n",
    "        image_paths[\"class_distribution_plots\"][\"all\"] = class_distribution_plot_path\n",
    "        plt.savefig(class_distribution_plot_path)\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "\n",
    "def export_dataset_to_csv(data_splits: Dict[str,CommonDataset],\n",
    "                          label_encoder: Optional[LabelEncoder]=None,\n",
    "                          output_dir: str='.',\n",
    "                          export_sample_images: bool=True,\n",
    "                          export_class_distribution_plots: bool=True) -> Dict[str,str]:\n",
    "    output_paths = {\"tables\":{},\n",
    "                    \"class_labels\":{},\n",
    "                    \"configs\":{}}\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for k, data in data_splits.items():\n",
    "        subset_data_path = os.path.join(output_dir, f\"{k}_data_table.csv\")\n",
    "        data.samples_df.to_csv(subset_data_path)\n",
    "        output_paths[\"tables\"][k] = subset_data_path\n",
    "        \n",
    "        if hasattr(data, \"config\"):\n",
    "            subset_config_path = os.path.join(output_dir, f\"{k}_config.yaml\")\n",
    "            save_config(config=data.config, config_path=subset_config_path)\n",
    "            output_paths[\"configs\"][k] = subset_config_path\n",
    "        \n",
    "        if hasattr(data, 'label_encoder') and (label_encoder is None):\n",
    "            subset_label_path = os.path.join(output_dir, k + \"_label_encoder.json\")\n",
    "            data.label_encoder.save(subset_label_path)\n",
    "            output_paths[\"class_labels\"][k] = subset_data_path\n",
    "            \n",
    "    if label_encoder is not None:\n",
    "        full_label_encoder_path = os.path.join(output_dir, \"label_encoder.json\")\n",
    "        label_encoder.save(full_label_encoder_path)\n",
    "        output_paths[\"class_labels\"][\"full\"] = full_label_encoder_path\n",
    "\n",
    "    \n",
    "    export_image_data_diagnostics(data_splits=data_splits,\n",
    "                                  output_dir=output_dir,\n",
    "                                  max_samples = 64,\n",
    "                                  export_sample_images=export_sample_images,\n",
    "                                  export_class_distribution_plots=export_class_distribution_plots)\n",
    "        \n",
    "    return output_paths\n",
    "    \n",
    "\n",
    "def import_dataset_from_csv(data_catalog_dir: str) -> Dict[str, CommonDataset]:\n",
    "    \n",
    "    data_paths = list(Path(data_catalog_dir).glob(\"*.csv\"))\n",
    "    config_paths = list(Path(data_catalog_dir).glob(\"*.yaml\"))\n",
    "    label_encoder_paths = list(Path(data_catalog_dir).glob(\"*.json\"))\n",
    "    \n",
    "    assert len(data_paths) == len(config_paths)\n",
    "    input_paths = {\"tables\":{},\n",
    "                   \"class_labels\":{},\n",
    "                   \"configs\":{}}\n",
    "    subsets = [\"train\", \"val\", \"test\"]\n",
    "    for subset in subsets:\n",
    "        input_paths[\"tables\"][subset] = [p for p in data_paths if p.stem.startswith(subset)][0]\n",
    "        input_paths[\"configs\"][subset] = [p for p in config_paths if p.stem.startswith(subset)][0]\n",
    "    \n",
    "    if len(label_encoder_paths) == 1:\n",
    "        label_encoder = LabelEncoder.load(label_encoder_paths[0])\n",
    "    else:\n",
    "        raise(f'Currently cannot distinguish between multiple label_encoders, please delete all but 1 in experiment directory. Contents: {label_encoder_paths}')\n",
    "    \n",
    "    data_splits = {}\n",
    "    for subset in subsets:\n",
    "        sample_df = pd.read_csv(input_paths[\"tables\"][subset])\n",
    "        config = load_config(input_paths[\"configs\"][subset])\n",
    "        data_splits[subset] = CommonDataset.from_dataframe(sample_df,\n",
    "                                                           config=config)\n",
    "        data_splits[subset].label_encoder = label_encoder\n",
    "        \n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f89899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_d = data_splits['train']\n",
    "\n",
    "\n",
    "# save_config(config=config, config_path=subset_config_path)\n",
    "# loaded = load_config(config_path=subset_config_path)\n",
    "\n",
    "# pp(OmegaConf.to_container(config, resolve=True))\n",
    "\n",
    "# pp(OmegaConf.to_container(loaded))\n",
    "\n",
    "# pp(data_paths, config_paths, label_encoder_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a31c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = {\"Nothofagaceae\": \"Fagaceae\"}\n",
    "label_encoder = LabelEncoder(replace=replace) # class2idx)\n",
    "label_encoder.fit(data_splits[\"test\"].targets)\n",
    "label_encoder.fit(data_splits[\"train\"].targets)\n",
    "for d in list(data_splits.values()):\n",
    "    d.label_encoder = label_encoder\n",
    "    \n",
    "label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_512_minus_PNAS_family_100_512\"\n",
    "\n",
    "export_dataset_to_csv(data_splits=data_splits,\n",
    "                          label_encoder=label_encoder,\n",
    "                          output_dir=output_dir)\n",
    "\n",
    "loaded_data_splits = import_dataset_from_csv(data_catalog_dir = output_dir)\n",
    "\n",
    "for k,v in loaded_data_splits.items():\n",
    "    print(k, repr(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd45bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd7609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c807a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.available\n",
    "\n",
    "# plt.style.use(\"seaborn-notebook\")\n",
    "plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b193b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self = data_splits['train']\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# # indices = [0,1, 2,3,4,5]\n",
    "# indices = np.array(indices)\n",
    "# batch = [self[idx] for idx in indices]\n",
    "\n",
    "# # y = [batch[idx][1] for idx in indices]\n",
    "# y = torch.Tensor(np.array([(item[1]) for item in batch])).to(int)\n",
    "# y\n",
    "\n",
    "# batch = (torch.stack([item[0] for item in batch]),\n",
    "#          torch.stack([torch.Tensor(item[1]) for item in batch]))\n",
    "# return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4354976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_dir = os.path.join(output_dir, \"images\")\n",
    "plot_dir = os.path.join(output_dir, \"plots\")\n",
    "os.makedirs(image_dir, exist_ok = True)\n",
    "os.makedirs(plot_dir, exist_ok = True)\n",
    "\n",
    "\n",
    "image_paths = {\"images\": {},\n",
    "               \"class_distribution_plots\":{}}\n",
    "\n",
    "max_samples = 64\n",
    "subsets = ['train', 'val', 'test']\n",
    "for subset in subsets:    \n",
    "    fig, ax = data_splits[subset].show_batch(indices=max_samples, include_colorbar=False,\n",
    "                                             suptitle = f\"subset: {subset}, {max_samples} random images\")\n",
    "    img_path = os.path.join(image_dir, f\"subset: {subset}, {max_samples} random images.jpg\")\n",
    "    image_paths[\"images\"][subset] = img_path\n",
    "    plt.savefig(img_path)\n",
    "\n",
    "fig, ax = plot_split_distributions(data_splits=data_splits)\n",
    "class_distribution_plot_path = os.path.join(plot_dir, f\"class_distribution_plots_{[subset for subset in data_splits.keys()]}\")\n",
    "image_paths[\"class_distribution_plots\"][\"all\"] = class_distribution_plot_path\n",
    "\n",
    "plt.savefig(class_distribution_plot_path)\n",
    "\n",
    "\n",
    "# dir(ax)\n",
    "# cb=plt.colorbar()\n",
    "# cb.remove()\n",
    "# plt.draw()\n",
    "\n",
    "# plt.subplots_adjust(left=None, bottom=0.0, right=None, top=0.95, wspace=None, hspace=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0523e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159491b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea08409",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.display_grid(indices=64,\n",
    "               label_font_size=\"medium\")\n",
    "plt.suptitle(k, fontsize=\"large\")\n",
    "rows = 5\n",
    "plt.subplots_adjust(left=None, bottom=0.0, right=None, top=0.9, wspace=None, hspace=0.05*rows) #wspace=0.05, hspace=0.1)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.stack\n",
    "\n",
    "set(pnas_df.family) #- set(label_encoder.classes[:20])\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns_context = \"talk\"\n",
    "sns_style = \"seaborn-bright\"\n",
    "sns.set_context(context=sns_context, font_scale=0.8)\n",
    "\n",
    "sns.set_palette(\"Accent\")\n",
    "# valid contexts = paper, notebook, talk, poster - \n",
    "# with notebook being 1:1 and paper being smaller and poster being largest\n",
    "# sns.set_style('darkgrid')\n",
    "# sns.set_palette('Set2')\n",
    "\n",
    "# plt.style.use(sns_style)\n",
    "fig, ax = plot_split_distributions(data_splits= {'train':train_dataset_extant_minus_pnas,\n",
    "                                                 'val': val_dataset_extant_minus_pnas,\n",
    "                                                 'test':test_dataset_extant_in_pnas})\n",
    "\n",
    "# for label in ax[1].xaxis.get_ticklabels()[::2]:\n",
    "#     label.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "classes = list(set(list(train_df['family'])))#[:100]\n",
    "num_classes = len(classes)\n",
    "df = pd.DataFrame(np.random.random((num_classes,num_classes)), columns=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!where latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('dark_background')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(df.values, cmap=\"BrBG\")\n",
    "\n",
    "\n",
    "label_format = '{:,.0f}'\n",
    "\n",
    "# nothing done to ax1 as it is a \"control chart.\"\n",
    "ax = plt.gca()\n",
    "\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "# fixing yticks with \"set_yticks\"\n",
    "# ticks_loc = ax.get_yticks().tolist()\n",
    "# ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\n",
    "\n",
    "# # fixing yticks with matplotlib.ticker \"FixedLocator\"\n",
    "# ticks_loc = ax3.get_yticks().tolist()\n",
    "# ax3.yaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\n",
    "# ax3.set_yticklabels([label_format.format(x) for x in ticks_loc])\n",
    "\n",
    "# # fixing xticks with FixedLocator but also using MaxNLocator to avoid cramped x-labels\n",
    "# ax.xaxis.set_major_locator(mticker.MaxNLocator(75))\n",
    "# ticks_loc = ax.get_xticks().tolist()\n",
    "# ax.xaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\n",
    "# ax.set_xticklabels([label_format.format(x) for x in ticks_loc])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax = plt.gca()\n",
    "\n",
    "# ax.set_xticklabels(classes)\n",
    "# plt.xticks(\n",
    "# rotation=90, #45, \n",
    "# horizontalalignment='right',\n",
    "# fontweight='light',\n",
    "# fontsize='small'\n",
    "# )\n",
    "\n",
    "\n",
    "# plot a heatmap with annotation\n",
    "# sns.heatmap(df, annot=True, annot_kws={\"size\": 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f79b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pnas_df.shape[0] == 5311, \"Expected full PNAS_family_100 dataset to have 5311 samples\"\n",
    "\n",
    "assert pnas_minus_extant.shape[0] == 2518\n",
    "assert pnas_in_extant.shape[0] == 2793\n",
    "\n",
    "assert pnas_in_extant.shape[0] == extant_in_pnas.shape[0]\n",
    "\n",
    "assert pnas_in_extant.merge(pnas_minus_extant, on=\"catalog_number\", how=\"inner\").shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnas_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c464bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(24,12))\n",
    "extant_in_pnas.iloc[:1,:].apply(lambda x: [ax[0].imshow(Image.open(x.path_extant).convert(\"L\"), cmap=\"gray\"), ax[1].imshow(Image.open(x.path_pnas).convert(\"L\"), cmap=\"gray\")], axis=1)\n",
    "ax[0].set_title(\"Extant (No-Crop)\")\n",
    "ax[1].set_title(\"PNAS (Cropped)\")\n",
    "plt.suptitle(Path(extant_in_pnas.iloc[0,:].path_extant).stem)\n",
    "# extant_in_pnas.iloc[:1,:].apply(lambda x: print(type(x)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "2793+22704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to only keep original columns\n",
    "suffixes=(\"_extant\", \"_pnas\")\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8896026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50400ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff14b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = extant_df\n",
    "# other_df = pnas_df\n",
    "# id_col = \"catalog_number\"\n",
    "\n",
    "\n",
    "data_df.sort_values(\"catalog_number\")\n",
    "intersected = data_df.merge(other_df, on=id_col, how='inner').sort_values(id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efaeb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df.sort_values(\"catalog_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extant_minus_pnas\n",
    "\n",
    "pnas_minus_extant\n",
    "pnas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(self.samples)\n",
    "data_df = data_df.convert_dtypes()\n",
    "\n",
    "other_df = pd.DataFrame(other.samples)\n",
    "other_df = other_df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4856080",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb306ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62041763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CommonDataArithmetic(CommonDataset):\n",
    "    \n",
    "    \n",
    "#     @property\n",
    "#     def samples_df(self):        \n",
    "#         data_df = pd.DataFrame(self.samples)\n",
    "#         data_df = data_df.convert_dtypes()\n",
    "#         return data_df\n",
    "\n",
    "    \n",
    "# other_df = pd.DataFrame(other.samples)\n",
    "# other_df = other_df.convert_dtypes()\n",
    "    \n",
    "    \n",
    "#     def intersection(self, other):\n",
    "#         samples_df = self.samples_df\n",
    "#         other_df = other.samples_df\n",
    "        \n",
    "#         intersection = data_df.merge(other_df, how='inner', on=self.id_col)\n",
    "#         return intersection\n",
    "\n",
    "#     def __sub__(self, other)\n",
    "    \n",
    "#         intersection = self.intersection(other)\n",
    "#         samples_df = self.samples_df\n",
    "        \n",
    "#         remainder = samples_df[samples_df[self.id_col].apply(lambda x: x not in intersection[self.id_col])]\n",
    "        \n",
    "#         return remainder\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "data_df = pd.DataFrame(self.samples)\n",
    "data_df = data_df.convert_dtypes()\n",
    "\n",
    "other_df = pd.DataFrame(other.samples)\n",
    "other_df = other_df.convert_dtypes()\n",
    "        \n",
    "        \n",
    "#         init_params = self.init_params\n",
    "#         init_params[\"files\"] = data_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc509010",
   "metadata": {},
   "outputs": [],
   "source": [
    "int.__sub__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(OmegaConf.to_container(self.config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_data = data_df.merge(other_df, how='inner', on=self.id_col)\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d83b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data = extant_train + pnas_train\n",
    "concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ea68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(concat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da77a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68645dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(context='talk', font_scale=0.8)\n",
    "# sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81869540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_counts(targets: Sequence,\n",
    "                         sort_by: Optional[Union[str, bool, Sequence]]=\"count\"\n",
    "                        ) -> Dict[str, int]:\n",
    "    \n",
    "    counts = collections.Counter(targets)\n",
    "    if isinstance(sort_by, list):\n",
    "        counts = {k: counts[k] for k in sort_by}\n",
    "    elif (sort_by == \"count\"):\n",
    "        counts = dict(sorted(counts.items(), key = lambda x:x[1], reverse=True))\n",
    "    elif (sort_by is True):\n",
    "        counts = dict(sorted(counts.items(), key = lambda x:x[0], reverse=True))\n",
    "        \n",
    "    return counts\n",
    "\n",
    "def plot_class_distributions(targets: List[Any], \n",
    "                             sort_by: Optional[Union[str, bool, Sequence]]=\"count\",\n",
    "                             ax=None,\n",
    "                             xticklabels: bool=True):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "        counts = plot_class_distributions(targets=data.targets, sort=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = compute_class_counts(targets,\n",
    "                                  sort_by=sort_by)\n",
    "                        \n",
    "    keys = list(counts.keys())\n",
    "    values = list(counts.values())\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(16,12))\n",
    "    ax = sns.histplot(x=keys, weights=values, discrete=True, ax=ax)\n",
    "    plt.sca(ax)\n",
    "    if xticklabels:\n",
    "        plt.xticks(\n",
    "            rotation=45, \n",
    "            horizontalalignment='right',\n",
    "            fontweight='light',\n",
    "            fontsize='medium'\n",
    "        )\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "    \n",
    "    return counts\n",
    "\n",
    "\n",
    "def plot_split_distributions(data_splits: Dict[str, CommonDataset]):\n",
    "    \"\"\"\n",
    "    Create 3 vertically-stacked count plots of train, val, and test dataset class label distributions\n",
    "    \"\"\"\n",
    "    assert isinstance(data_splits, dict)\n",
    "    num_splits = len(data_splits)\n",
    "    \n",
    "    if num_splits < 4:\n",
    "        rows = num_splits\n",
    "        cols = 1\n",
    "    else:\n",
    "        rows = int(num_splits // 2)\n",
    "        cols = int(num_splits % 2)\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(16*cols,8*rows))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    \n",
    "    train_key = [k for k,v in data_splits.items() if \"train\" in k]\n",
    "    if len(train_key)==1:\n",
    "        train_counts = compute_class_counts(data_splits[train_key[0]].targets,\n",
    "                                            sort_by=\"count\")\n",
    "    xticklabels=False\n",
    "    num_samples = 0\n",
    "    counts = {}\n",
    "    for i, (k, v) in enumerate(data_splits.items()):\n",
    "        if i == len(data_splits)-1:\n",
    "            xticklabels=True\n",
    "        counts[k] = plot_class_distributions(targets=v.targets, \n",
    "                                             sort_by=train_counts,\n",
    "                                             ax = ax[i],\n",
    "                                             xticklabels=xticklabels)\n",
    "        plt.gca().set_title(f\"{k} (n={len(v)})\", fontsize='large')\n",
    "        \n",
    "        num_samples += len(v)\n",
    "    \n",
    "    plt.suptitle('-'.join(list(data_splits.keys())) + f\"_splits (total={num_samples})\", fontsize='x-large')\n",
    "    plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.07)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba83fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.data.common import plot_split_distributions, compute_class_counts\n",
    "\n",
    "\n",
    "data_splits = {\"train\": data.train_dataset,\n",
    "               \"val\": data.val_dataset,\n",
    "               \"test\": data.test_dataset}\n",
    "\n",
    "# plot_split_distributions(data_splits=data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in list(data_splits.keys()):\n",
    "#     data_splits[k] = pd.DataFrame([data_splits[k]]).assign(split = k)\n",
    "    \n",
    "# target_splits = pd.concat(list(data_splits.values()))\n",
    "# target_splits.reset_index().describe(include='all')\n",
    "import numpy as np    \n",
    "\n",
    "\n",
    "# y_col = \"target\"\n",
    "y_col = \"family\"\n",
    "target_splits = pd.concat([pd.DataFrame(v.targets).assign(split = k) for k, v in data_splits.items()]).rename(columns={0:y_col})\n",
    "target_splits.reset_index().describe(include='all')\n",
    "\n",
    "# pd.DataFrame(target_splits.groupby(\"family\"))\n",
    "\n",
    "# pd.DataFrame(target_splits.groupby(\"split\"))\n",
    "\n",
    "pd.DataFrame(target_splits.groupby(\"split\")[\"family\"])#.agg([len]))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(data=target_splits,\n",
    "              x=\"family\",\n",
    "              hue=\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e69900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import dataclasses\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from rich import print as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\n",
    "xticklabels = True\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0857de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT_TYPES = DictConfig({\n",
    "#     \"unstacked_grouped_countplot\": {\"multiple\":\"dodge\",\n",
    "#                                     \"stat\":\"count\", \"kde\":True, \"shrink\":0.95, \"binwidth\":1.5},\n",
    "#     \"stacked_filled_grouped_histplot\": {\"multiple\":\"fill\",\n",
    "#                                         \"stat\":\"probability\", \"shrink\":0.95, \"binwidth\":0.6},\n",
    "#     \"stacked_grouped_countplot\": {\"multiple\":\"stack\",\n",
    "#                                   \"stat\":\"count\", \"shrink\":0.9, \"binwidth\":1.5}\n",
    "#     })\n",
    "        \n",
    "# kwargs = PLOT_TYPES\n",
    "# pp(dict(kwargs))\n",
    "# pp(OmegaConf.to_container(cfg, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad87183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def plot_grouped_class_distributions(data: pd.DataFrame,\n",
    "                                     x_col: str=\"family\",\n",
    "                                     group_col: Optional[str]=None,\n",
    "                                     suptitle: Optional[str]=None,\n",
    "                                     savefig: Optional[str]=None,\n",
    "                                     single_fig_plot: Optional[bool]=True,\n",
    "                                     log_dir: Union[Path, str]=\".\",\n",
    "                                     height = 13,\n",
    "                                     width = 25,\n",
    "                                     kwargs: Optional[Dict[str,str]]=None):\n",
    "\n",
    "    if isinstance(kwargs, dict):\n",
    "        kwargs = [kwargs]\n",
    "    elif kwargs is None:\n",
    "        kwargs = [{\"kwargs\":{}}]\n",
    "        \n",
    "    default_kwargs = {\"shrink\":0.9, \"binwidth\":3.0}\n",
    "    axes = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    counts = compute_class_counts(targets=data[x_col],\n",
    "                         sort_by=\"count\"\n",
    "                        )\n",
    "    class_order = list(counts.keys())\n",
    "    \n",
    "    if single_fig_plot:\n",
    "        rows = len(kwargs); cols = 1\n",
    "        figsize=(width*cols,height*rows)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "\n",
    "    for i in range(len(kwargs)):\n",
    "        kwargs_i = default_kwargs\n",
    "        kwargs_i.update(kwargs[i][\"kwargs\"])\n",
    "        \n",
    "        if single_fig_plot:\n",
    "            ax = axes[i]\n",
    "            if \"title\" in kwargs[i]:\n",
    "                ax.set_title(kwargs[i][\"title\"], fontsize=\"large\")\n",
    "            plt.subplots_adjust(bottom=0.05, top=0.96, wspace=None, hspace=0.25)\n",
    "            \n",
    "        else:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20,12))\n",
    "            axes.append(ax)\n",
    "            if \"title\" in kwargs[i]:\n",
    "#                 ax.set_title(kwargs[i][\"title\"], fontsize=\"large\")\n",
    "                plt.suptitle(kwargs[i][\"title\"], fontsize=\"large\")\n",
    "            plt.subplots_adjust(bottom=0.15, top=0.95, wspace=None, hspace=0.3)            \n",
    "        \n",
    "        ax = sns.histplot(data=data,\n",
    "                          x=x_col,\n",
    "                          hue=group_col,\n",
    "                          ax=ax,\n",
    "                          pthresh=0.1,\n",
    "                          **kwargs_i)\n",
    "\n",
    "        plt.sca(ax)\n",
    "        sns.despine()\n",
    "        xticklabels = bool(data[x_col].nunique() < 100)\n",
    "        if xticklabels:\n",
    "            plt.xticks(\n",
    "                rotation=45, \n",
    "                horizontalalignment='right',\n",
    "                fontweight='light',\n",
    "                fontsize='small'\n",
    "            )\n",
    "        else:\n",
    "            ax.set_xticklabels([])\n",
    "            \n",
    "            \n",
    "        if not single_fig_plot:\n",
    "            if \"savefig\" in kwargs[i]:\n",
    "                plt.savefig(kwargs[i][\"savefig\"])\n",
    "\n",
    "    if single_fig_plot:\n",
    "        plt.suptitle(suptitle, fontsize=\"x-large\")\n",
    "        if isinstance(savefig, (Path, str)):\n",
    "            print(f'Saving: savefig={savefig}')\n",
    "            plt.savefig(savefig)\n",
    "        elif isinstance(suptitle, str):\n",
    "            print(f'Saving: suptitle={suptitle}')\n",
    "            plt.savefig(os.path.join(log_dir, f\"{suptitle}.png\"))\n",
    "    return fig, axes\n",
    "\n",
    "#         if \"title\" in kwargs[i]:\n",
    "#             ax.set_title(kwargs[i][\"title\"], fontsize=\"large\")        \n",
    "#     plt.suptitle(suptitle)    \n",
    "#     plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21ab8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Latest data distribution plots -- July 18th, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [initialize_config(config_dir=config_dir,\n",
    "                        overrides=[\"dataset=pnas_dataset\"]),\n",
    "            initialize_config(config_dir=config_dir,\n",
    "                        overrides=[\"dataset=extant_dataset\"]),\n",
    "            initialize_config(config_dir=config_dir,\n",
    "                        overrides=[\"dataset=fossil_dataset\", \"hparams.image_size=1024\"])\n",
    "           ]\n",
    "\n",
    "logdir = f\"/media/data/jacob/GitHub/lightning-hydra-classifiers/outputs/data_distribution_logs\"\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(configs)):\n",
    "\n",
    "    cfg = configs[i]\n",
    "    data = LeavesLightningDataModule(cfg.datamodule.config)\n",
    "    data_splits = {\"train\": data.train_dataset,\n",
    "                   \"val\": data.val_dataset,\n",
    "                   \"test\": data.test_dataset}\n",
    "\n",
    "\n",
    "    dataset_name = cfg.dataset.config.name\n",
    "    label_col = cfg.dataset.config.class_type\n",
    "    group_col = \"subset\"\n",
    "\n",
    "    kwargs_options = [{\"kwargs\":{\"multiple\":\"dodge\", \"stat\":\"count\", \"kde\":True, \"shrink\":0.9, \"binwidth\":2*1.5},\n",
    "                       \"title\":f\"{dataset_name}, Per-class countplot, grouped by subset\",\n",
    "                       \"savefig\":os.path.join(logdir, f\"{dataset_name}_{label_col}_unstacked_subset_count_distributions.png\")},\n",
    "                      {\"kwargs\":{\"multiple\":\"fill\", \"stat\":\"probability\", \"shrink\":0.95, \"binwidth\":2*0.6},\n",
    "                       \"title\":f\"{dataset_name}, Per-class filled histograms, grouped by subset\",\n",
    "                       \"savefig\":os.path.join(logdir, f\"{dataset_name}_{label_col}_stacked_subset_prior_probabilities.png\")},\n",
    "                      {\"kwargs\":{\"multiple\":\"stack\", \"stat\":\"count\", \"shrink\":0.9, \"binwidth\":2*1.5},\n",
    "                       \"title\":f\"{dataset_name}, Per-class countplot, grouped by subset\",\n",
    "                       \"savefig\":os.path.join(logdir, f\"{dataset_name}_{label_col}_stacked_subset_count_distributions.png\")}\n",
    "                     ]\n",
    "\n",
    "    target_splits = pd.concat([pd.DataFrame(v.targets).assign(**{group_col:k}) for k, v in data_splits.items()\n",
    "                              ]).rename(columns={0:label_col})\n",
    "\n",
    "\n",
    "    ### Sort classes by count\n",
    "    data_df = target_splits\n",
    "    counts = compute_class_counts(targets=data_df[label_col],\n",
    "                                  sort_by=\"count\"\n",
    "                        )\n",
    "    class_order = {label:i for i, label in enumerate(counts.keys())}\n",
    "\n",
    "    data_df = data_df.assign(family_order = data_df.family.apply(lambda x: class_order[x]))\n",
    "    target_splits = data_df.sort_values(by=[\"family_order\"], ascending=True).drop(columns=[\"family_order\"])\n",
    "\n",
    "\n",
    "\n",
    "    plot_grouped_class_distributions(data=target_splits,\n",
    "                                     x_col=label_col,\n",
    "                                     group_col=group_col,\n",
    "                                     single_fig_plot=False,\n",
    "    #                                  suptitle=f\"Dataset: {dataset_name} {label_col} class distributions\",\n",
    "                                     log_dir = logdir,\n",
    "                                     kwargs = kwargs_options[:])\n",
    "\n",
    "    plot_grouped_class_distributions(data=target_splits,\n",
    "                                     x_col=label_col,\n",
    "                                     group_col=group_col,\n",
    "                                     single_fig_plot=True,\n",
    "                                     suptitle=f\"Dataset={dataset_name} {label_col} class distributions\",\n",
    "                                     log_dir = logdir,\n",
    "                                     kwargs = kwargs_options[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cc9dd",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb499ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d7953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1b8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29645ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "assert isinstance(data_splits, dict)\n",
    "num_splits = len(data_splits)\n",
    "\n",
    "if num_splits < 4:\n",
    "    rows = num_splits\n",
    "    cols = 1\n",
    "else:\n",
    "    rows = int(num_splits // 2)\n",
    "    cols = int(num_splits % 2)\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(16*cols,8*rows))\n",
    "ax = ax.flatten()\n",
    "\n",
    "\n",
    "train_key = [k for k,v in data_splits.items() if \"train\" in k]\n",
    "if len(train_key)==1:\n",
    "    train_counts = compute_class_counts(data_splits[train_key[0]].targets,\n",
    "                                        sort_by=\"count\")\n",
    "xticklabels=False\n",
    "num_samples = 0\n",
    "counts = {}\n",
    "for i, (k, v) in enumerate(data_splits.items()):\n",
    "    if i == len(data_splits)-1:\n",
    "        xticklabels=True\n",
    "    counts[k] = plot_class_distributions(targets=v.targets, \n",
    "                                         sort_by=train_counts,\n",
    "                                         ax = ax[i],\n",
    "                                         xticklabels=xticklabels)\n",
    "    plt.gca().set_title(f\"{k} (n={len(v)})\", fontsize='large')\n",
    "\n",
    "    num_samples += len(v)\n",
    "\n",
    "plt.suptitle('-'.join(list(data_splits.keys())) + f\"_splits (total={num_samples})\", fontsize='x-large')\n",
    "plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.07)\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "width = 0.5\n",
    "\n",
    "temp_summer=[ random.uniform(20,40) for i in range(5)]\n",
    "temp_winter=[ random.uniform(0,10) for i in range(5)]\n",
    "\n",
    "fig=plt.figure(figsize=(10,6))\n",
    "\n",
    "city=['City A','City B','City C','City D','City E']\n",
    "x_pos_summer=list(range(1,6))\n",
    "x_pos_winter=[ i+width for i in x_pos_summer]\n",
    "\n",
    "graph_summer=plt.bar(x_pos_summer, temp_summer, color='tomato', label='Summer', width=width)\n",
    "graph_winter=plt.bar(x_pos_winter, temp_winter, color='dodgerblue', label='Winter', width=width)\n",
    "\n",
    "plt.xticks([i+width/2 for i in x_pos_summer],city)\n",
    "plt.title('City Temperature')\n",
    "plt.ylabel('Temperature ($^\\circ$C)')\n",
    "\n",
    "#Annotating graphs\n",
    "for summer_bar,winter_bar,ts,tw in zip(graph_summer,graph_winter,temp_summer,temp_winter):\n",
    "    plt.text(summer_bar.get_x() + summer_bar.get_width()/2.0,summer_bar.get_height(),'%.2f$^\\circ$C'%ts,ha='center',va='bottom')\n",
    "    plt.text(winter_bar.get_x() + winter_bar.get_width()/2.0,winter_bar.get_height(),'%.2f$^\\circ$C'%tw,ha='center',va='bottom')\n",
    "\n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf1487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.train_dataset)\n",
    "display(data.val_dataset)\n",
    "display(data.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760940f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.train_dataloader()\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f590f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77365a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dir(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"0\":[0,1,2,3,4], \"1\":[0,1,2,3,4]}).T.to_records()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc870db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset.plot_trainvaltest_splits(data.train_dataset,\n",
    "                                     data.val_dataset,\n",
    "                                     data.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449816e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "torchvision.transforms.ToPILImage()(data.train_dataset[387][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pathlib import Path\n",
    "# path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "# # path_schema: str = \"{family}_{genus}_{species}_{catalog_number}\"\n",
    "\n",
    "# # # path_schema = Path(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_1536/train/Fabaceae\")\n",
    "# # filepath = 'Fabaceae_Derris_alborubra_Wolfe_9829.jpg'\n",
    "# sep = \"_\"\n",
    "\n",
    "# # from dataclasses import dataclass\n",
    "# # from typing import *\n",
    "\n",
    "\n",
    "# # @dataclass \n",
    "# # class PathSchema:\n",
    "# #     path_schema: str = Path(\"{family}_{genus}_{species}_{collection}_{catalog_number}\")\n",
    "# #     schema_parts: List[str] = path_schema.split(sep)\n",
    "# #     maxsplit = len(schema_parts) - 2\n",
    "    \n",
    "# #     def parse(self, path: Union[Path, str], sep: str=\"_\"):\n",
    "    \n",
    "# #         parts = Path(path).stem.split(sep, maxsplit=maxsplit)\n",
    "# #         if len(parts) == 5:\n",
    "# #             family, genus, species, collection, catalog_number = parts\n",
    "# #         if len(parts) == 4:\n",
    "# #             family, genus, species, catalog_number = parts\n",
    "# #             collection = catalog_number.split(\"_\")\n",
    "\n",
    "# #         return family, genus, species, collection, catalog_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Extant_Leaves/Aizoaceae/Aizoaceae_Galenia_pubescens_Hickey_Hickey_8097.jpg\").stem\n",
    "\n",
    "path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "schema_parts = path_schema.split(sep)\n",
    "maxsplit = len(schema_parts) - 2\n",
    "\n",
    "print(f\"schema_parts={schema_parts}\")\n",
    "print(f\"maxsplit={maxsplit}\")\n",
    "family, genus, species, collection, catalog_number = Path(filepath).stem.split(\"_\", maxsplit=maxsplit)\n",
    "print(family, genus, species, collection, catalog_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_1536/train/Fabaceae/Fabaceae_Derris_alborubra_Wolfe_9829.jpg\").stem\n",
    "\n",
    "path_schema: str = \"{family}_{genus}_{species}_{catalog_number}\"\n",
    "schema_parts = path_schema.split(sep)\n",
    "maxsplit = len(schema_parts) - 2\n",
    "\n",
    "print(f\"schema_parts={schema_parts}\")\n",
    "print(f\"maxsplit={maxsplit}\")\n",
    "\n",
    "family, genus, species, catalog_number = Path(filepath).stem.split(\"_\", maxsplit=maxsplit)\n",
    "print(family, genus, species, catalog_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ceb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "toPIL = torchvision.transforms.ToPILImage(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(stage='val')\n",
    "\n",
    "data.show_batch(stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.train_dataset\n",
    "\n",
    "train_dataloader = data.train_dataloader()\n",
    "\n",
    "train_dataset = data.get_dataset(\"train\")\n",
    "\n",
    "train_dataset.show_batch()\n",
    "\n",
    "# train_dataset = \n",
    "data.get_dataset(\"val\")\n",
    "\n",
    "# train_dataset = \n",
    "data.get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8df067",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cbf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(OmegaConf.to_container(cfg.dataset, resolve=True))\n",
    "\n",
    "pp(OmegaConf.to_container(cfg.datamodule.config.dataset, resolve=True))\n",
    "\n",
    "pp(OmegaConf.to_container(cfg.datamodule, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91180e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Fossil\"\n",
    "pp([k for k in CommonDataset.available_datasets.keys() if dataset_name in k])\n",
    "\n",
    "dataset_name = \"PNAS\"\n",
    "pp([k for k in CommonDataset.available_datasets.keys() if dataset_name in k])\n",
    "\n",
    "dataset_name = \"Extant\"\n",
    "pp([k for k in CommonDataset.available_datasets.keys() if dataset_name in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852fbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Extant_config = OmegaConf.create({\"name\": \"Extant_family_10_512\",\n",
    "                            \"val_split\": 0.2,\n",
    "                            \"test_split\": 0.3,\n",
    "                            \"threshold\": 3,\n",
    "                            \"seed\": 987485,\n",
    "                            \"class_type\": \"family\",\n",
    "                            \"x_col\":\"path\",\n",
    "                            \"y_col\":\"${.class_type}\",\n",
    "                            \"id_col\":\"catalog_number\"\n",
    "})\n",
    "\n",
    "\n",
    "config = OmegaConf.create({\"name\": \"Fossil_512\",\n",
    "                            \"val_split\": 0.2,\n",
    "                            \"test_split\": 0.3,\n",
    "                            \"threshold\": 3,\n",
    "                            \"seed\": 987485,\n",
    "                            \"class_type\": \"family\",\n",
    "                            \"x_col\":\"path\",\n",
    "                            \"y_col\":\"${.class_type}\",\n",
    "                            \"id_col\":\"catalog_number\"\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pp(OmegaConf.to_container(config, resolve=True))\n",
    "data = CommonDataset(config=config,\n",
    "                     files=None,\n",
    "                     class2idx=None)\n",
    "\n",
    "data[1].image\n",
    "print(data.__repr__())\n",
    "\n",
    "data.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff331f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/datamodule/fossil_datamodule.yaml\"\n",
    "config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/multi-gpu.yaml\"\n",
    "config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fae52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(OmegaConf.to_container(config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad8b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3953fa7d",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Fossil_512\"\n",
    "dataset_dirs = CommonDataSelect.available_datasets[name]\n",
    "\n",
    "dataset_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CommonDataset.available_datasets.keys()\n",
    "\n",
    "# CommonDataset.available_datasets[\"Fossil_512\"]#.keys()\n",
    "\n",
    "# fossil.available_datasets\n",
    "\n",
    "# d0 = CommonDataSelect.select_dataset_by_name(\"Fossil_512\")\n",
    "\n",
    "# dir(OmegaConf)\n",
    "\n",
    "# config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/datamodule/fossil_datamodule.yaml\"\n",
    "# config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e506aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# d1 = torchdata.datasets.Files.from_folder(Path(dataset_dirs[0]), regex=\"*/*.jpg\")\n",
    "d2 = torchdata.datasets.Files.from_folder(Path(dataset_dirs[1]), regex=\"*/*.jpg\")\n",
    "# d2\n",
    "\n",
    "from itertools import repeat, chain\n",
    "from more_itertools import collapse, flatten\n",
    "\n",
    "\n",
    "cls = torchdata.datasets.Files\n",
    "\n",
    "log.info(f\"Concatenating dataset_dirs located at: {dataset_dirs}\")\n",
    "file_list = list(flatten(\n",
    "                    [cls.from_folder(Path(root),\n",
    "                                     regex=\"*/*.jpg\").files\n",
    "                     for root in dataset_dirs]\n",
    "                                            ))\n",
    "data = cls(files=file_list,\n",
    "           name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccea848",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f17754",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = (split[0] for split in data_splits.values())\n",
    "y_train, y_val, y_test = (split[1] for split in data_splits.values())\n",
    "\n",
    "\n",
    "\n",
    "from rich import print as pp\n",
    "\n",
    "\n",
    "pp(data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.2\n",
    "test_split = 0.3\n",
    "\n",
    "train_split = 1 - (val_split + test_split)\n",
    "\n",
    "val_relative_split = val_split/(train_split + val_split)\n",
    "train_relative_split = train_split/(train_split + val_split)\n",
    "random_state = 0\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_split, random_state=random_state, stratify=y)\n",
    "print(f\"x_train.shape={x_train.shape}, x_test.shape={x_test.shape}, y_train.shape={y_train.shape}, y_test.shape={y_test.shape}\")\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_relative_split, random_state=random_state, stratify=y_train)\n",
    "\n",
    "print(f\"x_train.shape={x_train.shape}, x_val.shape={x_val.shape}, y_train.shape={y_train.shape}, y_val.shape={y_val.shape}\")\n",
    "\n",
    "\n",
    "print(f'Absolute splits: {[train_split, val_split, test_split]}')\n",
    "print(f'Relative splits: [{train_relative_split:.2f}, {val_relative_split:.2f}, {test_split}]')\n",
    "\n",
    "print(f'train+val={train_split+val_split} | test={test_split}')\n",
    "print(f'train={train_relative_split:.2f} | val={val_relative_split:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "skf.get_n_splits(x, y)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(x, y):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(f'y_test: {y_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    \"\"\"Common base class for all `torchtraining` objects.\n",
    "    Defines default `__str__` and `__repr__`.\n",
    "    Most objects should customize `__str__` according to specific\n",
    "    needs.\n",
    "    Custom objects usually use `yaml.dump` to easily see parameters\n",
    "    and whole pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{type(self).__module__}.{type(self).__name__}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        parameters = \", \".join(\n",
    "            \"{}={}\".format(key, value)\n",
    "            for key, value in self.__dict__.items()\n",
    "            if not key.startswith(\"_\")\n",
    "        )\n",
    "        return \"{}({})\".format(self, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e004a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata\n",
    "torchdata.datasets.Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata\n",
    "dir(torchdata.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87517b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa533f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=3\n",
    "test_split=0.3\n",
    "val_train_split=0.2\n",
    "batch_size=32\n",
    "num_workers=0\n",
    "seed=8567\n",
    "debug=False\n",
    "normalize=True\n",
    "image_size = 'auto'\n",
    "channels=3\n",
    "dataset_dir=None\n",
    "predict_on_split=\"val\"\n",
    "\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23824480",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = FossilLightningDataModule(name=dataset_name,\n",
    "                                       threshold=threshold,\n",
    "                                       test_split=test_split,\n",
    "                                       val_train_split=val_train_split,\n",
    "                                       batch_size=batch_size,\n",
    "                                       num_workers=num_workers,\n",
    "                                       seed=seed,\n",
    "                                       debug=debug,\n",
    "                                       normalize=normalize,\n",
    "                                       image_size=image_size,\n",
    "                                       channels=channels,\n",
    "                                       predict_on_split=predict_on_split)\n",
    "\n",
    "datamodule\n",
    "\n",
    "datamodule.setup(\"fit\")\n",
    "datamodule.setup(\"test\")\n",
    "\n",
    "# datamodule.show_batch(\"train\")\n",
    "# datamodule.show_batch(\"val\")\n",
    "# datamodule.show_batch(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f107c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/Fossil_512_train-test/2021-07-12/06-03/model/checkpoints/\"\n",
    "ckpt_path = os.path.join(ckpt_dir, \"best_model-epoch-epoch=05--val_loss-val_loss=96.52.ckpt\")\n",
    "\n",
    "print(os.path.isfile(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41140bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_state = torch.load(ckpt_path)\n",
    "\n",
    "print(type(ckpt_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45195226",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt_state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b03dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17684e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt_state['state_dict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransferLearningModel.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315352f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.create_trainvaltest_splits(dataset=data,\n",
    "                                                                  test_split=0.3,\n",
    "                                                                  val_train_split=0.2,\n",
    "                                                                  shuffle=True,\n",
    "                                                                  seed=3654,\n",
    "                                                                  plot_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_dir = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\"\n",
    "\n",
    "from hydra.experimental import compose, initialize, initialize_config_dir\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from rich import print as pp\n",
    "os.chdir(config_dir)\n",
    "\n",
    "# context initialization\n",
    "# with initialize(config_path=\"../configs\", job_name=\"test_app\"):\n",
    "\n",
    "with initialize_config_dir(config_dir=config_dir, job_name=\"multi-gpu_experiment\"):\n",
    "    \n",
    "    cfg = compose(config_name=\"multi-gpu\")\n",
    "#     print(OmegaConf.to_yaml(cfg))\n",
    "    \n",
    "    pp(OmegaConf.to_container(cfg, resolve=True))\n",
    "    \n",
    "    pp(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa712fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns_context = \"talk\"\n",
    "sns_style = \"seaborn-bright\"\n",
    "sns.set_context(context=sns_context, font_scale=0.8)\n",
    "# valid contexts = paper, notebook, talk, poster - \n",
    "# with notebook being 1:1 and paper being smaller and poster being largest\n",
    "plt.style.use(sns_style)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_class_distributions(targets: List[Any], \n",
    "#                              sort: Union[bool,Sequence]=True,\n",
    "#                              ax=None,\n",
    "#                              xticklabels: bool=True):\n",
    "#     \"\"\"\n",
    "#     Example:\n",
    "#         counts = plot_class_distributions(targets=data.targets, sort=True)\n",
    "#     \"\"\"\n",
    "#     counts = collections.Counter(targets)\n",
    "#     if hasattr(sort, \"__len__\"):\n",
    "#         counts = {k: counts[k] for k in sort}\n",
    "#     elif sort is True:\n",
    "#         counts = dict(sorted(counts.items(), key = lambda x:x[1], reverse=True))\n",
    "\n",
    "#     keys = list(counts.keys())\n",
    "#     values = list(counts.values())\n",
    "\n",
    "#     if ax is None:\n",
    "#         plt.figure(figsize=(16,12))\n",
    "#     ax = sns.histplot(x=keys, weights=values, discrete=True, ax=ax)\n",
    "#     plt.sca(ax)\n",
    "#     if xticklabels:\n",
    "#         plt.xticks(\n",
    "#             rotation=45, \n",
    "#             horizontalalignment='right',\n",
    "#             fontweight='light',\n",
    "#             fontsize='medium'\n",
    "#         )\n",
    "#     else:\n",
    "#         ax.set_xticklabels([])\n",
    "    \n",
    "#     return counts\n",
    "\n",
    "\n",
    "# def plot_trainvaltest_splits(train_data,\n",
    "#                              val_data,\n",
    "#                              test_data):\n",
    "#     \"\"\"\n",
    "#     Create 3 vertically-stacked count plots of train, val, and test dataset class label distributions\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots(3, 1, figsize=(16,8*3))\n",
    "\n",
    "#     train_counts = plot_class_distributions(targets=train_data.targets, sort=True, ax = ax[0], xticklabels=False)\n",
    "#     plt.gca().set_title(f\"train (n={len(train_data)})\", fontsize='large')\n",
    "#     sort_classes = train_counts.keys()\n",
    "\n",
    "#     val_counts = plot_class_distributions(targets=val_data.targets, ax = ax[1], sort=sort_classes, xticklabels=False)\n",
    "#     plt.gca().set_title(f\"val (n={len(val_data)})\", fontsize='large')\n",
    "#     test_counts = plot_class_distributions(targets=test_data.targets, ax = ax[2], sort=sort_classes)\n",
    "#     plt.gca().set_title(f\"test (n={len(test_data)})\", fontsize='large')\n",
    "\n",
    "#     plt.suptitle(f\"Train-Val-Test_splits (total={len(data)})\", fontsize='x-large')\n",
    "\n",
    "#     plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.07)\n",
    "    \n",
    "#     return fig, ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef90678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trainvaltest_splits(train_data,\n",
    "                         val_data,\n",
    "                         test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e38cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1262317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf328b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "242a0b5f",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d078a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b773369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708699d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267145f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = select_from_dataset(data,\n",
    "                                 indices=train_idx,\n",
    "                                 update_class2idx=False,\n",
    "                                 x_col = 'path',\n",
    "                                 y_col = \"family\")\n",
    "\n",
    "val_data = select_from_dataset(data,\n",
    "                               indices=val_idx,\n",
    "                               update_class2idx=False,\n",
    "                               x_col = 'path',\n",
    "                               y_col = \"family\")\n",
    "val_data\n",
    "\n",
    "test_data = select_from_dataset(data,\n",
    "                                indices=test_idx,\n",
    "                                update_class2idx=False,\n",
    "                                x_col = 'path',\n",
    "                                y_col = \"family\")\n",
    "\n",
    "\n",
    "\n",
    "train_counts = plot_class_distributions(targets=train_data.targets, sort=True)\n",
    "sort_classes = train_counts.keys()\n",
    "val_counts = plot_class_distributions(targets=val_data.targets, sort=sort_classes)\n",
    "test_counts = plot_class_distributions(targets=test_data.targets, sort=sort_classes)\n",
    "\n",
    "\n",
    "train_data = (train_val_samples)\n",
    "train_samples = np.array(train_val_samples)[train_idx]\n",
    "val_samples = np.array(train_val_samples)[val_idx]\n",
    "\n",
    "\n",
    "counts = plot_class_distributions(targets=data.targets, sort=True)\n",
    "\n",
    "train_val_idx.shape\n",
    "test_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fad507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 test_split=0.3,\n",
    "                 val_train_split=0.2,\n",
    "                 shuffle: bool=False,\n",
    "                 seed: int=None):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        self.indices = np.arange(range(dataset_size))\n",
    "#         test_split = int(np.floor(test_train_split * dataset_size))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        targets = dataset.targets\n",
    "\n",
    "        train_val_idx, test_idx = train_test_split(\n",
    "                                               indices,\n",
    "                                               test_size=test_split,\n",
    "                                               random_state=seed,\n",
    "                                               shuffle=shuffle,\n",
    "                                               stratify=targets)\n",
    "        \n",
    "            \n",
    "            \n",
    "        train_indices, self.test_indices = self.indices[], self.indices[test_split:]\n",
    "        train_size = len(train_indices)\n",
    "        validation_split = int(np.floor((1 - val_train_split) * train_size))\n",
    "\n",
    "        self.train_indices, self.val_indices = train_indices[ : validation_split], train_indices[validation_split:]\n",
    "\n",
    "        self.train_sampler = SubsetRandomSampler(self.train_indices)\n",
    "        self.val_sampler = SubsetRandomSampler(self.val_indices)\n",
    "        self.test_sampler = SubsetRandomSampler(self.test_indices)\n",
    "\n",
    "    def get_train_split_point(self):\n",
    "        return len(self.train_sampler) + len(self.val_indices)\n",
    "\n",
    "    def get_validation_split_point(self):\n",
    "        return len(self.train_sampler)\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_split(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing train-validation-test dataloaders')\n",
    "        self.train_loader = self.get_train_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.val_loader = self.get_validation_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.test_loader = self.get_test_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_train_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing train dataloader')\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.train_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.train_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_validation_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing validation dataloader')\n",
    "        self.val_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.val_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.val_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_test_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing test dataloader')\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.test_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d385b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.samples)#.iloc[:,0]\n",
    "df = df.assign(sub_dataset = df.apply(lambda x: x[0].parts[-3], axis=1)) #.value_counts()\n",
    "\n",
    "df = df.rename(columns={0:\"path\",\n",
    "                        1:\"family\",\n",
    "                        2:\"genus\",\n",
    "                        3:\"species\",\n",
    "                        4:\"collection\",\n",
    "                        5:\"catalog_number\"})#.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "targets = dataset.targets\n",
    "\n",
    "train_idx, valid_idx = train_test_split(\n",
    "                                        indices,\n",
    "                                        test_size=test_split,\n",
    "                                        random_state=seed,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=targets)\n",
    "\n",
    "print(np.unique(np.array(targets)[train_idx], return_counts=True))\n",
    "print(np.unique(np.array(targets)[valid_idx], return_counts=True))\n",
    "\n",
    "\n",
    "# val_split = 0.2\n",
    "# test_split = 0.3\n",
    "# total = 1.0\n",
    "# trainval_split = total-test_split\n",
    "# print(trainval_split)\n",
    "# print(trainval_split - val_split)\n",
    "# print((val_split/(trainval_split)))# - val_split)\n",
    "\n",
    "(val_split*0.7)# + 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2926f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FossilDatasetSubset(FossilDataset):\n",
    "    \n",
    "#     def __init__(self,\n",
    "#                  split\n",
    "#                  files: List[Path]=None,\n",
    "#                  name: Optional[str]=None,\n",
    "#                  return_items: List[str] = [\"image\",\"target\",\"path\"],\n",
    "#                  image_return_type: str = \"tensor\",\n",
    "#                  *args, **kwargs):\n",
    "#                 ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64857753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('starting')\n",
    "\n",
    "model = models.resnet18()\n",
    "# inputs = torch.randn(5, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "batch_size = 64\n",
    "\n",
    "dataloader = DataLoader(data,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "#                         sampler=None,\n",
    "#                         batch_sampler=None,\n",
    "#                         num_workers=0,\n",
    "#                         collate_fn=None,\n",
    "#                         pin_memory=False,\n",
    "#                         drop_last=False,\n",
    "#                         timeout=0,\n",
    "#                         worker_init_fn=None)\n",
    "\n",
    "# idx = [0,10,20,50,100]\n",
    "# idx = 10\n",
    "idx = list(range(0,1000,100))\n",
    "print(len(idx))\n",
    "data.display_grid(idx, repeat_n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data.samples[0][0].parts[-3]\n",
    "\n",
    "df = pd.DataFrame(data.samples)#.iloc[:,0]\n",
    "df = df.assign(sub_dataset = df.apply(lambda x: x[0].parts[-3], axis=1)) #.value_counts()\n",
    "\n",
    "df = df.rename(columns={0:\"path\",\n",
    "                        1:\"family\",\n",
    "                        2:\"genus\",\n",
    "                        3:\"species\",\n",
    "                        4:\"collection\",\n",
    "                        5:\"catalog_number\"})#.value_counts()\n",
    "\n",
    "# df.value_counts().plot(kind='bar')\n",
    "\n",
    "chart = sns.catplot(\n",
    "    data=df, #[data['Year'].isin([1980, 2008])],\n",
    "    x='family',\n",
    "    kind='count',\n",
    "    palette='Set1',\n",
    "    row='sub_dataset',\n",
    "    aspect=3,\n",
    "    height=3\n",
    ")\n",
    "chart.set_xticklabels(rotation=65, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6abab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sport = (df\n",
    "            .groupby('family')\n",
    "            .filter(lambda x : len(x) > 10)\n",
    "            .groupby(['family', 'genus'])\n",
    "#             .groupby(['genus', 'species'])\n",
    "            .size()\n",
    "            .unstack()\n",
    "           )\n",
    "by_sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09437b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def select_from_indices(data,\n",
    "#                         indices: Sequence,\n",
    "#                         update_class2idx: bool=False,\n",
    "#                         x_col = 'path',\n",
    "#                         y_col = \"family\") -> \"FossilDataset\":\n",
    "#     \"\"\"\n",
    "#     Helper method to create a new FossilDataset containing only samples contained in `indices`\n",
    "#     Useful for producing train/val/test splits\n",
    "    \n",
    "#     \"\"\"\n",
    "#     if update_class2idx:\n",
    "#         class2idx=None\n",
    "#     else:\n",
    "#         class2idx=data.class2idx\n",
    "\n",
    "    \n",
    "#     df = pd.DataFrame(data.samples)\n",
    "#     df = df.rename(columns={0:\"path\",\n",
    "#                             1:\"family\",\n",
    "#                             2:\"genus\",\n",
    "#                             3:\"species\",\n",
    "#                             4:\"collection\",\n",
    "#                             5:\"catalog_number\"})#.value_counts()\n",
    "    \n",
    "#     df = df.iloc[indices,:]\n",
    "    \n",
    "#     files = df[x_col].to_list()\n",
    "\n",
    "#     return FossilDataset(files=files,\n",
    "#                          name=data.name,\n",
    "#                          return_items=data.return_items,\n",
    "#                          image_return_type=data.image_return_type,\n",
    "#                          class2idx=class2idx)\n",
    "\n",
    "\n",
    "\n",
    "# def filter_df_by_threshold(df: pd.DataFrame,\n",
    "#                            threshold: int,\n",
    "#                            y_col: str='family'):\n",
    "#     \"\"\"\n",
    "#     Filter rare classes from dataset in a pd.DataFrame\n",
    "    \n",
    "#     Input:\n",
    "#         df (pd.DataFrame):\n",
    "#             Must contain at least 1 column with name given by `y_col`\n",
    "#         threshold (int):\n",
    "#             Exclude any rows from df that contain a `y_col` value with fewer than `threshold` members in all of df.\n",
    "#         y_col (str): default=\"family\"\n",
    "#             The column in df to look for rare classes to exclude.\n",
    "#     Output:\n",
    "#         (pd.DataFrame):\n",
    "#             Returns a dataframe with the same number of columns as df, and an equal or lower number of rows.\n",
    "#     \"\"\"\n",
    "#     return df.groupby(y_col).filter(lambda x: len(x) >= threshold)\n",
    "\n",
    "\n",
    "\n",
    "# def filter_samples_by_threshold(data: FossilDataset,\n",
    "#                                 threshold: int=1,\n",
    "#                                 update_class2idx: bool=True,\n",
    "#                                 x_col = 'path',\n",
    "#                                 y_col = \"family\") -> FossilDataset:\n",
    "#     if update_class2idx:\n",
    "#         class2idx=None\n",
    "#     else:\n",
    "#         class2idx=data.class2idx\n",
    "\n",
    "        \n",
    "#     df = pd.DataFrame(data.samples)\n",
    "#     df = df.rename(columns={0:\"path\",\n",
    "#                             1:\"family\",\n",
    "#                             2:\"genus\",\n",
    "#                             3:\"species\",\n",
    "#                             4:\"collection\",\n",
    "#                             5:\"catalog_number\"})#.value_counts()\n",
    "    \n",
    "#     df = filter_df_by_threshold(df=df,\n",
    "#                                 threshold=threshold,\n",
    "#                                 y_col=y_col)\n",
    "        \n",
    "#     files = df[x_col].to_list()\n",
    "\n",
    "#     return FossilDataset(files=files,\n",
    "#                          name=data.name,\n",
    "#                          return_items=data.return_items,\n",
    "#                          image_return_type=data.image_return_type,\n",
    "#                          class2idx=class2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429dc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset.targets\n",
    "\n",
    "# @classmethod\n",
    "# def create_trainvaltest_splits(cls,\n",
    "#                                dataset,\n",
    "#                                test_split: float=0.3,\n",
    "#                                val_train_split: float=0.2,\n",
    "#                                shuffle: bool=True,\n",
    "#                                seed: int=3654):\n",
    "\n",
    "#     dataset_size = len(dataset)\n",
    "#     indices = np.arange(dataset_size)\n",
    "\n",
    "#     samples = np.array(dataset.samples)\n",
    "#     targets = np.array(dataset.targets)\n",
    "\n",
    "#     train_val_idx, test_idx = train_test_split(\n",
    "#                                                indices,\n",
    "#                                                test_size=test_split,\n",
    "#                                                random_state=seed,\n",
    "#                                                shuffle=shuffle,\n",
    "#                                                stratify=targets)\n",
    "\n",
    "#     train_val_targets = targets[train_val_idx]\n",
    "\n",
    "#     trainval_indices = np.arange(len(train_val_targets))\n",
    "#     train_idx, val_idx = train_test_split(\n",
    "#                                           trainval_indices,\n",
    "#                                           test_size=val_train_split,\n",
    "#                                           random_state=seed,\n",
    "#                                           shuffle=shuffle,\n",
    "#                                           stratify=train_val_targets)\n",
    "\n",
    "#     train_data = data.select_from_indices(indices=train_idx,\n",
    "#                                           update_class2idx=False,\n",
    "#                                           x_col = 'path',\n",
    "#                                           y_col = \"family\")\n",
    "\n",
    "\n",
    "#     val_data = data.select_from_indices(indices=val_idx,\n",
    "#                                         update_class2idx=False,\n",
    "#                                         x_col = 'path',\n",
    "#                                         y_col = \"family\")\n",
    "\n",
    "\n",
    "#     test_data = data.select_from_indices(indices=test_idx,\n",
    "#                                          update_class2idx=False,\n",
    "#                                          x_col = 'path',\n",
    "#                                          y_col = \"family\")\n",
    "\n",
    "\n",
    "#     return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "g = sns.heatmap(\n",
    "    by_sport, \n",
    "    square=True, # make cells square\n",
    "    cbar_kws={'fraction' : 0.01}, # shrink colour bar\n",
    "    cmap='OrRd', # use orange/red colour map\n",
    "    linewidth=1 # space between cells\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e75187",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "g = sns.heatmap(\n",
    "    by_sport, \n",
    "    square=True,\n",
    "    cbar_kws={'fraction' : 0.01},\n",
    "    cmap='OrRd',\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "g.set_yticklabels(g.get_yticklabels(), rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(\n",
    "    data=data[data['Year'].isin([1980, 2008])],\n",
    "    x='Sport',\n",
    "    kind='count',\n",
    "    palette='Set1',\n",
    "    row='Year',\n",
    "    aspect=3,\n",
    "    height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "count_dist = collections.Counter(data.targets)\n",
    "# count_dist.update(data.targets)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_class_distributions(targets: List[Any])\n",
    "test = count_dist #{1:1,2:1,3:1,4:2,5:3,6:5,7:4,8:2,9:1,10:1}\n",
    "# with matplotlib\n",
    "plt.hist(list(test.keys()), weights=list(test.values()))\n",
    "\n",
    "test = sorted(test.items(), key = lambda x:x[1], reverse=True)\n",
    "\n",
    "test\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# keys = list(test.keys())\n",
    "# values = list(test.values())\n",
    "\n",
    "keys = [i[0] for i in test]\n",
    "values = [i[1] for i in test]\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "chart = sns.histplot(x=keys, weights=values, discrete=True)\n",
    "plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='x-large'  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# chart.set_xticklabels(\n",
    "#     chart.get_xticklabels(), \n",
    "#     rotation=45, \n",
    "#     horizontalalignment='right',\n",
    "#     fontweight='light',\n",
    "#     fontsize='x-large'\n",
    "    \n",
    "# )\n",
    "\n",
    "# None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e62ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with seaborn (use hist_kws to send arugments to plt.hist, used underneath)\n",
    "sns.distplot(range(len(list(test.keys()))), hist_kws={\"weights\":list(test.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf740675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "\n",
    "max_batches = 10\n",
    "\n",
    "reporter = MemReporter()\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(i, len(batch), batch[0].shape)\n",
    "\n",
    "    print('========= before backward =========')\n",
    "    reporter.report()\n",
    "    out = model(batch[0])\n",
    "    \n",
    "    loss = nn.functional.cross_entropy(out, batch[1])\n",
    "    loss.backward()\n",
    "    print('========= after backward =========')\n",
    "    reporter.report()\n",
    "    \n",
    "    if i>=max_batches:\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b0aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9e9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086977e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter = MemReporter(model)\n",
    "\n",
    "\n",
    "print('========= before loop =========')\n",
    "reporter.report()\n",
    "for batch in data[]\n",
    "out.backward()\n",
    "print('========= after backward =========')\n",
    "reporter.report()\n",
    "###################################################\n",
    "import torch\n",
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "lstm = torch.nn.LSTM(1024, 1024).cuda()\n",
    "reporter = MemReporter(lstm)\n",
    "reporter.report(verbose=True)\n",
    "inp = torch.Tensor(10, 10, 1024).cuda()\n",
    "out, _ = lstm(inp)\n",
    "out.mean().backward()\n",
    "reporter.report(verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "# with torch.autograd.profiler.profile() as prof:\n",
    "    print('starting')\n",
    "    inputs = torch.randn(5,3,224,224, device='cuda')\n",
    "    print('half way')\n",
    "    outputs = inputs + torch.randn(5,3,224,224, device='cuda')\n",
    "    print('ending')\n",
    "    \n",
    "# print(prof)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043a296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b794b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_memlab import MemReporter\n",
    "# linear = torch.nn.Linear(1024, 1024).cuda()\n",
    "# linear2 = torch.nn.Linear(1024, 1024).cuda()\n",
    "\n",
    "\n",
    "# def inner():\n",
    "#     torch.nn.Linear(100, 100).cuda()\n",
    "\n",
    "# def outer():\n",
    "#     linear = torch.nn.Linear(100, 100).cuda()\n",
    "#     linear2 = torch.nn.Linear(100, 100).cuda()\n",
    "#     inner()\n",
    "# reporter = MemReporter()\n",
    "# reporter.report()\n",
    "\n",
    "linear = torch.nn.Linear(1024, 1024).cuda()\n",
    "inp = torch.Tensor(512, 1024).cuda()\n",
    "# pass in a model to automatically infer the tensor names\n",
    "reporter = MemReporter(linear)\n",
    "out = linear(inp).mean()\n",
    "print('========= before backward =========')\n",
    "reporter.report()\n",
    "out.backward()\n",
    "print('========= after backward =========')\n",
    "reporter.report()\n",
    "###################################################\n",
    "import torch\n",
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "lstm = torch.nn.LSTM(1024, 1024).cuda()\n",
    "reporter = MemReporter(lstm)\n",
    "reporter.report(verbose=True)\n",
    "inp = torch.Tensor(10, 10, 1024).cuda()\n",
    "out, _ = lstm(inp)\n",
    "out.mean().backward()\n",
    "reporter.report(verbose=True)\n",
    "\n",
    "# import torch\n",
    "# from pytorch_memlab import LineProfiler\n",
    "\n",
    "# def inner():\n",
    "#     torch.nn.Linear(100, 100).cuda()\n",
    "\n",
    "# def outer():\n",
    "#     linear = torch.nn.Linear(100, 100).cuda()\n",
    "#     linear2 = torch.nn.Linear(100, 100).cuda()\n",
    "#     inner()\n",
    "\n",
    "# with LineProfiler(outer, inner) as prof:\n",
    "#     outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28144d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pytorch_memlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea443ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93579753",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.print_stats()\n",
    "\n",
    "dir(prof)\n",
    "# type(prof)\n",
    "\n",
    "prof.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "# with torch.autograd.profiler.profile() as prof:\n",
    "    print('starting')\n",
    "    inputs = torch.randn(5,3,224,224, device='cuda')\n",
    "    print('half way')\n",
    "    outputs = inputs + torch.randn(5,3,224,224, device='cuda')\n",
    "    print('ending')\n",
    "    \n",
    "# print(prof)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "\n",
    "# if i % 1000 == 0:\n",
    "#     print(\"Iteration: {}, memory: {}\".format(i, psutil.virtual_memory()))\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.profiler.PyTorchProfiler(dirpath=None,\n",
    "#                             filename=None,\n",
    "#                             group_by_input_shapes=False,\n",
    "#                             emit_nvtx=False,\n",
    "#                             export_to_chrome=True,\n",
    "#                             row_limit=20,\n",
    "#                             sort_by_key=None,\n",
    "#                             record_functions=None,\n",
    "#                             record_module_names=True,\n",
    "#                             profiled_functions=None,\n",
    "#                             output_filename=None, \n",
    "#                             **profiler_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = [0,1,2,3,4]\n",
    "# idx = 10\n",
    "# data.display_grid(idx, repeat_n=5)\n",
    "data.display_grid(idx, repeat_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "self=data\n",
    "indices=10\n",
    "repeat_n=5\n",
    "from itertools import repeat, chain\n",
    "from more_itertools import collapse\n",
    "import random\n",
    "indices = random.sample(range(self.num_samples), indices)\n",
    "idx = collapse((repeat(i,repeat_n) for i in indices))\n",
    "\n",
    "# print([i for i in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self = data\n",
    "# indices = idx\n",
    "\n",
    "# if isinstance(indices, int):\n",
    "#     indices = random.sample(range(self.num_samples), indices)\n",
    "# indices = list(indices)\n",
    "# images = [self[idx][0] for idx in indices]\n",
    "# labels = [self.classes[self[idx][1]] for idx in indices]\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58148b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.display_grid(idx)\n",
    "plt.suptitle(f\"{idx} random images\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05817745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c136bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a969b829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67100192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from PIL.Image import Image as PilImage\n",
    "# import textwrap, os\n",
    "\n",
    "# def display_images(\n",
    "#     images: [PilImage], \n",
    "#     columns=5, max_images=15,\n",
    "#     width=20, height=8,    \n",
    "#     label_wrap_length=50, \n",
    "#     label_font_size=8):\n",
    "\n",
    "#     if not images:\n",
    "#         print(\"No images to display.\")\n",
    "#         return \n",
    "\n",
    "#     if len(images) > max_images:\n",
    "#         print(f\"Showing {max_images} images of {len(images)}:\")\n",
    "#         images=images[0:max_images]\n",
    "\n",
    "#     rows = int(len(images)/columns)\n",
    "        \n",
    "#     height = max(height, rows * height)\n",
    "#     plt.figure(figsize=(width, height))\n",
    "#     for i, image in enumerate(images):\n",
    "\n",
    "#         plt.subplot(rows + 1, columns, i + 1)\n",
    "#         plt.imshow(image)\n",
    "\n",
    "#         if hasattr(image, 'filename'):\n",
    "#             title=image.filename\n",
    "#             if title.endswith(\"/\"): title = title[0:-1]\n",
    "#             title=os.path.basename(title)\n",
    "#             title=textwrap.wrap(title, label_wrap_length)\n",
    "#             title=\"\\n\".join(title)\n",
    "#             plt.title(title, fontsize=label_font_size); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad977d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[5][0]\n",
    "\n",
    "import random\n",
    "num_display = 12\n",
    "\n",
    "indices = random.sample(range(data.num_samples), num_display)\n",
    "\n",
    "indices\n",
    "\n",
    "# indices = range(0,4)\n",
    "\n",
    "display_images(\n",
    "    images = [data[idx][0] for idx in indices],\n",
    "    columns=5, max_images=15,\n",
    "    width=20, height=8,    \n",
    "    label_wrap_length=50, \n",
    "    label_font_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162e0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def parse_item(self, index: int):\n",
    "#         path = self.files[index]\n",
    "#         family, genus, species, collection, catalog_number = path.stem.split(\"_\", maxsplit=4)\n",
    "#         item = {\"path\":path,\n",
    "#                 \"target\":None,\n",
    "#                 \"family\":family,\n",
    "#                 \"genus\":genus,\n",
    "#                 \"species\":species,\n",
    "#                 \"collection\":collection,\n",
    "#                 \"catalog_number\":catalog_number}\n",
    "#         item[\"target\"] = item[self.class_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698979a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c79c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adb5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdbc7f33",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Previous Fossil class def code, now relocated to fossil.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import flatten\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00238688",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets = {\n",
    "    \"Wilf_Fossil_512\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Wilf_Fossil\",\n",
    "    \"Wilf_Fossil_1024\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Wilf_Fossil\",\n",
    "    \"Wilf_Fossil_1536\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Wilf_Fossil\",\n",
    "    \"Wilf_Fossil_2048\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Wilf_Fossil\",\n",
    "    \n",
    "    \"Florissant_Fossil_512\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Florissant_Fossil\",\n",
    "    \"Florissant_Fossil_1024\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Florissant_Fossil\",\n",
    "    \"Florissant_Fossil_1536\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Florissant_Fossil\",\n",
    "    \"Florissant_Fossil_2048\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Florissant_Fossil\"\n",
    "}\n",
    "\n",
    "available_datasets[\"Fossil_512\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Wilf_Fossil\",\n",
    "                                    \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Florissant_Fossil\"]\n",
    "available_datasets[\"Fossil_1024\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Wilf_Fossil\",\n",
    "                                     \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Florissant_Fossil\"]\n",
    "available_datasets[\"Fossil_1536\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Wilf_Fossil\",\n",
    "                                     \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Florissant_Fossil\"]\n",
    "available_datasets[\"Fossil_2048\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Wilf_Fossil\",\n",
    "                                     \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Florissant_Fossil\"]\n",
    "\n",
    "fossil_collections = {\"Florissant\":\"florissant_fossil\",\n",
    "                      \"Wilf\":\"wilf_fossil\"}\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    name: str\n",
    "    dataset: str=None\n",
    "    collection: str=None\n",
    "    resolution: int=None\n",
    "        \n",
    "    num_files: Optional[int]=None\n",
    "    num_classes: Optional[int]=None\n",
    "    class_type: str=\"family\"\n",
    "    path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "                \n",
    "        \n",
    "    def __init__(self, name: str, **kwargs):\n",
    "        self.name = name\n",
    "        parts = self.name.split(\"_\")\n",
    "        self.resolution = int(parts[-1])\n",
    "        if len(parts)==3:\n",
    "            self.dataset = parts[1]\n",
    "            self.collection = \"_\".join(parts[:2])\n",
    "        elif len(parts)==2:    \n",
    "            self.dataset = parts[0]\n",
    "            self.collection = [\"_\".join([c, self.dataset]) for c in fossil_collections.keys()]\n",
    "            \n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        disp = f\"\"\"<{str(type(self)).strip(\"'>\").split('.')[1]}>:\"\"\"\n",
    "        \n",
    "        disp += \"\\nFields:\\n\"\n",
    "        for k in self.__dataclass_fields__.keys():\n",
    "            disp += f\"    {k}: {getattr(self,k)}\\n\"\n",
    "        return disp\n",
    "    \n",
    "\n",
    "DatasetConfig(\"Fossil_512\")\n",
    "\n",
    "class FossilDataset(torchdata.datasets.Files): #ImageDataset):\n",
    "    \n",
    "#     loader: Callable = Image.open\n",
    "    transform = None\n",
    "    target_transform = None\n",
    "    \n",
    "    class_type: str=\"family\"\n",
    "    totensor: Callable = torchvision.transforms.ToTensor()\n",
    "    def __init__(self,\n",
    "                 files: List[Path],\n",
    "                 return_items: List[str] = [\"image\",\"target\",\"path\"],\n",
    "                 image_return_type: str = \"tensor\",\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(files=files, *args, **kwargs)\n",
    "        \n",
    "        self.name = kwargs.get(\"name\",\"\")\n",
    "        self.return_items = return_items\n",
    "        self.image_return_type = image_return_type\n",
    "        \n",
    "        self.samples = [self.parse_item(idx) for idx in range((len(self)))]\n",
    "        self.targets = [sample[1] for sample in self.samples]\n",
    "        self._imgs = None\n",
    "        self.classes = sorted(set(self.targets))\n",
    "        self.class2idx = {name:idx for idx, name in enumerate(self.classes)}\n",
    "        \n",
    "        self.config = DatasetConfig(self.name,\n",
    "                                    class_type=self.class_type,\n",
    "                                    num_files=len(self.files),\n",
    "                                    num_classes=len(self.classes)\n",
    "                                   )\n",
    "\n",
    "    def getitem(self, index: int):\n",
    "        path, family, genus, species, collection, catalog_number = self.samples[index]\n",
    "        img = Image.open(path)\n",
    "        return img, family, path\n",
    "\n",
    "#     @property\n",
    "#     def transform(self) -> Callable:#, img: PIL.Image):\n",
    "#         _transforms = []\n",
    "#         if self.image_return_type == \"tensor\":\n",
    "#             _transforms.append(self.totensor)\n",
    "#         return lambda x: x\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        img, family, path = self.getitem(index)\n",
    "        target = self.class2idx[family]\n",
    "        \n",
    "        if self.image_return_type == \"tensor\":\n",
    "            img = self.totensor(img)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target, path\n",
    "    \n",
    "    \n",
    "    def parse_item(self, index: int):\n",
    "        path = self.files[index]\n",
    "        family, genus, species, collection, catalog_number = path.stem.split(\"_\", maxsplit=4)\n",
    "        return path, family, genus, species, collection, catalog_number\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.config.__repr__()\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "        dataset_dirs = available_datasets[name]\n",
    "        if isinstance(available_datasets[name], str):\n",
    "            dataset_dirs = [available_datasets[name]]\n",
    "        assert isinstance(dataset_dirs, list)\n",
    "        file_list = list(flatten(\n",
    "                            [torchdata.datasets.Files.from_folder(Path(root),\n",
    "                                                                  regex=\"*/*.jpg\").files\n",
    "                             for root in dataset_dirs]\n",
    "                                                    ))\n",
    "        data = FossilDataset(file_list,\n",
    "                             name=name)\n",
    "\n",
    "        return data #.map(lambda x: (torchvision.transforms.ToTensor()(x[0]), x[1]))\n",
    "    \n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "#         dataset_dirs = available_datasets[name]\n",
    "#         if isinstance(available_datasets[name], list):\n",
    "#             file_list = list(flatten(\n",
    "#                                 [torchdata.datasets.Files.from_folder(Path(root),\n",
    "#                                                         regex=\"*/*.jpg\").files\n",
    "#                                for root in available_datasets[name]]\n",
    "#                                                             ))\n",
    "            \n",
    "#         elif isinstance(available_datasets[name], str):\n",
    "#             file_list = torchdata.datasets.Files.from_folder(Path(available_datasets[name]),\n",
    "#                                                              regex=\"*/*.jpg\").files\n",
    "\n",
    "#         data = FossilDataset(file_list,\n",
    "#                                  name=name)\n",
    "\n",
    "#         return data.map(torchvision.transforms.ToTensor())\n",
    "\n",
    "fossil_data = FossilDataset.create_dataset(name=\"Fossil_1024\")\n",
    "fossil_data\n",
    "\n",
    "# fossil_data = FossilDataset.create_dataset(name=\"Florissant_Fossil_1024\")\n",
    "# fossil_data\n",
    "\n",
    "# fossil_data = FossilDataset.create_dataset(name=\"Wilf_Fossil_1024\")\n",
    "# fossil_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6674c",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Future todo: Separate subclass of simpler Leaves/Fossil Dataset class to allow for more customization of return signatures (allowing dict records instead of tuple, multiple labels per sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763307da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelFossilDataset(FossilDataset): #ImageDataset):\n",
    "    \n",
    "    loader: Callable = Image.open\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    target_transform = None\n",
    "    \n",
    "    class_type: str=\"family\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 files: List[Path],\n",
    "                 return_items: List[str,str] = [\"image\",\"target\",\"path\"],\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(files=files, *args, **kwargs)\n",
    "        \n",
    "        self.name = kwargs.get(\"name\",\"\")\n",
    "        self.return_items = return_items\n",
    "        \n",
    "        self.samples = [self.parse_item(idx) for idx in range((len(self)))]            \n",
    "        self.targets = [sample[1] for sample in self.samples]\n",
    "        self.classes = sorted(set(self.targets))\n",
    "        self.class2idx = {name:idx for idx, name in enumerate(self.classes)}\n",
    "        \n",
    "        self.config = DatasetConfig(self.name,\n",
    "                                    class_type=self.class_type,\n",
    "                                    num_files=len(self.files),\n",
    "                                    num_classes=len(self.classes)\n",
    "                                    )\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def get_item(self, index: int):\n",
    "        item = self.samples[index]\n",
    "        img = self.loader(item[\"path\"])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return self.get_item((img, target, path))\n",
    "    \n",
    "        return Image.open(item[0]), self.class2idx[item[1]]\n",
    "\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.samples[index]\n",
    "        \n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return self.get_item((img, target, path))\n",
    "#         return Image.open(item[0]), self.class2idx[item[1]]\n",
    "    \n",
    "    def parse_item(self, index: int):\n",
    "        path = self.files[index]\n",
    "        family, genus, species, collection, catalog_number = path.stem.split(\"_\", maxsplit=4)\n",
    "        item = {\"path\":path,\n",
    "                \"target\":None,\n",
    "                \"family\":family,\n",
    "                \"genus\":genus,\n",
    "                \"species\":species,\n",
    "                \"collection\":collection,\n",
    "                \"catalog_number\":catalog_number}\n",
    "        item[\"target\"] = item[self.class_type]\n",
    "#         return path, family, genus, species, collection, catalog_number\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.config.__repr__()\n",
    "        \n",
    "#     @property\n",
    "#     def config(self):\n",
    "#         return DatasetConfig(self.name,\n",
    "#                              num_files=len(self.files),\n",
    "#                              num_classes=len(self.classes)\n",
    "#                             )\n",
    "    @classmethod\n",
    "    def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "        dataset_dirs = available_datasets[name]\n",
    "        if isinstance(available_datasets[name], str):\n",
    "            dataset_dirs = [available_datasets[name]]\n",
    "        assert isinstance(dataset_dirs, list)\n",
    "        file_list = list(flatten(\n",
    "                            [torchdata.datasets.Files.from_folder(Path(root),\n",
    "                                                                  regex=\"*/*.jpg\").files\n",
    "                             for root in dataset_dirs]\n",
    "                                                    ))\n",
    "        data = FossilDataset(file_list,\n",
    "                             name=name)\n",
    "\n",
    "        return data.map(lambda x: (torchvision.transforms.ToTensor()(x[0]), x[1]))\n",
    "    \n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "#         dataset_dirs = available_datasets[name]\n",
    "#         if isinstance(available_datasets[name], list):\n",
    "#             file_list = list(flatten(\n",
    "#                                 [torchdata.datasets.Files.from_folder(Path(root),\n",
    "#                                                         regex=\"*/*.jpg\").files\n",
    "#                                for root in available_datasets[name]]\n",
    "#                                                             ))\n",
    "            \n",
    "#         elif isinstance(available_datasets[name], str):\n",
    "#             file_list = torchdata.datasets.Files.from_folder(Path(available_datasets[name]),\n",
    "#                                                              regex=\"*/*.jpg\").files\n",
    "\n",
    "#         data = FossilDataset(file_list,\n",
    "#                                  name=name)\n",
    "\n",
    "#         return data.map(torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(fossil_data.samples[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "fossil_data.class2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba96e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf72bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35e42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ImageDataset(torchdata.datasets.Files):\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return Image.open(self.files[index])\n",
    "    \n",
    "# #     def __repr__(self):\n",
    "# #         return f\"\"\"{self.kwargs['name']}\"\"\"\n",
    "# #         return f\"\"\"ImageDataset: {self.kwargs['name']}\"\"\"\n",
    "\n",
    "#     def __init__(self, files: List[Path], *args, **kwargs):\n",
    "#         super().__init__(files=files, *args, **kwargs)\n",
    "        \n",
    "# #         self.name = kwargs.get(\"name\",\"\")\n",
    "# #         self.cfg = DatasetConfig(self.name)\n",
    "\n",
    "\n",
    "#     @classmethod\n",
    "#     def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "#         dataset_dirs = available_datasets[name]\n",
    "\n",
    "#         if isinstance(available_datasets[name], str):\n",
    "#             data = ImageDataset.from_folder(Path(available_datasets[name]),\n",
    "#                                             regex=\"*/*.jpg\",\n",
    "#                                             name=name)\n",
    "#         return data.map(torchvision.transforms.ToTensor())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553f435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e52c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b89cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e12aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74b662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b4358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514c07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c963a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import os\n",
    "import types\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import torch\n",
    "import timm\n",
    "from rich import print\n",
    "import matplotlib.pyplot as plt\n",
    "from contrastive_learning.data.pytorch.pnas import PNASLightningDataModule\n",
    "from contrastive_learning.data.pytorch.extant import ExtantLightningDataModule\n",
    "from contrastive_learning.data.pytorch.common import DataStageError, LeavesLightningDataModule\n",
    "\n",
    "from lightning_hydra_classifiers.callbacks.wandb_callbacks import WatchModelWithWandb, LogPerClassMetricsToWandb, WandbClassificationCallback # LogConfusionMatrixToWandb\n",
    "from lightning_hydra_classifiers.models.resnet import ResNet, get_scalar_metrics\n",
    "import lightning_hydra_classifiers\n",
    "from torch import nn\n",
    "import inspect\n",
    "\n",
    "import wandb\n",
    "pl.trainer.seed_everything(seed=9)\n",
    "\n",
    "    \n",
    "# class Config:\n",
    "#     pass\n",
    "\n",
    "\n",
    "# config = Config()\n",
    "\n",
    "# # config.model_name = 'resnet50'\n",
    "# # config.dataset_name = 'PNAS_family_100_512'\n",
    "# config.dataset_name = '(Extant-PNAS)_family_10_512'\n",
    "# config.normalize = True\n",
    "# config.num_workers = 4\n",
    "# config.batch_size = 16\n",
    "\n",
    "# config = Box({\n",
    "#     \"dataset\":{\n",
    "#         namef\"PNAS_{label_type}_{pnas_threshold}\"\n",
    "#     }\n",
    "    \n",
    "# })\n",
    "\n",
    "########################################\n",
    "# if 'Extant' in config.dataset_name:\n",
    "#     datamodule = ExtantLightningDataModule(name=config.dataset_name, batch_size=config.batch_size, debug=False, normalize=config.normalize, num_workers=config.num_workers)\n",
    "# elif 'PNAS' in config.dataset_name:\n",
    "#     datamodule = PNASLightningDataModule(name=config.dataset_name, batch_size=config.batch_size, debug=False, normalize=config.normalize, num_workers=config.num_workers)#, normalize=False)#True)\n",
    "# datamodule.setup('fit')\n",
    "# ########################################\n",
    "# num_classes = len(datamodule.classes)\n",
    "# config.num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deaf581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from box import Box\n",
    "import os\n",
    "\n",
    "os.environ['WANDB_CACHE_DIR'] = \"/media/data/jacob/wandb_cache\"\n",
    "class_type = \"family\"\n",
    "extant_threshold = 10\n",
    "pnas_threshold = 100\n",
    "image_size = 512\n",
    "seed = 257\n",
    "\n",
    "config = Box({})\n",
    "\n",
    "config.datasets = [{\n",
    "                  \"name\": f\"PNAS_{class_type}_{pnas_threshold}_{image_size}\",\n",
    "                  \"batch_size\":32,\n",
    "                  \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "                  \"num_workers\":4,\n",
    "                  \"image_size\":image_size,\n",
    "                  \"channels\":3,\n",
    "                  \"class_type\":class_type,\n",
    "                  \"debug\":False,\n",
    "                  \"normalize\":True,\n",
    "                  \"seed\":seed,\n",
    "                  \"dataset_dir\":None,\n",
    "                  \"predict_on_split\":\"val\",\n",
    "                  },\n",
    "    {\n",
    "                  \"name\":f\"Extant_{class_type}_{extant_threshold}_{image_size}\",  # f\"PNAS_{label_type}_{pnas_threshold}_{image_size}\"\n",
    "                  \"batch_size\":32,\n",
    "                  \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "                  \"num_workers\":4,\n",
    "                  \"image_size\":image_size,\n",
    "                  \"channels\":3,\n",
    "                  \"class_type\":class_type,\n",
    "                  \"debug\":False,\n",
    "                  \"normalize\":True,\n",
    "                  \"seed\":seed,\n",
    "                  \"dataset_dir\":None,\n",
    "                  \"predict_on_split\":\"val\",\n",
    "                  }]\n",
    "\n",
    "\n",
    "\n",
    "config.wandb = {\n",
    "                \"init\":\n",
    "                       {\n",
    "                        \"entity\":\"jrose\",\n",
    "                        \"project\":\"image_classification_datasets\",\n",
    "                        \"job_type\":'create-dataset',\n",
    "                        \"group\":None,\n",
    "                        \"run_dir\":os.environ['WANDB_CACHE_DIR'],\n",
    "                        \"tags\":[d.name for d in config.datasets]\n",
    "                       },\n",
    "                \"artifacts\":\n",
    "                        {\n",
    "                        \"root_dir\":None\n",
    "                        },\n",
    "                \"input_artifacts\":\n",
    "                       [\n",
    "                           {\n",
    "                            \"entity\":\"jrose\",\n",
    "                            \"project\":\"image_classification_datasets\",\n",
    "                            \"name\": config.datasets[0].name,\n",
    "                            \"version\": \"v6\",\n",
    "                            \"type\": \"raw_data\",\n",
    "                            \"root_dir\":None,\n",
    "                            \"uri\":None\n",
    "                           }\n",
    "                       ]\n",
    "}\n",
    "\n",
    "i = 0\n",
    "\n",
    "config.wandb.artifacts.root_dir = os.path.join(config.wandb.init.run_dir,\n",
    "                                               \"artifacts\")\n",
    "\n",
    "config.wandb.input_artifacts[i].uri = \"/\".join([config.wandb.input_artifacts[i].entity,\n",
    "                                                config.wandb.input_artifacts[i].project,\n",
    "                                                config.wandb.input_artifacts[i].name]) \\\n",
    "                                           + f':{config.wandb.input_artifacts[i].version}'\n",
    "\n",
    "\n",
    "config.wandb.input_artifacts[i].root_dir = os.path.join(config.wandb.artifacts.root_dir,\n",
    "                                                        \"datasets\",\n",
    "                                                         config.wandb.input_artifacts[i].name \\\n",
    "                                                         + f':{config.wandb.input_artifacts[i].version}'\n",
    "                                                        )\n",
    "\n",
    "\n",
    "# def fetch_datamodule_from_dataset_artifact(config: Box, run_or_api=None) -> LeavesLightningDataModule:\n",
    "#     run = run_or_api or wandb.Api()\n",
    "#     artifact = run.use_artifact(config.wandb.input_artifact.uri,\n",
    "#                                 type=config.wandb.input_artifact.type)\n",
    "#     dataset_artifact_dir = artifact.download(root=config.wandb.input_artifact.root_dir)\n",
    "\n",
    "\n",
    "#     datamodule = get_datamodule(config.dataset)\n",
    "#     datamodule.setup('fit')\n",
    "#     datamodule.setup('test')\n",
    "#     ########################\n",
    "#     config.model.num_classes = config.dataset.num_classes\n",
    "\n",
    "def fetch_datamodule_from_dataset_artifact(config: Box, run_or_api=None) -> LeavesLightningDataModule:\n",
    "    run = run_or_api or wandb.Api()\n",
    "    artifact = run.use_artifact(config.wandb.input_artifact.uri,\n",
    "                                type=config.wandb.input_artifact.type)\n",
    "    dataset_artifact_dir = artifact.download(root=config.wandb.input_artifact.root_dir)\n",
    "\n",
    "\n",
    "    datamodule = get_datamodule(config.dataset)\n",
    "    datamodule.setup('fit')\n",
    "    datamodule.setup('test')\n",
    "    ########################\n",
    "    config.model.num_classes = config.dataset.num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageStat\n",
    "import seaborn_image as isns\n",
    "import scipy\n",
    "\n",
    "def image_stat(img: np.ndarray):\n",
    "    if img.ndim==3:\n",
    "        h, w, c = img.shape\n",
    "    else:\n",
    "        h, w, c = (*img.shape, 1)\n",
    "    return {\n",
    "        \"min\":np.min(img),\n",
    "        \"max\":np.max(img),\n",
    "        \"var\":np.var(img),\n",
    "        \"mean\":np.mean(img),\n",
    "        \"mode\":scipy.stats.mode(img,axis=None),\n",
    "        \"height\":h,\n",
    "        \"width\":w,\n",
    "        \"channels\":c,\n",
    "        \"num_pixels\":h*w*c\n",
    "    }\n",
    "\n",
    "\n",
    "def load_and_analyze_image(image_path: str):\n",
    "    img = np.array(Image.open(image_path))\n",
    "    return img, image_stat(img)\n",
    "\n",
    "# def load_analyze_and_save_annotated_image(image_path: str):\n",
    "#     img, stats = load_and_analyze_image(image_path)\n",
    "#     return img, image_stat(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fig2data ( fig ):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a numpy 3D array of RGBA values\n",
    "    \"\"\"\n",
    "    # draw the renderer\n",
    "    fig.canvas.draw()\n",
    " \n",
    "    # Get the RGBA buffer from the figure\n",
    "    w,h = fig.canvas.get_width_height()\n",
    "    buf = np.fromstring(fig.canvas.tostring_argb(), dtype=np.uint8)\n",
    "    buf.shape = ( w, h, 4 )\n",
    " \n",
    "    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n",
    "    buf = np.roll( buf, 3, axis = 2 )\n",
    "    return buf\n",
    "\n",
    " \n",
    "def fig2img ( fig ):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a PIL Image in RGBA format and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a Python Imaging Library ( PIL ) image\n",
    "    \"\"\"\n",
    "    # put the figure pixmap into a numpy array\n",
    "    buf = fig2data(fig)\n",
    "    w, h, d = buf.shape\n",
    "    return Image.frombytes(\"RGBA\", (w, h), buf.tostring())\n",
    "\n",
    "# f = isns.imghist(img_array,\n",
    "#                  describe=True)\n",
    "# results = fig2img(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b926485",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.wandb.input_artifacts[0].uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f3dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def trainvaltest_split(x: Union[List[Any],np.ndarray]=None,\n",
    "#                        y: Union[List[Any],np.ndarray]=None,\n",
    "#                        splits: List[float]=(0.5, 0.2, 0.3),\n",
    "#                        random_state: int=None,\n",
    "#                        stratify: bool=True\n",
    "#                        ) -> Dict[str,Tuple[np.ndarray]]:\n",
    "#     \"\"\"\n",
    "#     Wrapper function to split data into 3 stratified subsets specified by `splits`.\n",
    "    \n",
    "#     User specifies absolute fraction of total requested for each subset (e.g. splits=[0.5, 0.2, 0.3])\n",
    "    \n",
    "#     Function calculates adjusted fractions necessary in order to use sklearn's builtin train_test_split function over a sequence of 2 steps.\n",
    "    \n",
    "#     Step 1: Separate test set from the rest of the data (constituting the union of train + val)\n",
    "    \n",
    "#     Step 2: Separate the train and val sets from the remainder produced by step 1.\n",
    "    \n",
    "    \n",
    "    \n",
    "#     Output:\n",
    "#         Dict: {'train':(x_train, y_train),\n",
    "#                 'val':(x_val_y_val),\n",
    "#                 'test':(x_test, y_test)}\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     assert len(splits) == 3, \"Must provide eactly 3 float values for `splits`\"\n",
    "#     assert np.isclose(np.sum(splits), 1.0), f\"Sum of all splits values {splits} = {np.sum(splits)} must be 1.0\"\n",
    "    \n",
    "#     train_split, val_split, test_split = splits\n",
    "#     val_relative_split = val_split/(train_split + val_split)\n",
    "#     train_relative_split = train_split/(train_split + val_split)\n",
    "    \n",
    "    \n",
    "#     if stratify and (y is None):\n",
    "#         raise ValueError(\"If y is not provided, stratify must be set to False.\")\n",
    "    \n",
    "#     y = np.array(y)\n",
    "#     if x is None:\n",
    "#         x = np.arange(len(y))\n",
    "#     else:\n",
    "#         x = np.array(x)\n",
    "    \n",
    "#     stratify_y = y if stratify else None    \n",
    "#     x_train_val, x_test, y_train_val, y_test = train_test_split(x, y,\n",
    "#                                                         test_size=test_split, \n",
    "#                                                         random_state=random_state,\n",
    "#                                                         stratify=y)\n",
    "#     log.info(f\"(x_train+x_val).shape={x_train_val.shape}, (y_train+y_val).shape={y_train_val.shape}\")\n",
    "#     log.info(f\"x_test.shape={x_test.shape}, y_test.shape={y_test.shape}\")\n",
    "    \n",
    "# #     print(f\"(x_train+x_val).shape={x_train_val.shape}, (y_train+y_val).shape={y_train_val.shape}\")\n",
    "# #     print(f\"x_test.shape={x_test.shape}, y_test.shape={y_test.shape}\")\n",
    "\n",
    "#     stratify_y_train = y_train_val if stratify else None\n",
    "#     x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val,\n",
    "#                                                       test_size=val_relative_split,\n",
    "#                                                       random_state=random_state, \n",
    "#                                                       stratify=y_train_val)\n",
    "    \n",
    "#     x = np.concatenate((x_train, x_val, x_test)).tolist()\n",
    "#     assert len(set(x)) == len(x), f\"[Warning] Check for possible data leakage. len(set(x))={len(set(x))} != len(x)={len(x)}\"\n",
    "    \n",
    "#     log.info(f\"x_train.shape={x_train.shape}, y_train.shape={y_train.shape}\")\n",
    "#     log.info(f\"x_val.shape={x_val.shape}, y_val.shape={y_val.shape}\")\n",
    "    \n",
    "#     log.info(f'Absolute splits: {[train_split, val_split, test_split]}')\n",
    "#     log.info(f'Relative splits: [{train_relative_split:.2f}, {val_relative_split:.2f}, {test_split}]')\n",
    "    \n",
    "#     return {\"train\":(x_train, y_train),\n",
    "#             \"val\":(x_val, y_val),\n",
    "#             \"test\":(x_test, y_test)}\n",
    "\n",
    "#####################\n",
    "\n",
    "# y = data.targets\n",
    "\n",
    "# data_splits = trainvaltest_split(x=None,\n",
    "#                                  y=y,\n",
    "#                                  splits=(0.5, 0.2, 0.3),\n",
    "#                                  random_state=0,\n",
    "#                                  stratify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02634d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(config.wandb.input_artifacts[0].uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a417618",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c5ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d6e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(artifact)\n",
    "# dir(artifact.manifest)\n",
    "# artifact.manifest.entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = artifact.get('dataset/test.table.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.data\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame(data=df, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_samples = data_df\n",
    "samples = list(data_df[['image', 'label']].itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17516b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = len(set(wide_samples.label.values))\n",
    "plt.bar(range(num_classes), wide_samples.groupby(\"label\")[\"catalog_number\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca428e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e659013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29caf26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_mem = data_df.image.apply(lambda x: np.array(x._image))\n",
    "in_mem = data_df.image.apply(lambda x: x._image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43def2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_mem[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12add1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(data_df.image[0]._image).shape\n",
    "\n",
    "\n",
    "in_mem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c61277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "df = data.get_column('image')\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(artifact)\n",
    "\n",
    "downloaded_artifact = artifact.checkout(root=config.wandb.input_artifacts[0].root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.abspath\n",
    "(downloaded_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contrastive_learning.data.pytorch.pnas import PNASLeavesDataset\n",
    "from contrastive_learning.data.pytorch.extant import ExtantLeavesDataset\n",
    "# from contrastive_learning.data.pytorch.common import DataStageError\n",
    "from paleoai_data.dataset_drivers import base_dataset\n",
    "\n",
    "# Step 1. Instantiate PyTorch Datasets for each of Extant Leaves & PNAS, separately\n",
    "pnas_torch = PNASLeavesDataset(name = f\"PNAS_{label_type}_{pnas_threshold}\",\n",
    "                 split: str=\"train\",\n",
    "                 dataset_dir: Optional[str]=None,\n",
    "                 return_paths: bool=False,)\n",
    "extant_torch = ExtantLeavesDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def create_dataset_by_name(name: str,\n",
    "#                            version: str='v0.2',\n",
    "#                            exclude_classes = ['notcataloged','notcatalogued', 'II. IDs, families uncertain', 'Unidentified']):\n",
    "#     data_df = query_db(version=version, **{'dataset':name})\n",
    "#     dataset = base_dataset.BaseDataset.from_dataframe(df=data_df, name=name, exclude_classes=exclude_classes)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=WANDB_PROJECT, job_type=\"model_result_analysis\") as run:\n",
    "    \n",
    "    # Retrieve the original raw dataset\n",
    "    dataset_artifact = run.use_artifact(\"raw_data:latest\")\n",
    "    data_table = dataset_artifact.get(\"raw_examples\")\n",
    "    \n",
    "    # Retrieve the train and test score tables\n",
    "    train_artifact = run.use_artifact(\"train_results:latest\")\n",
    "    train_table = train_artifact.get(\"train_iou_score_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = PNASLightningDataModule(batch_size=16)\n",
    "# data = ExtantLightningDataModule(batch_size=16, num_workers=12)\n",
    "# data.setup(stage='fit')\n",
    "\n",
    "# data.setup(stage='test')\n",
    "\n",
    "# data.setup(stage=None)\n",
    "\n",
    "# try:\n",
    "#     data.setup(stage='other')\n",
    "#     print('success')\n",
    "# except DataStageError as e:\n",
    "#     print(e.with_traceback(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.setup(stage='fit')\n",
    "train_dataloader = data.get_dataloader(stage='train')\n",
    "val_dataloader = data.get_dataloader(stage='val')\n",
    "data.setup(stage='test')\n",
    "test_dataloader = data.get_dataloader(stage='test')\n",
    "\n",
    "# train_dataloader\n",
    "#         if stage=='train': return self.train_dataloader()\n",
    "#         if stage=='val': return self.val_dataloader()\n",
    "#         if stage=='test': return self.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data.train_dataset.transform = None #data.default_train_transforms() #None\n",
    "x, y = data.train_dataset[0]\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import ImageOps\n",
    "\n",
    "# print(x.max(), x.min())\n",
    "# plt.imshow(ImageOps.invert(x))#.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_idx = 0\n",
    "\n",
    "data.show_batch('train', batch_idx=batch_idx)\n",
    "# data.show_batch('train', cmap='plasma')\n",
    "plt.savefig(f'ExtantLeaves v0_3 train batch {batch_idx}.png')\n",
    "\n",
    "data.show_batch('val', batch_idx=batch_idx)\n",
    "plt.savefig(f'ExtantLeaves v0_3 val batch {batch_idx}.png')\n",
    "\n",
    "data.show_batch('test', batch_idx=batch_idx)\n",
    "plt.savefig(f'ExtantLeaves v0_3 test batch {batch_idx}.png')\n",
    "# data.show_batch('train', cmap='magma')\n",
    "# data.show_batch('train', cmap='cividis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4f5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b209d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = data\n",
    "stage = 'test'\n",
    "batch_idx = 0\n",
    "\n",
    "x, y = self.get_batch(stage=stage, batch_idx=batch_idx)\n",
    "\n",
    "x = x[:12,...]\n",
    "\n",
    "batch_size = x.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(24,24))\n",
    "grid_img = torchvision.utils.make_grid(x, nrow=int(np.ceil(np.sqrt(batch_size))))\n",
    "\n",
    "img_min, img_max = grid_img.min(), grid_img.max()\n",
    "print(img_min, img_max)\n",
    "\n",
    "grid_img = (grid_img - img_min)/(img_max - img_min)\n",
    "img_min, img_max = grid_img.min(), grid_img.max()\n",
    "print(img_min, img_max)\n",
    "\n",
    "\n",
    "\n",
    "print('before:', grid_img.shape)\n",
    "\n",
    "if torch.argmin(torch.Tensor(grid_img.shape)) == 0:\n",
    "    grid_img = grid_img.permute(1,2,0)\n",
    "print('after:', grid_img.shape)\n",
    "\n",
    "img_ax = ax.imshow(grid_img[:,:,0], cmap='viridis')#, vmin = img_min, vmax = img_max)\n",
    "fig.colorbar(img_ax, ax=ax)#)#cax=ax)\n",
    "plt.axis('off')\n",
    "plt.suptitle(f'{stage} batch')\n",
    "#         return fig, ax\n",
    "\n",
    "help(plt.imshow)\n",
    "\n",
    "%debug\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "\n",
    "x.min()\n",
    "\n",
    "plt.imshow(x[1,...].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79726ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('sequoia': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "1c0686ae52b501c5138d1ad3c292b1aad199ba0a61e288abba1616901d57bf21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}