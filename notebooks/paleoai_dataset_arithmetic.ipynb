{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059978ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PaleoAI Dataset Arithmetic\n",
    "\n",
    "`paleoai_dataset_arithmetic.ipynb`\n",
    "\n",
    "This notebook is for using pandas dataframes for elegantly adding or subtracting sets of data samples while maintaining unique-specimen constraints based on enforcing uniqueness for a user-specified id column. Using simple operator overloading in Python class definitions, we can perform complex queries with minimal boilerplate.\n",
    "\n",
    "Author: Jacob A Rose  \n",
    "Created on: Monday July 19th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d1fd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7632b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module;function;builtin_function_or_method;ABCMeta;type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d49c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list\r\n",
    "# !pip list\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "data = np.random.randint(0, 100, size=(100, 4))\r\n",
    "\r\n",
    "print(data)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "test = \"won\"\r\n",
    "print(test)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2223fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def left_union(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\", suffixes=(\"_x\", \"_y\")) -> pd.DataFrame:\r\n",
    "#     \"\"\"\r\n",
    "#     Return a new dataframe containing all rows from `data_df`, concatenated with any rows that only exist in `other_df`. Any rows that are shared between the 2 default to only including the values from `data_df`.\r\n",
    "    \r\n",
    "#     \"\"\"\r\n",
    "#     return data_df.merge(other_df, how='outer', on=id_col, suffixes=suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1af6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "\r\n",
    "def intersection(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\", suffixes=(\"_x\", \"_y\")) -> pd.DataFrame:\r\n",
    "    \"\"\"\r\n",
    "    Return a new dataframe containing only rows that share the same values for `id_col` between `data_df` and `other_df`\r\n",
    "    \r\n",
    "    Equivalent to an AND join between sets\r\n",
    "    \"\"\"\r\n",
    "    return data_df.merge(other_df, how='inner', on=id_col, suffixes=suffixes)\r\n",
    "\r\n",
    "\r\n",
    "def left_exclusive(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\") -> pd.DataFrame:\r\n",
    "    \"\"\"\r\n",
    "    Return a new dataframe containing only rows from `data_df` that do not share an `id_col` value with any row from `other_df`.\r\n",
    "    \r\n",
    "    Equivalent to subtracting the set of `id_col` values in `other_df` from `data_df`\r\n",
    "    \"\"\"\r\n",
    "    omit = list(other_df[id_col].values)\r\n",
    "    \r\n",
    "    return data_df[data_df[id_col].apply(lambda x: x not in omit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extant_df = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Extant-dataset_leavesdb-v0_3.csv\", index_col=0)\r\n",
    "# pnas_train = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100/train.csv\")\r\n",
    "# pnas_test = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100/test.csv\")\r\n",
    "# pnas_df = pd.concat([pnas_train, pnas_test])\r\n",
    "\r\n",
    "# extant_in_pnas = intersection(data_df=extant_df,\r\n",
    "#                               other_df=pnas_df,\r\n",
    "#                               id_col=\"catalog_number\",\r\n",
    "#                               suffixes=(\"_extant\", \"_pnas\"))\r\n",
    "\r\n",
    "# # In order to only keep original columns\r\n",
    "# suffixes=(\"_extant\", \"_pnas\")\r\n",
    "# extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\r\n",
    "# extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extant_minus_pnas -> rows exclusive to extant dataset\r\n",
    "# extant_minus_pnas = left_exclusive(data_df=extant_df,\r\n",
    "#                                    other_df=pnas_df,\r\n",
    "#                                    id_col=\"catalog_number\",\r\n",
    "#                                    suffixes=(\"_extant\", \"_pnas\"))\r\n",
    "\r\n",
    "# # pnas_minus_extant -> rows exclusive to pnas dataset\r\n",
    "# pnas_minus_extant = left_exclusive(data_df=pnas_df,\r\n",
    "#                                    other_df=extant_df,\r\n",
    "#                                    id_col=\"catalog_number\",\r\n",
    "#                                    suffixes=(\"_pnas\", \"_extant\"))\r\n",
    "\r\n",
    "# extant_minus_pnas = left_exclusive(data_df=extant_df, other_df=pnas_df, id_col=\"catalog_number\", suffixes=(\"_extant\", \"_pnas\"))\r\n",
    "\r\n",
    "# pnas_minus_extant = left_exclusive(data_df=pnas_df, other_df=extant_df, id_col=\"catalog_number\", suffixes=(\"_pnas\", \"_extant\"))\r\n",
    "\r\n",
    "# extant_and_pnas = left_union(data_df=extant_df, other_df=pnas_df, id_col=\"catalog_number\", suffixes=(\"_extant\", \"_pnas\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4bd07d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07512ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\r\n",
    "import os.path\r\n",
    " \r\n",
    "def initialize_logger():\r\n",
    "    logger = logging.getLogger()\r\n",
    "    logger.setLevel(logging.DEBUG)\r\n",
    "     \r\n",
    "    # create console handler and set level to info\r\n",
    "    handler = logging.StreamHandler()\r\n",
    "    handler.setLevel(logging.INFO)\r\n",
    "    formatter = logging.Formatter(\"%(levelname)s - %(message)s\")\r\n",
    "    handler.setFormatter(formatter)\r\n",
    "    logger.addHandler(handler)\r\n",
    "\r\n",
    "initialize_logger()\r\n",
    "    \r\n",
    "import torchdata\r\n",
    "from typing import Union, List, Any, Tuple\r\n",
    "# from collections import Counter\r\n",
    "from lightning_hydra_classifiers.utils import template_utils\r\n",
    "from lightning_hydra_classifiers.utils.common_utils import trainvaltest_split\r\n",
    "import collections\r\n",
    "from omegaconf import OmegaConf, DictConfig\r\n",
    "from lightning_hydra_classifiers.data.common import CommonDataSelect, CommonDataset, LeavesLightningDataModule\r\n",
    "from lightning_hydra_classifiers.data import fossil, extant, pnas\r\n",
    "from rich import print as pp\r\n",
    "import os\r\n",
    "\r\n",
    "from typing import *\r\n",
    "from pathlib import Path\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from IPython.display import display\r\n",
    "\r\n",
    "log = template_utils.get_logger(__name__, level=logging.DEBUG)\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "pd.set_option('display.max_rows', 500)\r\n",
    "pd.set_option('display.max_columns', 500)\r\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eae5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\"\r\n",
    "\r\n",
    "from hydra.experimental import compose, initialize, initialize_config_dir\r\n",
    "from omegaconf import OmegaConf, DictConfig\r\n",
    "os.chdir(config_dir)\r\n",
    "print(f\"cwd = {os.getcwd()}\")\r\n",
    "\r\n",
    "def initialize_config(config_dir: str,\r\n",
    "                      overrides=None):\r\n",
    "    with initialize_config_dir(config_dir=config_dir, job_name=\"multi-gpu_experiment\"):\r\n",
    "\r\n",
    "        cfg = compose(config_name=\"multi-gpu\", overrides=overrides)\r\n",
    "        OmegaConf.set_struct(cfg, False)\r\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.utils.common_utils import LabelEncoder, trainval_split\r\n",
    "from lightning_hydra_classifiers.data.common import CommonDataset, LeavesLightningDataModule, plot_split_distributions\r\n",
    "# default_config = initialize_config(config_dir=config_dir,\r\n",
    "#                                    overrides=[\"datamodule=default_datamodule\"])\r\n",
    "# config = DictConfig({\"datamodule.dataset.name\":\"Extant_family_10_minus_PNAS_family_100_512\"})\r\n",
    "# default_config = DictConfig({'datamodule':LeavesLightningDataModule.default_config()})\r\n",
    "\r\n",
    "# pp(OmegaConf.to_container(datamodule.datamodule_config, resolve=True))\r\n",
    "# default_config = DictConfig({'datamodule':LeavesLightningDataModule.default_config()})\r\n",
    "# user_config = DictConfig({\"datamodule\":\r\n",
    "#                               {\"dataset\":\r\n",
    "#                                    {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\r\n",
    "#                               }\r\n",
    "#                          })\r\n",
    "# pp(OmegaConf.to_container(OmegaConf.merge(default_config, user_config), resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a27d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_1024_minus_PNAS_family_100_1024\"\r\n",
    "\r\n",
    "#         config = DictConfig({\"dataset\":\r\n",
    "#                                        {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\r\n",
    "#                             })\r\n",
    "# config.dataset.config.name = \"Extant_family_10_1024_in_PNAS_family_100_1024\"\r\n",
    "datamodule = LeavesLightningDataModule(config=None, #config, #default_config,\r\n",
    "                                       data_dir=output_dir)\r\n",
    "config.hparams.classes = datamodule.classes\r\n",
    "config.hparams.num_classes = len(config.hparams.classes)\r\n",
    "config.dataset.config.classes = datamodule.classes\r\n",
    "config.dataset.config.num_classes = len(config.hparams.classes)\r\n",
    "\r\n",
    "\r\n",
    "data_loader = datamodule.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eedd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca99f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7a7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e50a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'family'\r\n",
    "seed = 5687\r\n",
    "val_train_split = 0.2\r\n",
    "\r\n",
    "# pnas_name = \"PNAS_family_100_512\"\r\n",
    "# extant_name = \"Extant_family_10_512\"\r\n",
    "\r\n",
    "pnas_name = \"PNAS_family_100_1024\"#512\"\r\n",
    "extant_name = \"Extant_family_10_1024\" #512\"\r\n",
    "\r\n",
    "\r\n",
    "## Load primary Extant and PNAS datamodules\r\n",
    "pnas_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=pnas_dataset\",\r\n",
    "                                  \"datamodule=standalone_datamodule\"])\r\n",
    "# pnas_cfg[f\"dataset.config.name\"] = pnas_name\r\n",
    "\r\n",
    "pnas_cfg.datamodule.config.dataset.name = pnas_name\r\n",
    "pp(OmegaConf.to_container(pnas_cfg, resolve=False))\r\n",
    "\r\n",
    "pnas_datamodule = LeavesLightningDataModule(pnas_cfg)#.datamodule.config)\r\n",
    "\r\n",
    "extant_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=extant_dataset\",\r\n",
    "                                   \"datamodule=standalone_datamodule\"])\r\n",
    "# pnas_cfg[f\"dataset.config.name\"] = pnas_name\r\n",
    "\r\n",
    "extant_cfg.datamodule.config.dataset.name = extant_name\r\n",
    "extant_datamodule = LeavesLightningDataModule(extant_cfg) #.datamodule.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7894265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\r\n",
    "\r\n",
    "dataset_name = f\"{extant_name}_minus_{pnas_name}\" # Extant_family_10_512_minus_PNAS_family_100_512\r\n",
    "test_dataset_name = f\"{extant_name}_in_{pnas_name}\"\r\n",
    "output_dir = f\"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/{dataset_name}\"\r\n",
    "\r\n",
    "#########################################\r\n",
    "\r\n",
    "extant_dataset = extant_datamodule.dataset\r\n",
    "pnas_dataset = pnas_datamodule.dataset\r\n",
    "extant_df = extant_dataset.samples_df\r\n",
    "pnas_df = pnas_dataset.samples_df\r\n",
    "\r\n",
    "#########################################\r\n",
    "#########################################\r\n",
    "\r\n",
    "# extant_minus_pnas -> rows exclusive to extant dataset\r\n",
    "extant_minus_pnas = left_exclusive(data_df=extant_df,\r\n",
    "                                   other_df=pnas_df,\r\n",
    "                                   id_col=\"catalog_number\")\r\n",
    "# extant_in_pnas -> rows from extant dataset that share a catalog_number with PNAS\r\n",
    "extant_in_pnas = intersection(data_df=extant_df,\r\n",
    "                              other_df=pnas_df,\r\n",
    "                              id_col=\"catalog_number\",\r\n",
    "                              suffixes=(\"_extant\", \"_pnas\"))\r\n",
    "\r\n",
    "suffixes=(\"_extant\", \"_pnas\")\r\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\r\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})\r\n",
    "\r\n",
    "#########################################\r\n",
    "### GENERATE TRAIN VAL SPLIT INDICES\r\n",
    "#########################################\r\n",
    "\r\n",
    "y = extant_minus_pnas[y_col]\r\n",
    "data_splits = trainval_split(x=None,\r\n",
    "                             y=y,\r\n",
    "                             val_train_split=val_train_split,\r\n",
    "                             random_state=seed,\r\n",
    "                             stratify=True\r\n",
    "                             )\r\n",
    "train_idx, val_idx = data_splits['train'][0], data_splits['val'][0]\r\n",
    "\r\n",
    "#########################################\r\n",
    "### CREATE COMMONDATASETS FROM DATAFRAMES, USING THE SPLIT INDICES\r\n",
    "#########################################\r\n",
    "\r\n",
    "\r\n",
    "train_df = extant_minus_pnas.iloc[train_idx,:]\r\n",
    "val_df = extant_minus_pnas.iloc[val_idx,:]\r\n",
    "\r\n",
    "train_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\r\n",
    "                                                               sample_df=train_df,\r\n",
    "                                                               config=None,\r\n",
    "                                                               return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                               subset_key=\"train\")\r\n",
    "\r\n",
    "val_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\r\n",
    "                                                             sample_df=val_df,\r\n",
    "                                                             config=None,\r\n",
    "                                                             return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                             subset_key=\"val\")\r\n",
    "\r\n",
    "test_dataset_extant_in_pnas = CommonDataset.from_dataframe(sample_df=extant_in_pnas,\r\n",
    "                                                            config=None,\r\n",
    "                                                            return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                            subset_key=\"test\")\r\n",
    "\r\n",
    "data_splits= {'train':train_dataset_extant_minus_pnas,\r\n",
    "              'val': val_dataset_extant_minus_pnas,\r\n",
    "              'test':test_dataset_extant_in_pnas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ece2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits['train'].config.name = dataset_name\r\n",
    "data_splits['train'].config.subset_key = \"train\"\r\n",
    "\r\n",
    "data_splits['val'].config.name = dataset_name\r\n",
    "data_splits['val'].config.subset_key = \"val\"\r\n",
    "\r\n",
    "data_splits['test'].config.name = test_dataset_name\r\n",
    "data_splits['test'].config.subset_key = \"test\"\r\n",
    "\r\n",
    "#########################################\r\n",
    "### LABEL ENCODER\r\n",
    "#########################################\r\n",
    "\r\n",
    "replace = {\"Nothofagaceae\": \"Fagaceae\"}\r\n",
    "label_encoder = LabelEncoder(replace=replace) # class2idx)\r\n",
    "label_encoder.fit(data_splits[\"test\"].targets)\r\n",
    "label_encoder.fit(data_splits[\"train\"].targets)\r\n",
    "for d in list(data_splits.values()):\r\n",
    "    d.label_encoder = label_encoder\r\n",
    "\r\n",
    "    \r\n",
    "#########################################\r\n",
    "### EXPORT DATASET CONFIGURATION TO A COMBO OF CSV, JSON, YAML, AND JPG FILES.\r\n",
    "#########################################\r\n",
    "from lightning_hydra_classifiers.data.common import export_dataset_to_csv, import_dataset_from_csv\r\n",
    "# output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_512_minus_PNAS_family_100_512\"\r\n",
    "export_dataset_to_csv(data_splits=data_splits,\r\n",
    "                          label_encoder=label_encoder,\r\n",
    "                          output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca54d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_data_splits, conf = import_dataset_from_csv(data_catalog_dir = output_dir)\r\n",
    "# print(conf)\r\n",
    "# for k,v in loaded_data_splits.items():\r\n",
    "#     print(k, repr(v))\r\n",
    "\r\n",
    "\r\n",
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_1024_minus_PNAS_family_100_1024\"\r\n",
    "\r\n",
    "#         config = DictConfig({\"dataset\":\r\n",
    "#                                        {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\r\n",
    "#                             })\r\n",
    "# config.dataset.config.name = \"Extant_family_10_1024_in_PNAS_family_100_1024\"\r\n",
    "datamodule = LeavesLightningDataModule(config=None, #config, #default_config,\r\n",
    "                                       data_dir=output_dir)\r\n",
    "config.hparams.classes = datamodule.classes\r\n",
    "config.hparams.num_classes = len(config.hparams.classes)\r\n",
    "config.dataset.config.classes = datamodule.classes\r\n",
    "config.dataset.config.num_classes = len(config.hparams.classes)\r\n",
    "\r\n",
    "\r\n",
    "data_loader = datamodule.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e0f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc7092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_1024_minus_PNAS_family_100_1024\"\r\n",
    "\r\n",
    "config = DictConfig({\"dataset\":\r\n",
    "                               {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\r\n",
    "                    })\r\n",
    "datamodule = LeavesLightningDataModule(config=config, #default_config,\r\n",
    "                                       data_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c165571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def left_exclusive(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\") -> pd.DataFrame:\r\n",
    "#     \"\"\"\r\n",
    "#     Return a new dataframe containing only rows from `data_df` that do not share an `id_col` value with any row from `other_df`.\r\n",
    "    \r\n",
    "#     Equivalent to subtracting the set of `id_col` values in `other_df` from `data_df`\r\n",
    "#     \"\"\"\r\n",
    "#     omit = list(other_df[id_col].values)\r\n",
    "    \r\n",
    "#     return data_df[data_df[id_col].apply(lambda x: x not in omit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff96a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnas_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=pnas_dataset\"])\r\n",
    "pnas_data = LeavesLightningDataModule(pnas_cfg.datamodule.config)\r\n",
    "\r\n",
    "extant_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=extant_dataset\"])\r\n",
    "extant_data = LeavesLightningDataModule(extant_cfg.datamodule.config)\r\n",
    "\r\n",
    "extant_dataset = extant_data.dataset\r\n",
    "pnas_dataset = pnas_data.dataset\r\n",
    "\r\n",
    "print(len(extant_dataset), len(pnas_dataset))\r\n",
    "\r\n",
    "extant_df = extant_dataset.samples_df\r\n",
    "pnas_df = pnas_dataset.samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extant_minus_pnas -> rows exclusive to extant dataset\r\n",
    "extant_minus_pnas = left_exclusive(data_df=extant_df,\r\n",
    "                                   other_df=pnas_df,\r\n",
    "                                   id_col=\"catalog_number\")\r\n",
    "\r\n",
    "# pnas_minus_extant -> rows exclusive to pnas dataset\r\n",
    "# pnas_minus_extant = left_exclusive(data_df=pnas_df,\r\n",
    "#                                    other_df=extant_df,\r\n",
    "#                                    id_col=\"catalog_number\")\r\n",
    "\r\n",
    "\r\n",
    "# print(extant_minus_pnas.shape, pnas_minus_extant.shape)\r\n",
    "\r\n",
    "# pnas_in_extant = intersection(data_df=pnas_df,\r\n",
    "#                               other_df=extant_df,\r\n",
    "#                               id_col=\"catalog_number\",\r\n",
    "#                               suffixes=(\"_pnas\", \"_extant\"))\r\n",
    "\r\n",
    "extant_in_pnas = intersection(data_df=extant_df,\r\n",
    "                              other_df=pnas_df,\r\n",
    "                              id_col=\"catalog_number\",\r\n",
    "                              suffixes=(\"_extant\", \"_pnas\"))\r\n",
    "\r\n",
    "\r\n",
    "# print(extant_in_pnas.shape, pnas_in_extant.shape)\r\n",
    "\r\n",
    "suffixes=(\"_extant\", \"_pnas\")\r\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\r\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})\r\n",
    "\r\n",
    "extant_in_pnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'family'\r\n",
    "seed = 5687\r\n",
    "val_train_split = 0.2\r\n",
    "\r\n",
    "y = extant_minus_pnas[y_col]\r\n",
    "\r\n",
    "data_splits = trainval_split(x=None,\r\n",
    "                             y=y,\r\n",
    "                               val_train_split=val_train_split,\r\n",
    "                               random_state=seed,\r\n",
    "                               stratify=True\r\n",
    "                               )\r\n",
    "\r\n",
    "data_splits['train'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = data_splits['train'][0], data_splits['val'][0]\r\n",
    "\r\n",
    "train_df = extant_minus_pnas.iloc[train_idx,:]\r\n",
    "val_df = extant_minus_pnas.iloc[val_idx,:]\r\n",
    "\r\n",
    "train_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\r\n",
    "                                                               sample_df=train_df,\r\n",
    "                                                               config=None,\r\n",
    "                                                               return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                               subset_key=\"train\")\r\n",
    "#                                                                subset_key=None) #\"train\")\r\n",
    "\r\n",
    "\r\n",
    "val_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\r\n",
    "                                                             sample_df=val_df,\r\n",
    "                                                             config=None,\r\n",
    "                                                             return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                             subset_key=\"val\")\r\n",
    "\r\n",
    "\r\n",
    "test_dataset_extant_in_pnas = CommonDataset.from_dataframe(sample_df=extant_in_pnas,\r\n",
    "                                                            config=None,\r\n",
    "                                                            return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                            subset_key=\"test\")\r\n",
    "\r\n",
    "data_splits= {'train':train_dataset_extant_minus_pnas,\r\n",
    "              'val': val_dataset_extant_minus_pnas,\r\n",
    "              'test':test_dataset_extant_in_pnas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in data_splits.items():    \r\n",
    "#     print(k, v.__repr__())\r\n",
    "\r\n",
    "# import_dataset_from_csv(self, data_dir: str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78b15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ecb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a41739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OmegaConf.to_container(test_dataset_extant_in_pnas.config, resolve=True)\r\n",
    "# label_encoder = LabelEncoder() # class2idx)\r\n",
    "# label_encoder.fit(data_splits[\"train\"].targets)\r\n",
    "\r\n",
    "# for d in list(data_splits.values()):\r\n",
    "#     d.label_encoder = label_encoder\r\n",
    "# test_dataset_extant_in_pnas.label_encoder\r\n",
    "# label_encoder = LabelEncoder() # class2idx)\r\n",
    "# label_encoder.fit(data_splits[\"test\"].targets)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# test_dataset_extant_in_pnas.label_encoder\r\n",
    "# label_encoder\r\n",
    "\r\n",
    "# test_df = data_splits[\"test\"].samples_df\r\n",
    "\r\n",
    "# replace = {\"Nothofagaceae\": \"Fagaceae\"}\r\n",
    "# label_encoder = LabelEncoder(replace=replace) # class2idx)\r\n",
    "# label_encoder.fit(data_splits[\"test\"].targets)\r\n",
    "\r\n",
    "# label_encoder.fit(data_splits[\"train\"].targets)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# for d in list(data_splits.values()):\r\n",
    "#     d.label_encoder = label_encoder\r\n",
    "    \r\n",
    "# label_encoder\r\n",
    "# test_df[test_df.family==\"Nothofagaceae\"].replace(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import *\r\n",
    "# from omegaconf import DictConfig\r\n",
    "# from pathlib import Path\r\n",
    "# import matplotlib.pyplot as plt\r\n",
    "# pnas_df[pnas_df.catalog_number==\"Wolfe_8535\"]\r\n",
    "\r\n",
    "# set(data_splits[\"test\"].samples_df.family.astype(pd.CategoricalDtype()).cat.categories) - set(pnas_df.family.astype(pd.CategoricalDtype()).cat.categories)\r\n",
    "\r\n",
    "# test_dataset_extant_in_pnas.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(config: DictConfig, config_path: str):\r\n",
    "    with open(config_path, \"w\") as f:\r\n",
    "        f.write(OmegaConf.to_yaml(config, resolve=True))\r\n",
    "\r\n",
    "def load_config(config_path: str) -> DictConfig:    \r\n",
    "    with open(config_path, \"r\") as f:\r\n",
    "        loaded = OmegaConf.load(f)\r\n",
    "    return loaded\r\n",
    "\r\n",
    "\r\n",
    "def export_image_data_diagnostics(data_splits: Dict[str,CommonDataset],\r\n",
    "                                  output_dir: str='.',\r\n",
    "                                  max_samples: int = 64,\r\n",
    "                                  export_sample_images: bool=True,\r\n",
    "                                  export_class_distribution_plots: bool=True) -> Dict[str,str]:\r\n",
    "    image_paths = {\"images\": {},\r\n",
    "                   \"class_distribution_plots\":{}}\r\n",
    "    \r\n",
    "    image_dir = os.path.join(output_dir, \"images\")\r\n",
    "    plot_dir = os.path.join(output_dir, \"plots\")\r\n",
    "    os.makedirs(image_dir, exist_ok = True)\r\n",
    "    os.makedirs(plot_dir, exist_ok = True)\r\n",
    "\r\n",
    "    if export_sample_images:\r\n",
    "#         subsets = ['train', 'val', 'test']\r\n",
    "        for subset in data_splits.keys():\r\n",
    "            fig, ax = data_splits[subset].show_batch(indices=max_samples, include_colorbar=False,\r\n",
    "                                                     suptitle = f\"subset: {subset}, {max_samples} random images\")\r\n",
    "            img_path = os.path.join(image_dir, f\"subset: {subset}, {max_samples} random images.jpg\")\r\n",
    "            image_paths[\"images\"][subset] = img_path\r\n",
    "            plt.savefig(img_path)\r\n",
    "\r\n",
    "    if export_class_distribution_plots:\r\n",
    "        fig, ax = plot_split_distributions(data_splits=data_splits)\r\n",
    "        class_distribution_plot_path = os.path.join(plot_dir, f\"class_distribution_plots_{[subset for subset in data_splits.keys()]}\")\r\n",
    "        image_paths[\"class_distribution_plots\"][\"all\"] = class_distribution_plot_path\r\n",
    "        plt.savefig(class_distribution_plot_path)\r\n",
    "\r\n",
    "    return image_paths\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def export_dataset_to_csv(data_splits: Dict[str,CommonDataset],\r\n",
    "                          label_encoder: Optional[LabelEncoder]=None,\r\n",
    "                          output_dir: str='.',\r\n",
    "                          export_sample_images: bool=True,\r\n",
    "                          export_class_distribution_plots: bool=True) -> Dict[str,str]:\r\n",
    "    output_paths = {\"tables\":{},\r\n",
    "                    \"class_labels\":{},\r\n",
    "                    \"configs\":{}}\r\n",
    "    os.makedirs(output_dir, exist_ok=True)\r\n",
    "    for k, data in data_splits.items():\r\n",
    "        subset_data_path = os.path.join(output_dir, f\"{k}_data_table.csv\")\r\n",
    "        data.samples_df.to_csv(subset_data_path)\r\n",
    "        output_paths[\"tables\"][k] = subset_data_path\r\n",
    "        \r\n",
    "        if hasattr(data, \"config\"):\r\n",
    "            subset_config_path = os.path.join(output_dir, f\"{k}_config.yaml\")\r\n",
    "            save_config(config=data.config, config_path=subset_config_path)\r\n",
    "            output_paths[\"configs\"][k] = subset_config_path\r\n",
    "        \r\n",
    "        if hasattr(data, 'label_encoder') and (label_encoder is None):\r\n",
    "            subset_label_path = os.path.join(output_dir, k + \"_label_encoder.json\")\r\n",
    "            data.label_encoder.save(subset_label_path)\r\n",
    "            output_paths[\"class_labels\"][k] = subset_data_path\r\n",
    "            \r\n",
    "    if label_encoder is not None:\r\n",
    "        full_label_encoder_path = os.path.join(output_dir, \"label_encoder.json\")\r\n",
    "        label_encoder.save(full_label_encoder_path)\r\n",
    "        output_paths[\"class_labels\"][\"full\"] = full_label_encoder_path\r\n",
    "\r\n",
    "    \r\n",
    "    export_image_data_diagnostics(data_splits=data_splits,\r\n",
    "                                  output_dir=output_dir,\r\n",
    "                                  max_samples = 64,\r\n",
    "                                  export_sample_images=export_sample_images,\r\n",
    "                                  export_class_distribution_plots=export_class_distribution_plots)\r\n",
    "        \r\n",
    "    return output_paths\r\n",
    "    \r\n",
    "\r\n",
    "def import_dataset_from_csv(data_catalog_dir: str) -> Dict[str, CommonDataset]:\r\n",
    "    \r\n",
    "    data_paths = list(Path(data_catalog_dir).glob(\"*.csv\"))\r\n",
    "    config_paths = list(Path(data_catalog_dir).glob(\"*.yaml\"))\r\n",
    "    label_encoder_paths = list(Path(data_catalog_dir).glob(\"*.json\"))\r\n",
    "    \r\n",
    "    assert len(data_paths) == len(config_paths)\r\n",
    "    input_paths = {\"tables\":{},\r\n",
    "                   \"class_labels\":{},\r\n",
    "                   \"configs\":{}}\r\n",
    "    subsets = [\"train\", \"val\", \"test\"]\r\n",
    "    for subset in subsets:\r\n",
    "        input_paths[\"tables\"][subset] = [p for p in data_paths if p.stem.startswith(subset)][0]\r\n",
    "        input_paths[\"configs\"][subset] = [p for p in config_paths if p.stem.startswith(subset)][0]\r\n",
    "    \r\n",
    "    if len(label_encoder_paths) == 1:\r\n",
    "        label_encoder = LabelEncoder.load(label_encoder_paths[0])\r\n",
    "    else:\r\n",
    "        raise(f'Currently cannot distinguish between multiple label_encoders, please delete all but 1 in experiment directory. Contents: {label_encoder_paths}')\r\n",
    "    \r\n",
    "    data_splits = {}\r\n",
    "    for subset in subsets:\r\n",
    "        sample_df = pd.read_csv(input_paths[\"tables\"][subset])\r\n",
    "        config = load_config(input_paths[\"configs\"][subset])\r\n",
    "        data_splits[subset] = CommonDataset.from_dataframe(sample_df,\r\n",
    "                                                           config=config)\r\n",
    "        data_splits[subset].label_encoder = label_encoder\r\n",
    "        \r\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_d = data_splits['train']\r\n",
    "\r\n",
    "\r\n",
    "# save_config(config=config, config_path=subset_config_path)\r\n",
    "# loaded = load_config(config_path=subset_config_path)\r\n",
    "\r\n",
    "# pp(OmegaConf.to_container(config, resolve=True))\r\n",
    "\r\n",
    "# pp(OmegaConf.to_container(loaded))\r\n",
    "\r\n",
    "# pp(data_paths, config_paths, label_encoder_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59eba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = {\"Nothofagaceae\": \"Fagaceae\"}\r\n",
    "label_encoder = LabelEncoder(replace=replace) # class2idx)\r\n",
    "label_encoder.fit(data_splits[\"test\"].targets)\r\n",
    "label_encoder.fit(data_splits[\"train\"].targets)\r\n",
    "for d in list(data_splits.values()):\r\n",
    "    d.label_encoder = label_encoder\r\n",
    "    \r\n",
    "label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_512_minus_PNAS_family_100_512\"\r\n",
    "\r\n",
    "export_dataset_to_csv(data_splits=data_splits,\r\n",
    "                          label_encoder=label_encoder,\r\n",
    "                          output_dir=output_dir)\r\n",
    "\r\n",
    "loaded_data_splits = import_dataset_from_csv(data_catalog_dir = output_dir)\r\n",
    "\r\n",
    "for k,v in loaded_data_splits.items():\r\n",
    "    print(k, repr(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86eae05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa98d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bde27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49720e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "plt.style.available\r\n",
    "\r\n",
    "# plt.style.use(\"seaborn-notebook\")\r\n",
    "plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self = data_splits['train']\r\n",
    "# import torch\r\n",
    "# import numpy as np\r\n",
    "# # indices = [0,1, 2,3,4,5]\r\n",
    "# indices = np.array(indices)\r\n",
    "# batch = [self[idx] for idx in indices]\r\n",
    "\r\n",
    "# # y = [batch[idx][1] for idx in indices]\r\n",
    "# y = torch.Tensor(np.array([(item[1]) for item in batch])).to(int)\r\n",
    "# y\r\n",
    "\r\n",
    "# batch = (torch.stack([item[0] for item in batch]),\r\n",
    "#          torch.stack([torch.Tensor(item[1]) for item in batch]))\r\n",
    "# return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "image_dir = os.path.join(output_dir, \"images\")\r\n",
    "plot_dir = os.path.join(output_dir, \"plots\")\r\n",
    "os.makedirs(image_dir, exist_ok = True)\r\n",
    "os.makedirs(plot_dir, exist_ok = True)\r\n",
    "\r\n",
    "\r\n",
    "image_paths = {\"images\": {},\r\n",
    "               \"class_distribution_plots\":{}}\r\n",
    "\r\n",
    "max_samples = 64\r\n",
    "subsets = ['train', 'val', 'test']\r\n",
    "for subset in subsets:    \r\n",
    "    fig, ax = data_splits[subset].show_batch(indices=max_samples, include_colorbar=False,\r\n",
    "                                             suptitle = f\"subset: {subset}, {max_samples} random images\")\r\n",
    "    img_path = os.path.join(image_dir, f\"subset: {subset}, {max_samples} random images.jpg\")\r\n",
    "    image_paths[\"images\"][subset] = img_path\r\n",
    "    plt.savefig(img_path)\r\n",
    "\r\n",
    "fig, ax = plot_split_distributions(data_splits=data_splits)\r\n",
    "class_distribution_plot_path = os.path.join(plot_dir, f\"class_distribution_plots_{[subset for subset in data_splits.keys()]}\")\r\n",
    "image_paths[\"class_distribution_plots\"][\"all\"] = class_distribution_plot_path\r\n",
    "\r\n",
    "plt.savefig(class_distribution_plot_path)\r\n",
    "\r\n",
    "\r\n",
    "# dir(ax)\r\n",
    "# cb=plt.colorbar()\r\n",
    "# cb.remove()\r\n",
    "# plt.draw()\r\n",
    "\r\n",
    "# plt.subplots_adjust(left=None, bottom=0.0, right=None, top=0.95, wspace=None, hspace=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5357d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4346633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.display_grid(indices=64,\r\n",
    "               label_font_size=\"medium\")\r\n",
    "plt.suptitle(k, fontsize=\"large\")\r\n",
    "rows = 5\r\n",
    "plt.subplots_adjust(left=None, bottom=0.0, right=None, top=0.9, wspace=None, hspace=0.05*rows) #wspace=0.05, hspace=0.1)\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "torch.stack\r\n",
    "\r\n",
    "set(pnas_df.family) #- set(label_encoder.classes[:20])\r\n",
    "\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.style.use('fivethirtyeight')\r\n",
    "sns_context = \"talk\"\r\n",
    "sns_style = \"seaborn-bright\"\r\n",
    "sns.set_context(context=sns_context, font_scale=0.8)\r\n",
    "\r\n",
    "sns.set_palette(\"Accent\")\r\n",
    "# valid contexts = paper, notebook, talk, poster - \r\n",
    "# with notebook being 1:1 and paper being smaller and poster being largest\r\n",
    "# sns.set_style('darkgrid')\r\n",
    "# sns.set_palette('Set2')\r\n",
    "\r\n",
    "# plt.style.use(sns_style)\r\n",
    "fig, ax = plot_split_distributions(data_splits= {'train':train_dataset_extant_minus_pnas,\r\n",
    "                                                 'val': val_dataset_extant_minus_pnas,\r\n",
    "                                                 'test':test_dataset_extant_in_pnas})\r\n",
    "\r\n",
    "# for label in ax[1].xaxis.get_ticklabels()[::2]:\r\n",
    "#     label.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27391c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "classes = list(set(list(train_df['family'])))#[:100]\r\n",
    "num_classes = len(classes)\r\n",
    "df = pd.DataFrame(np.random.random((num_classes,num_classes)), columns=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!where latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('dark_background')\r\n",
    "plt.style.use('fivethirtyeight')\r\n",
    "\r\n",
    "plt.figure(figsize = (15,15))\r\n",
    "plt.imshow(df.values, cmap=\"BrBG\")\r\n",
    "\r\n",
    "\r\n",
    "label_format = '{:,.0f}'\r\n",
    "\r\n",
    "# nothing done to ax1 as it is a \"control chart.\"\r\n",
    "ax = plt.gca()\r\n",
    "\r\n",
    "\r\n",
    "import matplotlib.ticker as mticker\r\n",
    "\r\n",
    "# fixing yticks with \"set_yticks\"\r\n",
    "# ticks_loc = ax.get_yticks().tolist()\r\n",
    "# ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\r\n",
    "\r\n",
    "# # fixing yticks with matplotlib.ticker \"FixedLocator\"\r\n",
    "# ticks_loc = ax3.get_yticks().tolist()\r\n",
    "# ax3.yaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\r\n",
    "# ax3.set_yticklabels([label_format.format(x) for x in ticks_loc])\r\n",
    "\r\n",
    "# # fixing xticks with FixedLocator but also using MaxNLocator to avoid cramped x-labels\r\n",
    "# ax.xaxis.set_major_locator(mticker.MaxNLocator(75))\r\n",
    "# ticks_loc = ax.get_xticks().tolist()\r\n",
    "# ax.xaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\r\n",
    "# ax.set_xticklabels([label_format.format(x) for x in ticks_loc])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# ax = plt.gca()\r\n",
    "\r\n",
    "# ax.set_xticklabels(classes)\r\n",
    "# plt.xticks(\r\n",
    "# rotation=90, #45, \r\n",
    "# horizontalalignment='right',\r\n",
    "# fontweight='light',\r\n",
    "# fontsize='small'\r\n",
    "# )\r\n",
    "\r\n",
    "\r\n",
    "# plot a heatmap with annotation\r\n",
    "# sns.heatmap(df, annot=True, annot_kws={\"size\": 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b25fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa655af",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pnas_df.shape[0] == 5311, \"Expected full PNAS_family_100 dataset to have 5311 samples\"\r\n",
    "\r\n",
    "assert pnas_minus_extant.shape[0] == 2518\r\n",
    "assert pnas_in_extant.shape[0] == 2793\r\n",
    "\r\n",
    "assert pnas_in_extant.shape[0] == extant_in_pnas.shape[0]\r\n",
    "\r\n",
    "assert pnas_in_extant.merge(pnas_minus_extant, on=\"catalog_number\", how=\"inner\").shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c15e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnas_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4665def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "from PIL import Image\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "fig, ax = plt.subplots(1,2, figsize=(24,12))\r\n",
    "extant_in_pnas.iloc[:1,:].apply(lambda x: [ax[0].imshow(Image.open(x.path_extant).convert(\"L\"), cmap=\"gray\"), ax[1].imshow(Image.open(x.path_pnas).convert(\"L\"), cmap=\"gray\")], axis=1)\r\n",
    "ax[0].set_title(\"Extant (No-Crop)\")\r\n",
    "ax[1].set_title(\"PNAS (Cropped)\")\r\n",
    "plt.suptitle(Path(extant_in_pnas.iloc[0,:].path_extant).stem)\r\n",
    "# extant_in_pnas.iloc[:1,:].apply(lambda x: print(type(x)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2793+22704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to only keep original columns\r\n",
    "suffixes=(\"_extant\", \"_pnas\")\r\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\r\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3ab70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae747247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96c4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = extant_df\r\n",
    "# other_df = pnas_df\r\n",
    "# id_col = \"catalog_number\"\r\n",
    "\r\n",
    "\r\n",
    "data_df.sort_values(\"catalog_number\")\r\n",
    "intersected = data_df.merge(other_df, on=id_col, how='inner').sort_values(id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df.sort_values(\"catalog_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12aa206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extant_minus_pnas\r\n",
    "\r\n",
    "pnas_minus_extant\r\n",
    "pnas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(self.samples)\r\n",
    "data_df = data_df.convert_dtypes()\r\n",
    "\r\n",
    "other_df = pd.DataFrame(other.samples)\r\n",
    "other_df = other_df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fdca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba480f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CommonDataArithmetic(CommonDataset):\r\n",
    "    \r\n",
    "    \r\n",
    "#     @property\r\n",
    "#     def samples_df(self):        \r\n",
    "#         data_df = pd.DataFrame(self.samples)\r\n",
    "#         data_df = data_df.convert_dtypes()\r\n",
    "#         return data_df\r\n",
    "\r\n",
    "    \r\n",
    "# other_df = pd.DataFrame(other.samples)\r\n",
    "# other_df = other_df.convert_dtypes()\r\n",
    "    \r\n",
    "    \r\n",
    "#     def intersection(self, other):\r\n",
    "#         samples_df = self.samples_df\r\n",
    "#         other_df = other.samples_df\r\n",
    "        \r\n",
    "#         intersection = data_df.merge(other_df, how='inner', on=self.id_col)\r\n",
    "#         return intersection\r\n",
    "\r\n",
    "#     def __sub__(self, other)\r\n",
    "    \r\n",
    "#         intersection = self.intersection(other)\r\n",
    "#         samples_df = self.samples_df\r\n",
    "        \r\n",
    "#         remainder = samples_df[samples_df[self.id_col].apply(lambda x: x not in intersection[self.id_col])]\r\n",
    "        \r\n",
    "#         return remainder\r\n",
    "    \r\n",
    "    \r\n",
    "    \r\n",
    "    \r\n",
    "    \r\n",
    "data_df = pd.DataFrame(self.samples)\r\n",
    "data_df = data_df.convert_dtypes()\r\n",
    "\r\n",
    "other_df = pd.DataFrame(other.samples)\r\n",
    "other_df = other_df.convert_dtypes()\r\n",
    "        \r\n",
    "        \r\n",
    "#         init_params = self.init_params\r\n",
    "#         init_params[\"files\"] = data_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "int.__sub__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(OmegaConf.to_container(self.config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_data = data_df.merge(other_df, how='inner', on=self.id_col)\r\n",
    "\r\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data = extant_train + pnas_train\r\n",
    "concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52744043",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(concat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f22d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\r\n",
    "import collections\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "sns.set_context(context='talk', font_scale=0.8)\r\n",
    "# sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c4124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_counts(targets: Sequence,\r\n",
    "                         sort_by: Optional[Union[str, bool, Sequence]]=\"count\"\r\n",
    "                        ) -> Dict[str, int]:\r\n",
    "    \r\n",
    "    counts = collections.Counter(targets)\r\n",
    "    if isinstance(sort_by, list):\r\n",
    "        counts = {k: counts[k] for k in sort_by}\r\n",
    "    elif (sort_by == \"count\"):\r\n",
    "        counts = dict(sorted(counts.items(), key = lambda x:x[1], reverse=True))\r\n",
    "    elif (sort_by is True):\r\n",
    "        counts = dict(sorted(counts.items(), key = lambda x:x[0], reverse=True))\r\n",
    "        \r\n",
    "    return counts\r\n",
    "\r\n",
    "def plot_class_distributions(targets: List[Any], \r\n",
    "                             sort_by: Optional[Union[str, bool, Sequence]]=\"count\",\r\n",
    "                             ax=None,\r\n",
    "                             xticklabels: bool=True):\r\n",
    "    \"\"\"\r\n",
    "    Example:\r\n",
    "        counts = plot_class_distributions(targets=data.targets, sort=True)\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    counts = compute_class_counts(targets,\r\n",
    "                                  sort_by=sort_by)\r\n",
    "                        \r\n",
    "    keys = list(counts.keys())\r\n",
    "    values = list(counts.values())\r\n",
    "\r\n",
    "    if ax is None:\r\n",
    "        plt.figure(figsize=(16,12))\r\n",
    "    ax = sns.histplot(x=keys, weights=values, discrete=True, ax=ax)\r\n",
    "    plt.sca(ax)\r\n",
    "    if xticklabels:\r\n",
    "        plt.xticks(\r\n",
    "            rotation=45, \r\n",
    "            horizontalalignment='right',\r\n",
    "            fontweight='light',\r\n",
    "            fontsize='medium'\r\n",
    "        )\r\n",
    "    else:\r\n",
    "        ax.set_xticklabels([])\r\n",
    "    \r\n",
    "    return counts\r\n",
    "\r\n",
    "\r\n",
    "def plot_split_distributions(data_splits: Dict[str, CommonDataset]):\r\n",
    "    \"\"\"\r\n",
    "    Create 3 vertically-stacked count plots of train, val, and test dataset class label distributions\r\n",
    "    \"\"\"\r\n",
    "    assert isinstance(data_splits, dict)\r\n",
    "    num_splits = len(data_splits)\r\n",
    "    \r\n",
    "    if num_splits < 4:\r\n",
    "        rows = num_splits\r\n",
    "        cols = 1\r\n",
    "    else:\r\n",
    "        rows = int(num_splits // 2)\r\n",
    "        cols = int(num_splits % 2)\r\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(16*cols,8*rows))\r\n",
    "    ax = ax.flatten()\r\n",
    "    \r\n",
    "    \r\n",
    "    train_key = [k for k,v in data_splits.items() if \"train\" in k]\r\n",
    "    if len(train_key)==1:\r\n",
    "        train_counts = compute_class_counts(data_splits[train_key[0]].targets,\r\n",
    "                                            sort_by=\"count\")\r\n",
    "    xticklabels=False\r\n",
    "    num_samples = 0\r\n",
    "    counts = {}\r\n",
    "    for i, (k, v) in enumerate(data_splits.items()):\r\n",
    "        if i == len(data_splits)-1:\r\n",
    "            xticklabels=True\r\n",
    "        counts[k] = plot_class_distributions(targets=v.targets, \r\n",
    "                                             sort_by=train_counts,\r\n",
    "                                             ax = ax[i],\r\n",
    "                                             xticklabels=xticklabels)\r\n",
    "        plt.gca().set_title(f\"{k} (n={len(v)})\", fontsize='large')\r\n",
    "        \r\n",
    "        num_samples += len(v)\r\n",
    "    \r\n",
    "    plt.suptitle('-'.join(list(data_splits.keys())) + f\"_splits (total={num_samples})\", fontsize='x-large')\r\n",
    "    plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.07)\r\n",
    "    \r\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.data.common import plot_split_distributions, compute_class_counts\r\n",
    "\r\n",
    "\r\n",
    "data_splits = {\"train\": data.train_dataset,\r\n",
    "               \"val\": data.val_dataset,\r\n",
    "               \"test\": data.test_dataset}\r\n",
    "\r\n",
    "# plot_split_distributions(data_splits=data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35246228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in list(data_splits.keys()):\r\n",
    "#     data_splits[k] = pd.DataFrame([data_splits[k]]).assign(split = k)\r\n",
    "    \r\n",
    "# target_splits = pd.concat(list(data_splits.values()))\r\n",
    "# target_splits.reset_index().describe(include='all')\r\n",
    "import numpy as np    \r\n",
    "\r\n",
    "\r\n",
    "# y_col = \"target\"\r\n",
    "y_col = \"family\"\r\n",
    "target_splits = pd.concat([pd.DataFrame(v.targets).assign(split = k) for k, v in data_splits.items()]).rename(columns={0:y_col})\r\n",
    "target_splits.reset_index().describe(include='all')\r\n",
    "\r\n",
    "# pd.DataFrame(target_splits.groupby(\"family\"))\r\n",
    "\r\n",
    "# pd.DataFrame(target_splits.groupby(\"split\"))\r\n",
    "\r\n",
    "pd.DataFrame(target_splits.groupby(\"split\")[\"family\"])#.agg([len]))\r\n",
    "\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "sns.countplot(data=target_splits,\r\n",
    "              x=\"family\",\r\n",
    "              hue=\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\r\n",
    "import dataclasses\r\n",
    "from omegaconf import DictConfig, OmegaConf\r\n",
    "from rich import print as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\r\n",
    "xticklabels = True\r\n",
    "\r\n",
    "sns.set_style('darkgrid')\r\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f02677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT_TYPES = DictConfig({\r\n",
    "#     \"unstacked_grouped_countplot\": {\"multiple\":\"dodge\",\r\n",
    "#                                     \"stat\":\"count\", \"kde\":True, \"shrink\":0.95, \"binwidth\":1.5},\r\n",
    "#     \"stacked_filled_grouped_histplot\": {\"multiple\":\"fill\",\r\n",
    "#                                         \"stat\":\"probability\", \"shrink\":0.95, \"binwidth\":0.6},\r\n",
    "#     \"stacked_grouped_countplot\": {\"multiple\":\"stack\",\r\n",
    "#                                   \"stat\":\"count\", \"shrink\":0.9, \"binwidth\":1.5}\r\n",
    "#     })\r\n",
    "        \r\n",
    "# kwargs = PLOT_TYPES\r\n",
    "# pp(dict(kwargs))\r\n",
    "# pp(OmegaConf.to_container(cfg, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def plot_grouped_class_distributions(data: pd.DataFrame,\r\n",
    "                                     x_col: str=\"family\",\r\n",
    "                                     group_col: Optional[str]=None,\r\n",
    "                                     suptitle: Optional[str]=None,\r\n",
    "                                     savefig: Optional[str]=None,\r\n",
    "                                     single_fig_plot: Optional[bool]=True,\r\n",
    "                                     log_dir: Union[Path, str]=\".\",\r\n",
    "                                     height = 13,\r\n",
    "                                     width = 25,\r\n",
    "                                     kwargs: Optional[Dict[str,str]]=None):\r\n",
    "\r\n",
    "    if isinstance(kwargs, dict):\r\n",
    "        kwargs = [kwargs]\r\n",
    "    elif kwargs is None:\r\n",
    "        kwargs = [{\"kwargs\":{}}]\r\n",
    "        \r\n",
    "    default_kwargs = {\"shrink\":0.9, \"binwidth\":3.0}\r\n",
    "    axes = []\r\n",
    "    \r\n",
    "    \r\n",
    "    \r\n",
    "    counts = compute_class_counts(targets=data[x_col],\r\n",
    "                         sort_by=\"count\"\r\n",
    "                        )\r\n",
    "    class_order = list(counts.keys())\r\n",
    "    \r\n",
    "    if single_fig_plot:\r\n",
    "        rows = len(kwargs); cols = 1\r\n",
    "        figsize=(width*cols,height*rows)\r\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=figsize)\r\n",
    "        axes = axes.flatten()\r\n",
    "        \r\n",
    "\r\n",
    "    for i in range(len(kwargs)):\r\n",
    "        kwargs_i = default_kwargs\r\n",
    "        kwargs_i.update(kwargs[i][\"kwargs\"])\r\n",
    "        \r\n",
    "        if single_fig_plot:\r\n",
    "            ax = axes[i]\r\n",
    "            if \"title\" in kwargs[i]:\r\n",
    "                ax.set_title(kwargs[i][\"title\"], fontsize=\"large\")\r\n",
    "            plt.subplots_adjust(bottom=0.05, top=0.96, wspace=None, hspace=0.25)\r\n",
    "            \r\n",
    "        else:\r\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20,12))\r\n",
    "            axes.append(ax)\r\n",
    "            if \"title\" in kwargs[i]:\r\n",
    "#                 ax.set_title(kwargs[i][\"title\"], fontsize=\"large\")\r\n",
    "                plt.suptitle(kwargs[i][\"title\"], fontsize=\"large\")\r\n",
    "            plt.subplots_adjust(bottom=0.15, top=0.95, wspace=None, hspace=0.3)            \r\n",
    "        \r\n",
    "        ax = sns.histplot(data=data,\r\n",
    "                          x=x_col,\r\n",
    "                          hue=group_col,\r\n",
    "                          ax=ax,\r\n",
    "                          pthresh=0.1,\r\n",
    "                          **kwargs_i)\r\n",
    "\r\n",
    "        plt.sca(ax)\r\n",
    "        sns.despine()\r\n",
    "        xticklabels = bool(data[x_col].nunique() < 100)\r\n",
    "        if xticklabels:\r\n",
    "            plt.xticks(\r\n",
    "                rotation=45, \r\n",
    "                horizontalalignment='right',\r\n",
    "                fontweight='light',\r\n",
    "                fontsize='small'\r\n",
    "            )\r\n",
    "        else:\r\n",
    "            ax.set_xticklabels([])\r\n",
    "            \r\n",
    "            \r\n",
    "        if not single_fig_plot:\r\n",
    "            if \"savefig\" in kwargs[i]:\r\n",
    "                plt.savefig(kwargs[i][\"savefig\"])\r\n",
    "\r\n",
    "    if single_fig_plot:\r\n",
    "        plt.suptitle(suptitle, fontsize=\"x-large\")\r\n",
    "        if isinstance(savefig, (Path, str)):\r\n",
    "            print(f'Saving: savefig={savefig}')\r\n",
    "            plt.savefig(savefig)\r\n",
    "        elif isinstance(suptitle, str):\r\n",
    "            print(f'Saving: suptitle={suptitle}')\r\n",
    "            plt.savefig(os.path.join(log_dir, f\"{suptitle}.png\"))\r\n",
    "    return fig, axes\r\n",
    "\r\n",
    "#         if \"title\" in kwargs[i]:\r\n",
    "#             ax.set_title(kwargs[i][\"title\"], fontsize=\"large\")        \r\n",
    "#     plt.suptitle(suptitle)    \r\n",
    "#     plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadda8fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Latest data distribution plots -- July 18th, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=pnas_dataset\"]),\r\n",
    "            initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=extant_dataset\"]),\r\n",
    "            initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=fossil_dataset\", \"hparams.image_size=1024\"])\r\n",
    "           ]\r\n",
    "\r\n",
    "logdir = f\"/media/data/jacob/GitHub/lightning-hydra-classifiers/outputs/data_distribution_logs\"\r\n",
    "os.makedirs(logdir, exist_ok=True)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "for i in range(len(configs)):\r\n",
    "\r\n",
    "    cfg = configs[i]\r\n",
    "    data = LeavesLightningDataModule(cfg.datamodule.config)\r\n",
    "    data_splits = {\"train\": data.train_dataset,\r\n",
    "                   \"val\": data.val_dataset,\r\n",
    "                   \"test\": data.test_dataset}\r\n",
    "\r\n",
    "\r\n",
    "    dataset_name = cfg.dataset.config.name\r\n",
    "    label_col = cfg.dataset.config.class_type\r\n",
    "    group_col = \"subset\"\r\n",
    "\r\n",
    "    kwargs_options = [{\"kwargs\":{\"multiple\":\"dodge\", \"stat\":\"count\", \"kde\":True, \"shrink\":0.9, \"binwidth\":2*1.5},\r\n",
    "                       \"title\":f\"{dataset_name}, Per-class countplot, grouped by subset\",\r\n",
    "                       \"savefig\":os.path.join(logdir, f\"{dataset_name}_{label_col}_unstacked_subset_count_distributions.png\")},\r\n",
    "                      {\"kwargs\":{\"multiple\":\"fill\", \"stat\":\"probability\", \"shrink\":0.95, \"binwidth\":2*0.6},\r\n",
    "                       \"title\":f\"{dataset_name}, Per-class filled histograms, grouped by subset\",\r\n",
    "                       \"savefig\":os.path.join(logdir, f\"{dataset_name}_{label_col}_stacked_subset_prior_probabilities.png\")},\r\n",
    "                      {\"kwargs\":{\"multiple\":\"stack\", \"stat\":\"count\", \"shrink\":0.9, \"binwidth\":2*1.5},\r\n",
    "                       \"title\":f\"{dataset_name}, Per-class countplot, grouped by subset\",\r\n",
    "                       \"savefig\":os.path.join(logdir, f\"{dataset_name}_{label_col}_stacked_subset_count_distributions.png\")}\r\n",
    "                     ]\r\n",
    "\r\n",
    "    target_splits = pd.concat([pd.DataFrame(v.targets).assign(**{group_col:k}) for k, v in data_splits.items()\r\n",
    "                              ]).rename(columns={0:label_col})\r\n",
    "\r\n",
    "\r\n",
    "    ### Sort classes by count\r\n",
    "    data_df = target_splits\r\n",
    "    counts = compute_class_counts(targets=data_df[label_col],\r\n",
    "                                  sort_by=\"count\"\r\n",
    "                        )\r\n",
    "    class_order = {label:i for i, label in enumerate(counts.keys())}\r\n",
    "\r\n",
    "    data_df = data_df.assign(family_order = data_df.family.apply(lambda x: class_order[x]))\r\n",
    "    target_splits = data_df.sort_values(by=[\"family_order\"], ascending=True).drop(columns=[\"family_order\"])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    plot_grouped_class_distributions(data=target_splits,\r\n",
    "                                     x_col=label_col,\r\n",
    "                                     group_col=group_col,\r\n",
    "                                     single_fig_plot=False,\r\n",
    "    #                                  suptitle=f\"Dataset: {dataset_name} {label_col} class distributions\",\r\n",
    "                                     log_dir = logdir,\r\n",
    "                                     kwargs = kwargs_options[:])\r\n",
    "\r\n",
    "    plot_grouped_class_distributions(data=target_splits,\r\n",
    "                                     x_col=label_col,\r\n",
    "                                     group_col=group_col,\r\n",
    "                                     single_fig_plot=True,\r\n",
    "                                     suptitle=f\"Dataset={dataset_name} {label_col} class distributions\",\r\n",
    "                                     log_dir = logdir,\r\n",
    "                                     kwargs = kwargs_options[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c405c69",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f3e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b6bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823f198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3949c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "assert isinstance(data_splits, dict)\n",
    "num_splits = len(data_splits)\n",
    "\n",
    "if num_splits < 4:\n",
    "    rows = num_splits\n",
    "    cols = 1\n",
    "else:\n",
    "    rows = int(num_splits // 2)\n",
    "    cols = int(num_splits % 2)\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(16*cols,8*rows))\n",
    "ax = ax.flatten()\n",
    "\n",
    "\n",
    "train_key = [k for k,v in data_splits.items() if \"train\" in k]\n",
    "if len(train_key)==1:\n",
    "    train_counts = compute_class_counts(data_splits[train_key[0]].targets,\n",
    "                                        sort_by=\"count\")\n",
    "xticklabels=False\n",
    "num_samples = 0\n",
    "counts = {}\n",
    "for i, (k, v) in enumerate(data_splits.items()):\n",
    "    if i == len(data_splits)-1:\n",
    "        xticklabels=True\n",
    "    counts[k] = plot_class_distributions(targets=v.targets, \n",
    "                                         sort_by=train_counts,\n",
    "                                         ax = ax[i],\n",
    "                                         xticklabels=xticklabels)\n",
    "    plt.gca().set_title(f\"{k} (n={len(v)})\", fontsize='large')\n",
    "\n",
    "    num_samples += len(v)\n",
    "\n",
    "plt.suptitle('-'.join(list(data_splits.keys())) + f\"_splits (total={num_samples})\", fontsize='x-large')\n",
    "plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.07)\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "width = 0.5\n",
    "\n",
    "temp_summer=[ random.uniform(20,40) for i in range(5)]\n",
    "temp_winter=[ random.uniform(0,10) for i in range(5)]\n",
    "\n",
    "fig=plt.figure(figsize=(10,6))\n",
    "\n",
    "city=['City A','City B','City C','City D','City E']\n",
    "x_pos_summer=list(range(1,6))\n",
    "x_pos_winter=[ i+width for i in x_pos_summer]\n",
    "\n",
    "graph_summer=plt.bar(x_pos_summer, temp_summer, color='tomato', label='Summer', width=width)\n",
    "graph_winter=plt.bar(x_pos_winter, temp_winter, color='dodgerblue', label='Winter', width=width)\n",
    "\n",
    "plt.xticks([i+width/2 for i in x_pos_summer],city)\n",
    "plt.title('City Temperature')\n",
    "plt.ylabel('Temperature ($^\\circ$C)')\n",
    "\n",
    "#Annotating graphs\n",
    "for summer_bar,winter_bar,ts,tw in zip(graph_summer,graph_winter,temp_summer,temp_winter):\n",
    "    plt.text(summer_bar.get_x() + summer_bar.get_width()/2.0,summer_bar.get_height(),'%.2f$^\\circ$C'%ts,ha='center',va='bottom')\n",
    "    plt.text(winter_bar.get_x() + winter_bar.get_width()/2.0,winter_bar.get_height(),'%.2f$^\\circ$C'%tw,ha='center',va='bottom')\n",
    "\n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94ef86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316acfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.train_dataset)\n",
    "display(data.val_dataset)\n",
    "display(data.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.train_dataloader()\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dir(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"0\":[0,1,2,3,4], \"1\":[0,1,2,3,4]}).T.to_records()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07155880",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset.plot_trainvaltest_splits(data.train_dataset,\n",
    "                                     data.val_dataset,\n",
    "                                     data.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e95dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "torchvision.transforms.ToPILImage()(data.train_dataset[387][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pathlib import Path\n",
    "# path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "# # path_schema: str = \"{family}_{genus}_{species}_{catalog_number}\"\n",
    "\n",
    "# # # path_schema = Path(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_1536/train/Fabaceae\")\n",
    "# # filepath = 'Fabaceae_Derris_alborubra_Wolfe_9829.jpg'\n",
    "# sep = \"_\"\n",
    "\n",
    "# # from dataclasses import dataclass\n",
    "# # from typing import *\n",
    "\n",
    "\n",
    "# # @dataclass \n",
    "# # class PathSchema:\n",
    "# #     path_schema: str = Path(\"{family}_{genus}_{species}_{collection}_{catalog_number}\")\n",
    "# #     schema_parts: List[str] = path_schema.split(sep)\n",
    "# #     maxsplit = len(schema_parts) - 2\n",
    "    \n",
    "# #     def parse(self, path: Union[Path, str], sep: str=\"_\"):\n",
    "    \n",
    "# #         parts = Path(path).stem.split(sep, maxsplit=maxsplit)\n",
    "# #         if len(parts) == 5:\n",
    "# #             family, genus, species, collection, catalog_number = parts\n",
    "# #         if len(parts) == 4:\n",
    "# #             family, genus, species, catalog_number = parts\n",
    "# #             collection = catalog_number.split(\"_\")\n",
    "\n",
    "# #         return family, genus, species, collection, catalog_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e962d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Extant_Leaves/Aizoaceae/Aizoaceae_Galenia_pubescens_Hickey_Hickey_8097.jpg\").stem\n",
    "\n",
    "path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "schema_parts = path_schema.split(sep)\n",
    "maxsplit = len(schema_parts) - 2\n",
    "\n",
    "print(f\"schema_parts={schema_parts}\")\n",
    "print(f\"maxsplit={maxsplit}\")\n",
    "family, genus, species, collection, catalog_number = Path(filepath).stem.split(\"_\", maxsplit=maxsplit)\n",
    "print(family, genus, species, collection, catalog_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_1536/train/Fabaceae/Fabaceae_Derris_alborubra_Wolfe_9829.jpg\").stem\n",
    "\n",
    "path_schema: str = \"{family}_{genus}_{species}_{catalog_number}\"\n",
    "schema_parts = path_schema.split(sep)\n",
    "maxsplit = len(schema_parts) - 2\n",
    "\n",
    "print(f\"schema_parts={schema_parts}\")\n",
    "print(f\"maxsplit={maxsplit}\")\n",
    "\n",
    "family, genus, species, catalog_number = Path(filepath).stem.split(\"_\", maxsplit=maxsplit)\n",
    "print(family, genus, species, catalog_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3880ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "toPIL = torchvision.transforms.ToPILImage(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6725e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34305",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(stage='val')\n",
    "\n",
    "data.show_batch(stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.train_dataset\n",
    "\n",
    "train_dataloader = data.train_dataloader()\n",
    "\n",
    "train_dataset = data.get_dataset(\"train\")\n",
    "\n",
    "train_dataset.show_batch()\n",
    "\n",
    "# train_dataset = \n",
    "data.get_dataset(\"val\")\n",
    "\n",
    "# train_dataset = \n",
    "data.get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8074b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef95008",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(OmegaConf.to_container(cfg.dataset, resolve=True))\n",
    "\n",
    "pp(OmegaConf.to_container(cfg.datamodule.config.dataset, resolve=True))\n",
    "\n",
    "pp(OmegaConf.to_container(cfg.datamodule, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc73ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Fossil\"\n",
    "pp([k for k in CommonDataset.available_datasets.keys() if dataset_name in k])\n",
    "\n",
    "dataset_name = \"PNAS\"\n",
    "pp([k for k in CommonDataset.available_datasets.keys() if dataset_name in k])\n",
    "\n",
    "dataset_name = \"Extant\"\n",
    "pp([k for k in CommonDataset.available_datasets.keys() if dataset_name in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd79377",
   "metadata": {},
   "outputs": [],
   "source": [
    "Extant_config = OmegaConf.create({\"name\": \"Extant_family_10_512\",\n",
    "                            \"val_split\": 0.2,\n",
    "                            \"test_split\": 0.3,\n",
    "                            \"threshold\": 3,\n",
    "                            \"seed\": 987485,\n",
    "                            \"class_type\": \"family\",\n",
    "                            \"x_col\":\"path\",\n",
    "                            \"y_col\":\"${.class_type}\",\n",
    "                            \"id_col\":\"catalog_number\"\n",
    "})\n",
    "\n",
    "\n",
    "config = OmegaConf.create({\"name\": \"Fossil_512\",\n",
    "                            \"val_split\": 0.2,\n",
    "                            \"test_split\": 0.3,\n",
    "                            \"threshold\": 3,\n",
    "                            \"seed\": 987485,\n",
    "                            \"class_type\": \"family\",\n",
    "                            \"x_col\":\"path\",\n",
    "                            \"y_col\":\"${.class_type}\",\n",
    "                            \"id_col\":\"catalog_number\"\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pp(OmegaConf.to_container(config, resolve=True))\n",
    "data = CommonDataset(config=config,\n",
    "                     files=None,\n",
    "                     class2idx=None)\n",
    "\n",
    "data[1].image\n",
    "print(data.__repr__())\n",
    "\n",
    "data.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/datamodule/fossil_datamodule.yaml\"\n",
    "config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/multi-gpu.yaml\"\n",
    "config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(OmegaConf.to_container(config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b235b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e84091f7",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa6e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Fossil_512\"\n",
    "dataset_dirs = CommonDataSelect.available_datasets[name]\n",
    "\n",
    "dataset_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CommonDataset.available_datasets.keys()\n",
    "\n",
    "# CommonDataset.available_datasets[\"Fossil_512\"]#.keys()\n",
    "\n",
    "# fossil.available_datasets\n",
    "\n",
    "# d0 = CommonDataSelect.select_dataset_by_name(\"Fossil_512\")\n",
    "\n",
    "# dir(OmegaConf)\n",
    "\n",
    "# config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/datamodule/fossil_datamodule.yaml\"\n",
    "# config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a79969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# d1 = torchdata.datasets.Files.from_folder(Path(dataset_dirs[0]), regex=\"*/*.jpg\")\n",
    "d2 = torchdata.datasets.Files.from_folder(Path(dataset_dirs[1]), regex=\"*/*.jpg\")\n",
    "# d2\n",
    "\n",
    "from itertools import repeat, chain\n",
    "from more_itertools import collapse, flatten\n",
    "\n",
    "\n",
    "cls = torchdata.datasets.Files\n",
    "\n",
    "log.info(f\"Concatenating dataset_dirs located at: {dataset_dirs}\")\n",
    "file_list = list(flatten(\n",
    "                    [cls.from_folder(Path(root),\n",
    "                                     regex=\"*/*.jpg\").files\n",
    "                     for root in dataset_dirs]\n",
    "                                            ))\n",
    "data = cls(files=file_list,\n",
    "           name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = (split[0] for split in data_splits.values())\n",
    "y_train, y_val, y_test = (split[1] for split in data_splits.values())\n",
    "\n",
    "\n",
    "\n",
    "from rich import print as pp\n",
    "\n",
    "\n",
    "pp(data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.2\n",
    "test_split = 0.3\n",
    "\n",
    "train_split = 1 - (val_split + test_split)\n",
    "\n",
    "val_relative_split = val_split/(train_split + val_split)\n",
    "train_relative_split = train_split/(train_split + val_split)\n",
    "random_state = 0\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_split, random_state=random_state, stratify=y)\n",
    "print(f\"x_train.shape={x_train.shape}, x_test.shape={x_test.shape}, y_train.shape={y_train.shape}, y_test.shape={y_test.shape}\")\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_relative_split, random_state=random_state, stratify=y_train)\n",
    "\n",
    "print(f\"x_train.shape={x_train.shape}, x_val.shape={x_val.shape}, y_train.shape={y_train.shape}, y_val.shape={y_val.shape}\")\n",
    "\n",
    "\n",
    "print(f'Absolute splits: {[train_split, val_split, test_split]}')\n",
    "print(f'Relative splits: [{train_relative_split:.2f}, {val_relative_split:.2f}, {test_split}]')\n",
    "\n",
    "print(f'train+val={train_split+val_split} | test={test_split}')\n",
    "print(f'train={train_relative_split:.2f} | val={val_relative_split:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ee5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "skf.get_n_splits(x, y)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa10d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(x, y):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(f'y_test: {y_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b32c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    \"\"\"Common base class for all `torchtraining` objects.\n",
    "    Defines default `__str__` and `__repr__`.\n",
    "    Most objects should customize `__str__` according to specific\n",
    "    needs.\n",
    "    Custom objects usually use `yaml.dump` to easily see parameters\n",
    "    and whole pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{type(self).__module__}.{type(self).__name__}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        parameters = \", \".join(\n",
    "            \"{}={}\".format(key, value)\n",
    "            for key, value in self.__dict__.items()\n",
    "            if not key.startswith(\"_\")\n",
    "        )\n",
    "        return \"{}({})\".format(self, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bacfebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata\n",
    "torchdata.datasets.Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ba6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata\n",
    "dir(torchdata.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62895b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38dcb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cccee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=3\n",
    "test_split=0.3\n",
    "val_train_split=0.2\n",
    "batch_size=32\n",
    "num_workers=0\n",
    "seed=8567\n",
    "debug=False\n",
    "normalize=True\n",
    "image_size = 'auto'\n",
    "channels=3\n",
    "dataset_dir=None\n",
    "predict_on_split=\"val\"\n",
    "\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = FossilLightningDataModule(name=dataset_name,\n",
    "                                       threshold=threshold,\n",
    "                                       test_split=test_split,\n",
    "                                       val_train_split=val_train_split,\n",
    "                                       batch_size=batch_size,\n",
    "                                       num_workers=num_workers,\n",
    "                                       seed=seed,\n",
    "                                       debug=debug,\n",
    "                                       normalize=normalize,\n",
    "                                       image_size=image_size,\n",
    "                                       channels=channels,\n",
    "                                       predict_on_split=predict_on_split)\n",
    "\n",
    "datamodule\n",
    "\n",
    "datamodule.setup(\"fit\")\n",
    "datamodule.setup(\"test\")\n",
    "\n",
    "# datamodule.show_batch(\"train\")\n",
    "# datamodule.show_batch(\"val\")\n",
    "# datamodule.show_batch(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbe470",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/Fossil_512_train-test/2021-07-12/06-03/model/checkpoints/\"\n",
    "ckpt_path = os.path.join(ckpt_dir, \"best_model-epoch-epoch=05--val_loss-val_loss=96.52.ckpt\")\n",
    "\n",
    "print(os.path.isfile(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_state = torch.load(ckpt_path)\n",
    "\n",
    "print(type(ckpt_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01569add",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt_state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fe106",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt_state['state_dict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransferLearningModel.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cde5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.create_trainvaltest_splits(dataset=data,\n",
    "                                                                  test_split=0.3,\n",
    "                                                                  val_train_split=0.2,\n",
    "                                                                  shuffle=True,\n",
    "                                                                  seed=3654,\n",
    "                                                                  plot_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_dir = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\"\n",
    "\n",
    "from hydra.experimental import compose, initialize, initialize_config_dir\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from rich import print as pp\n",
    "os.chdir(config_dir)\n",
    "\n",
    "# context initialization\n",
    "# with initialize(config_path=\"../configs\", job_name=\"test_app\"):\n",
    "\n",
    "with initialize_config_dir(config_dir=config_dir, job_name=\"multi-gpu_experiment\"):\n",
    "    \n",
    "    cfg = compose(config_name=\"multi-gpu\")\n",
    "#     print(OmegaConf.to_yaml(cfg))\n",
    "    \n",
    "    pp(OmegaConf.to_container(cfg, resolve=True))\n",
    "    \n",
    "    pp(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns_context = \"talk\"\n",
    "sns_style = \"seaborn-bright\"\n",
    "sns.set_context(context=sns_context, font_scale=0.8)\n",
    "# valid contexts = paper, notebook, talk, poster - \n",
    "# with notebook being 1:1 and paper being smaller and poster being largest\n",
    "plt.style.use(sns_style)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e921cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_class_distributions(targets: List[Any], \n",
    "#                              sort: Union[bool,Sequence]=True,\n",
    "#                              ax=None,\n",
    "#                              xticklabels: bool=True):\n",
    "#     \"\"\"\n",
    "#     Example:\n",
    "#         counts = plot_class_distributions(targets=data.targets, sort=True)\n",
    "#     \"\"\"\n",
    "#     counts = collections.Counter(targets)\n",
    "#     if hasattr(sort, \"__len__\"):\n",
    "#         counts = {k: counts[k] for k in sort}\n",
    "#     elif sort is True:\n",
    "#         counts = dict(sorted(counts.items(), key = lambda x:x[1], reverse=True))\n",
    "\n",
    "#     keys = list(counts.keys())\n",
    "#     values = list(counts.values())\n",
    "\n",
    "#     if ax is None:\n",
    "#         plt.figure(figsize=(16,12))\n",
    "#     ax = sns.histplot(x=keys, weights=values, discrete=True, ax=ax)\n",
    "#     plt.sca(ax)\n",
    "#     if xticklabels:\n",
    "#         plt.xticks(\n",
    "#             rotation=45, \n",
    "#             horizontalalignment='right',\n",
    "#             fontweight='light',\n",
    "#             fontsize='medium'\n",
    "#         )\n",
    "#     else:\n",
    "#         ax.set_xticklabels([])\n",
    "    \n",
    "#     return counts\n",
    "\n",
    "\n",
    "# def plot_trainvaltest_splits(train_data,\n",
    "#                              val_data,\n",
    "#                              test_data):\n",
    "#     \"\"\"\n",
    "#     Create 3 vertically-stacked count plots of train, val, and test dataset class label distributions\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots(3, 1, figsize=(16,8*3))\n",
    "\n",
    "#     train_counts = plot_class_distributions(targets=train_data.targets, sort=True, ax = ax[0], xticklabels=False)\n",
    "#     plt.gca().set_title(f\"train (n={len(train_data)})\", fontsize='large')\n",
    "#     sort_classes = train_counts.keys()\n",
    "\n",
    "#     val_counts = plot_class_distributions(targets=val_data.targets, ax = ax[1], sort=sort_classes, xticklabels=False)\n",
    "#     plt.gca().set_title(f\"val (n={len(val_data)})\", fontsize='large')\n",
    "#     test_counts = plot_class_distributions(targets=test_data.targets, ax = ax[2], sort=sort_classes)\n",
    "#     plt.gca().set_title(f\"test (n={len(test_data)})\", fontsize='large')\n",
    "\n",
    "#     plt.suptitle(f\"Train-Val-Test_splits (total={len(data)})\", fontsize='x-large')\n",
    "\n",
    "#     plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.07)\n",
    "    \n",
    "#     return fig, ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e139b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc20673",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trainvaltest_splits(train_data,\n",
    "                         val_data,\n",
    "                         test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08361878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690076d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d4554a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2fa40f6",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028eff03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b3d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe1f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a858e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = select_from_dataset(data,\n",
    "                                 indices=train_idx,\n",
    "                                 update_class2idx=False,\n",
    "                                 x_col = 'path',\n",
    "                                 y_col = \"family\")\n",
    "\n",
    "val_data = select_from_dataset(data,\n",
    "                               indices=val_idx,\n",
    "                               update_class2idx=False,\n",
    "                               x_col = 'path',\n",
    "                               y_col = \"family\")\n",
    "val_data\n",
    "\n",
    "test_data = select_from_dataset(data,\n",
    "                                indices=test_idx,\n",
    "                                update_class2idx=False,\n",
    "                                x_col = 'path',\n",
    "                                y_col = \"family\")\n",
    "\n",
    "\n",
    "\n",
    "train_counts = plot_class_distributions(targets=train_data.targets, sort=True)\n",
    "sort_classes = train_counts.keys()\n",
    "val_counts = plot_class_distributions(targets=val_data.targets, sort=sort_classes)\n",
    "test_counts = plot_class_distributions(targets=test_data.targets, sort=sort_classes)\n",
    "\n",
    "\n",
    "train_data = (train_val_samples)\n",
    "train_samples = np.array(train_val_samples)[train_idx]\n",
    "val_samples = np.array(train_val_samples)[val_idx]\n",
    "\n",
    "\n",
    "counts = plot_class_distributions(targets=data.targets, sort=True)\n",
    "\n",
    "train_val_idx.shape\n",
    "test_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 test_split=0.3,\n",
    "                 val_train_split=0.2,\n",
    "                 shuffle: bool=False,\n",
    "                 seed: int=None):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        self.indices = np.arange(range(dataset_size))\n",
    "#         test_split = int(np.floor(test_train_split * dataset_size))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        targets = dataset.targets\n",
    "\n",
    "        train_val_idx, test_idx = train_test_split(\n",
    "                                               indices,\n",
    "                                               test_size=test_split,\n",
    "                                               random_state=seed,\n",
    "                                               shuffle=shuffle,\n",
    "                                               stratify=targets)\n",
    "        \n",
    "            \n",
    "            \n",
    "        train_indices, self.test_indices = self.indices[], self.indices[test_split:]\n",
    "        train_size = len(train_indices)\n",
    "        validation_split = int(np.floor((1 - val_train_split) * train_size))\n",
    "\n",
    "        self.train_indices, self.val_indices = train_indices[ : validation_split], train_indices[validation_split:]\n",
    "\n",
    "        self.train_sampler = SubsetRandomSampler(self.train_indices)\n",
    "        self.val_sampler = SubsetRandomSampler(self.val_indices)\n",
    "        self.test_sampler = SubsetRandomSampler(self.test_indices)\n",
    "\n",
    "    def get_train_split_point(self):\n",
    "        return len(self.train_sampler) + len(self.val_indices)\n",
    "\n",
    "    def get_validation_split_point(self):\n",
    "        return len(self.train_sampler)\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_split(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing train-validation-test dataloaders')\n",
    "        self.train_loader = self.get_train_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.val_loader = self.get_validation_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.test_loader = self.get_test_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_train_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing train dataloader')\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.train_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.train_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_validation_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing validation dataloader')\n",
    "        self.val_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.val_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.val_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_test_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing test dataloader')\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.test_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05aa2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.samples)#.iloc[:,0]\n",
    "df = df.assign(sub_dataset = df.apply(lambda x: x[0].parts[-3], axis=1)) #.value_counts()\n",
    "\n",
    "df = df.rename(columns={0:\"path\",\n",
    "                        1:\"family\",\n",
    "                        2:\"genus\",\n",
    "                        3:\"species\",\n",
    "                        4:\"collection\",\n",
    "                        5:\"catalog_number\"})#.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a870b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "targets = dataset.targets\n",
    "\n",
    "train_idx, valid_idx = train_test_split(\n",
    "                                        indices,\n",
    "                                        test_size=test_split,\n",
    "                                        random_state=seed,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=targets)\n",
    "\n",
    "print(np.unique(np.array(targets)[train_idx], return_counts=True))\n",
    "print(np.unique(np.array(targets)[valid_idx], return_counts=True))\n",
    "\n",
    "\n",
    "# val_split = 0.2\n",
    "# test_split = 0.3\n",
    "# total = 1.0\n",
    "# trainval_split = total-test_split\n",
    "# print(trainval_split)\n",
    "# print(trainval_split - val_split)\n",
    "# print((val_split/(trainval_split)))# - val_split)\n",
    "\n",
    "(val_split*0.7)# + 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FossilDatasetSubset(FossilDataset):\n",
    "    \n",
    "#     def __init__(self,\n",
    "#                  split\n",
    "#                  files: List[Path]=None,\n",
    "#                  name: Optional[str]=None,\n",
    "#                  return_items: List[str] = [\"image\",\"target\",\"path\"],\n",
    "#                  image_return_type: str = \"tensor\",\n",
    "#                  *args, **kwargs):\n",
    "#                 ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cdb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('starting')\n",
    "\n",
    "model = models.resnet18()\n",
    "# inputs = torch.randn(5, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "batch_size = 64\n",
    "\n",
    "dataloader = DataLoader(data,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "#                         sampler=None,\n",
    "#                         batch_sampler=None,\n",
    "#                         num_workers=0,\n",
    "#                         collate_fn=None,\n",
    "#                         pin_memory=False,\n",
    "#                         drop_last=False,\n",
    "#                         timeout=0,\n",
    "#                         worker_init_fn=None)\n",
    "\n",
    "# idx = [0,10,20,50,100]\n",
    "# idx = 10\n",
    "idx = list(range(0,1000,100))\n",
    "print(len(idx))\n",
    "data.display_grid(idx, repeat_n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data.samples[0][0].parts[-3]\n",
    "\n",
    "df = pd.DataFrame(data.samples)#.iloc[:,0]\n",
    "df = df.assign(sub_dataset = df.apply(lambda x: x[0].parts[-3], axis=1)) #.value_counts()\n",
    "\n",
    "df = df.rename(columns={0:\"path\",\n",
    "                        1:\"family\",\n",
    "                        2:\"genus\",\n",
    "                        3:\"species\",\n",
    "                        4:\"collection\",\n",
    "                        5:\"catalog_number\"})#.value_counts()\n",
    "\n",
    "# df.value_counts().plot(kind='bar')\n",
    "\n",
    "chart = sns.catplot(\n",
    "    data=df, #[data['Year'].isin([1980, 2008])],\n",
    "    x='family',\n",
    "    kind='count',\n",
    "    palette='Set1',\n",
    "    row='sub_dataset',\n",
    "    aspect=3,\n",
    "    height=3\n",
    ")\n",
    "chart.set_xticklabels(rotation=65, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc997c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sport = (df\n",
    "            .groupby('family')\n",
    "            .filter(lambda x : len(x) > 10)\n",
    "            .groupby(['family', 'genus'])\n",
    "#             .groupby(['genus', 'species'])\n",
    "            .size()\n",
    "            .unstack()\n",
    "           )\n",
    "by_sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503190d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def select_from_indices(data,\n",
    "#                         indices: Sequence,\n",
    "#                         update_class2idx: bool=False,\n",
    "#                         x_col = 'path',\n",
    "#                         y_col = \"family\") -> \"FossilDataset\":\n",
    "#     \"\"\"\n",
    "#     Helper method to create a new FossilDataset containing only samples contained in `indices`\n",
    "#     Useful for producing train/val/test splits\n",
    "    \n",
    "#     \"\"\"\n",
    "#     if update_class2idx:\n",
    "#         class2idx=None\n",
    "#     else:\n",
    "#         class2idx=data.class2idx\n",
    "\n",
    "    \n",
    "#     df = pd.DataFrame(data.samples)\n",
    "#     df = df.rename(columns={0:\"path\",\n",
    "#                             1:\"family\",\n",
    "#                             2:\"genus\",\n",
    "#                             3:\"species\",\n",
    "#                             4:\"collection\",\n",
    "#                             5:\"catalog_number\"})#.value_counts()\n",
    "    \n",
    "#     df = df.iloc[indices,:]\n",
    "    \n",
    "#     files = df[x_col].to_list()\n",
    "\n",
    "#     return FossilDataset(files=files,\n",
    "#                          name=data.name,\n",
    "#                          return_items=data.return_items,\n",
    "#                          image_return_type=data.image_return_type,\n",
    "#                          class2idx=class2idx)\n",
    "\n",
    "\n",
    "\n",
    "# def filter_df_by_threshold(df: pd.DataFrame,\n",
    "#                            threshold: int,\n",
    "#                            y_col: str='family'):\n",
    "#     \"\"\"\n",
    "#     Filter rare classes from dataset in a pd.DataFrame\n",
    "    \n",
    "#     Input:\n",
    "#         df (pd.DataFrame):\n",
    "#             Must contain at least 1 column with name given by `y_col`\n",
    "#         threshold (int):\n",
    "#             Exclude any rows from df that contain a `y_col` value with fewer than `threshold` members in all of df.\n",
    "#         y_col (str): default=\"family\"\n",
    "#             The column in df to look for rare classes to exclude.\n",
    "#     Output:\n",
    "#         (pd.DataFrame):\n",
    "#             Returns a dataframe with the same number of columns as df, and an equal or lower number of rows.\n",
    "#     \"\"\"\n",
    "#     return df.groupby(y_col).filter(lambda x: len(x) >= threshold)\n",
    "\n",
    "\n",
    "\n",
    "# def filter_samples_by_threshold(data: FossilDataset,\n",
    "#                                 threshold: int=1,\n",
    "#                                 update_class2idx: bool=True,\n",
    "#                                 x_col = 'path',\n",
    "#                                 y_col = \"family\") -> FossilDataset:\n",
    "#     if update_class2idx:\n",
    "#         class2idx=None\n",
    "#     else:\n",
    "#         class2idx=data.class2idx\n",
    "\n",
    "        \n",
    "#     df = pd.DataFrame(data.samples)\n",
    "#     df = df.rename(columns={0:\"path\",\n",
    "#                             1:\"family\",\n",
    "#                             2:\"genus\",\n",
    "#                             3:\"species\",\n",
    "#                             4:\"collection\",\n",
    "#                             5:\"catalog_number\"})#.value_counts()\n",
    "    \n",
    "#     df = filter_df_by_threshold(df=df,\n",
    "#                                 threshold=threshold,\n",
    "#                                 y_col=y_col)\n",
    "        \n",
    "#     files = df[x_col].to_list()\n",
    "\n",
    "#     return FossilDataset(files=files,\n",
    "#                          name=data.name,\n",
    "#                          return_items=data.return_items,\n",
    "#                          image_return_type=data.image_return_type,\n",
    "#                          class2idx=class2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset.targets\n",
    "\n",
    "# @classmethod\n",
    "# def create_trainvaltest_splits(cls,\n",
    "#                                dataset,\n",
    "#                                test_split: float=0.3,\n",
    "#                                val_train_split: float=0.2,\n",
    "#                                shuffle: bool=True,\n",
    "#                                seed: int=3654):\n",
    "\n",
    "#     dataset_size = len(dataset)\n",
    "#     indices = np.arange(dataset_size)\n",
    "\n",
    "#     samples = np.array(dataset.samples)\n",
    "#     targets = np.array(dataset.targets)\n",
    "\n",
    "#     train_val_idx, test_idx = train_test_split(\n",
    "#                                                indices,\n",
    "#                                                test_size=test_split,\n",
    "#                                                random_state=seed,\n",
    "#                                                shuffle=shuffle,\n",
    "#                                                stratify=targets)\n",
    "\n",
    "#     train_val_targets = targets[train_val_idx]\n",
    "\n",
    "#     trainval_indices = np.arange(len(train_val_targets))\n",
    "#     train_idx, val_idx = train_test_split(\n",
    "#                                           trainval_indices,\n",
    "#                                           test_size=val_train_split,\n",
    "#                                           random_state=seed,\n",
    "#                                           shuffle=shuffle,\n",
    "#                                           stratify=train_val_targets)\n",
    "\n",
    "#     train_data = data.select_from_indices(indices=train_idx,\n",
    "#                                           update_class2idx=False,\n",
    "#                                           x_col = 'path',\n",
    "#                                           y_col = \"family\")\n",
    "\n",
    "\n",
    "#     val_data = data.select_from_indices(indices=val_idx,\n",
    "#                                         update_class2idx=False,\n",
    "#                                         x_col = 'path',\n",
    "#                                         y_col = \"family\")\n",
    "\n",
    "\n",
    "#     test_data = data.select_from_indices(indices=test_idx,\n",
    "#                                          update_class2idx=False,\n",
    "#                                          x_col = 'path',\n",
    "#                                          y_col = \"family\")\n",
    "\n",
    "\n",
    "#     return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "g = sns.heatmap(\n",
    "    by_sport, \n",
    "    square=True, # make cells square\n",
    "    cbar_kws={'fraction' : 0.01}, # shrink colour bar\n",
    "    cmap='OrRd', # use orange/red colour map\n",
    "    linewidth=1 # space between cells\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac039848",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "g = sns.heatmap(\n",
    "    by_sport, \n",
    "    square=True,\n",
    "    cbar_kws={'fraction' : 0.01},\n",
    "    cmap='OrRd',\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "g.set_yticklabels(g.get_yticklabels(), rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(\n",
    "    data=data[data['Year'].isin([1980, 2008])],\n",
    "    x='Sport',\n",
    "    kind='count',\n",
    "    palette='Set1',\n",
    "    row='Year',\n",
    "    aspect=3,\n",
    "    height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf16a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "count_dist = collections.Counter(data.targets)\n",
    "# count_dist.update(data.targets)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_class_distributions(targets: List[Any])\n",
    "test = count_dist #{1:1,2:1,3:1,4:2,5:3,6:5,7:4,8:2,9:1,10:1}\n",
    "# with matplotlib\n",
    "plt.hist(list(test.keys()), weights=list(test.values()))\n",
    "\n",
    "test = sorted(test.items(), key = lambda x:x[1], reverse=True)\n",
    "\n",
    "test\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# keys = list(test.keys())\n",
    "# values = list(test.values())\n",
    "\n",
    "keys = [i[0] for i in test]\n",
    "values = [i[1] for i in test]\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "chart = sns.histplot(x=keys, weights=values, discrete=True)\n",
    "plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='x-large'  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# chart.set_xticklabels(\n",
    "#     chart.get_xticklabels(), \n",
    "#     rotation=45, \n",
    "#     horizontalalignment='right',\n",
    "#     fontweight='light',\n",
    "#     fontsize='x-large'\n",
    "    \n",
    "# )\n",
    "\n",
    "# None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851804be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with seaborn (use hist_kws to send arugments to plt.hist, used underneath)\n",
    "sns.distplot(range(len(list(test.keys()))), hist_kws={\"weights\":list(test.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a29f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "\n",
    "max_batches = 10\n",
    "\n",
    "reporter = MemReporter()\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(i, len(batch), batch[0].shape)\n",
    "\n",
    "    print('========= before backward =========')\n",
    "    reporter.report()\n",
    "    out = model(batch[0])\n",
    "    \n",
    "    loss = nn.functional.cross_entropy(out, batch[1])\n",
    "    loss.backward()\n",
    "    print('========= after backward =========')\n",
    "    reporter.report()\n",
    "    \n",
    "    if i>=max_batches:\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcd923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf4ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff21cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68073fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter = MemReporter(model)\n",
    "\n",
    "\n",
    "print('========= before loop =========')\n",
    "reporter.report()\n",
    "for batch in data[]\n",
    "out.backward()\n",
    "print('========= after backward =========')\n",
    "reporter.report()\n",
    "###################################################\n",
    "import torch\n",
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "lstm = torch.nn.LSTM(1024, 1024).cuda()\n",
    "reporter = MemReporter(lstm)\n",
    "reporter.report(verbose=True)\n",
    "inp = torch.Tensor(10, 10, 1024).cuda()\n",
    "out, _ = lstm(inp)\n",
    "out.mean().backward()\n",
    "reporter.report(verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "# with torch.autograd.profiler.profile() as prof:\n",
    "    print('starting')\n",
    "    inputs = torch.randn(5,3,224,224, device='cuda')\n",
    "    print('half way')\n",
    "    outputs = inputs + torch.randn(5,3,224,224, device='cuda')\n",
    "    print('ending')\n",
    "    \n",
    "# print(prof)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dee80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a542c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_memlab import MemReporter\n",
    "# linear = torch.nn.Linear(1024, 1024).cuda()\n",
    "# linear2 = torch.nn.Linear(1024, 1024).cuda()\n",
    "\n",
    "\n",
    "# def inner():\n",
    "#     torch.nn.Linear(100, 100).cuda()\n",
    "\n",
    "# def outer():\n",
    "#     linear = torch.nn.Linear(100, 100).cuda()\n",
    "#     linear2 = torch.nn.Linear(100, 100).cuda()\n",
    "#     inner()\n",
    "# reporter = MemReporter()\n",
    "# reporter.report()\n",
    "\n",
    "linear = torch.nn.Linear(1024, 1024).cuda()\n",
    "inp = torch.Tensor(512, 1024).cuda()\n",
    "# pass in a model to automatically infer the tensor names\n",
    "reporter = MemReporter(linear)\n",
    "out = linear(inp).mean()\n",
    "print('========= before backward =========')\n",
    "reporter.report()\n",
    "out.backward()\n",
    "print('========= after backward =========')\n",
    "reporter.report()\n",
    "###################################################\n",
    "import torch\n",
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "lstm = torch.nn.LSTM(1024, 1024).cuda()\n",
    "reporter = MemReporter(lstm)\n",
    "reporter.report(verbose=True)\n",
    "inp = torch.Tensor(10, 10, 1024).cuda()\n",
    "out, _ = lstm(inp)\n",
    "out.mean().backward()\n",
    "reporter.report(verbose=True)\n",
    "\n",
    "# import torch\n",
    "# from pytorch_memlab import LineProfiler\n",
    "\n",
    "# def inner():\n",
    "#     torch.nn.Linear(100, 100).cuda()\n",
    "\n",
    "# def outer():\n",
    "#     linear = torch.nn.Linear(100, 100).cuda()\n",
    "#     linear2 = torch.nn.Linear(100, 100).cuda()\n",
    "#     inner()\n",
    "\n",
    "# with LineProfiler(outer, inner) as prof:\n",
    "#     outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pytorch_memlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522ef50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.print_stats()\n",
    "\n",
    "dir(prof)\n",
    "# type(prof)\n",
    "\n",
    "prof.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cb2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "# with torch.autograd.profiler.profile() as prof:\n",
    "    print('starting')\n",
    "    inputs = torch.randn(5,3,224,224, device='cuda')\n",
    "    print('half way')\n",
    "    outputs = inputs + torch.randn(5,3,224,224, device='cuda')\n",
    "    print('ending')\n",
    "    \n",
    "# print(prof)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "\n",
    "# if i % 1000 == 0:\n",
    "#     print(\"Iteration: {}, memory: {}\".format(i, psutil.virtual_memory()))\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.profiler.PyTorchProfiler(dirpath=None,\n",
    "#                             filename=None,\n",
    "#                             group_by_input_shapes=False,\n",
    "#                             emit_nvtx=False,\n",
    "#                             export_to_chrome=True,\n",
    "#                             row_limit=20,\n",
    "#                             sort_by_key=None,\n",
    "#                             record_functions=None,\n",
    "#                             record_module_names=True,\n",
    "#                             profiled_functions=None,\n",
    "#                             output_filename=None, \n",
    "#                             **profiler_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = [0,1,2,3,4]\n",
    "# idx = 10\n",
    "# data.display_grid(idx, repeat_n=5)\n",
    "data.display_grid(idx, repeat_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "self=data\n",
    "indices=10\n",
    "repeat_n=5\n",
    "from itertools import repeat, chain\n",
    "from more_itertools import collapse\n",
    "import random\n",
    "indices = random.sample(range(self.num_samples), indices)\n",
    "idx = collapse((repeat(i,repeat_n) for i in indices))\n",
    "\n",
    "# print([i for i in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self = data\n",
    "# indices = idx\n",
    "\n",
    "# if isinstance(indices, int):\n",
    "#     indices = random.sample(range(self.num_samples), indices)\n",
    "# indices = list(indices)\n",
    "# images = [self[idx][0] for idx in indices]\n",
    "# labels = [self.classes[self[idx][1]] for idx in indices]\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.display_grid(idx)\n",
    "plt.suptitle(f\"{idx} random images\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888efe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a088d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417391ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df15be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from PIL.Image import Image as PilImage\n",
    "# import textwrap, os\n",
    "\n",
    "# def display_images(\n",
    "#     images: [PilImage], \n",
    "#     columns=5, max_images=15,\n",
    "#     width=20, height=8,    \n",
    "#     label_wrap_length=50, \n",
    "#     label_font_size=8):\n",
    "\n",
    "#     if not images:\n",
    "#         print(\"No images to display.\")\n",
    "#         return \n",
    "\n",
    "#     if len(images) > max_images:\n",
    "#         print(f\"Showing {max_images} images of {len(images)}:\")\n",
    "#         images=images[0:max_images]\n",
    "\n",
    "#     rows = int(len(images)/columns)\n",
    "        \n",
    "#     height = max(height, rows * height)\n",
    "#     plt.figure(figsize=(width, height))\n",
    "#     for i, image in enumerate(images):\n",
    "\n",
    "#         plt.subplot(rows + 1, columns, i + 1)\n",
    "#         plt.imshow(image)\n",
    "\n",
    "#         if hasattr(image, 'filename'):\n",
    "#             title=image.filename\n",
    "#             if title.endswith(\"/\"): title = title[0:-1]\n",
    "#             title=os.path.basename(title)\n",
    "#             title=textwrap.wrap(title, label_wrap_length)\n",
    "#             title=\"\\n\".join(title)\n",
    "#             plt.title(title, fontsize=label_font_size); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f882f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[5][0]\n",
    "\n",
    "import random\n",
    "num_display = 12\n",
    "\n",
    "indices = random.sample(range(data.num_samples), num_display)\n",
    "\n",
    "indices\n",
    "\n",
    "# indices = range(0,4)\n",
    "\n",
    "display_images(\n",
    "    images = [data[idx][0] for idx in indices],\n",
    "    columns=5, max_images=15,\n",
    "    width=20, height=8,    \n",
    "    label_wrap_length=50, \n",
    "    label_font_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1205d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def parse_item(self, index: int):\n",
    "#         path = self.files[index]\n",
    "#         family, genus, species, collection, catalog_number = path.stem.split(\"_\", maxsplit=4)\n",
    "#         item = {\"path\":path,\n",
    "#                 \"target\":None,\n",
    "#                 \"family\":family,\n",
    "#                 \"genus\":genus,\n",
    "#                 \"species\":species,\n",
    "#                 \"collection\":collection,\n",
    "#                 \"catalog_number\":catalog_number}\n",
    "#         item[\"target\"] = item[self.class_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159ae47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b06b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae0a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa9a4b7b",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Previous Fossil class def code, now relocated to fossil.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c80876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import flatten\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009aca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets = {\n",
    "    \"Wilf_Fossil_512\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Wilf_Fossil\",\n",
    "    \"Wilf_Fossil_1024\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Wilf_Fossil\",\n",
    "    \"Wilf_Fossil_1536\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Wilf_Fossil\",\n",
    "    \"Wilf_Fossil_2048\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Wilf_Fossil\",\n",
    "    \n",
    "    \"Florissant_Fossil_512\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Florissant_Fossil\",\n",
    "    \"Florissant_Fossil_1024\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Florissant_Fossil\",\n",
    "    \"Florissant_Fossil_1536\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Florissant_Fossil\",\n",
    "    \"Florissant_Fossil_2048\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Florissant_Fossil\"\n",
    "}\n",
    "\n",
    "available_datasets[\"Fossil_512\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Wilf_Fossil\",\n",
    "                                    \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Florissant_Fossil\"]\n",
    "available_datasets[\"Fossil_1024\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Wilf_Fossil\",\n",
    "                                     \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Florissant_Fossil\"]\n",
    "available_datasets[\"Fossil_1536\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Wilf_Fossil\",\n",
    "                                     \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Florissant_Fossil\"]\n",
    "available_datasets[\"Fossil_2048\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Wilf_Fossil\",\n",
    "                                     \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Florissant_Fossil\"]\n",
    "\n",
    "fossil_collections = {\"Florissant\":\"florissant_fossil\",\n",
    "                      \"Wilf\":\"wilf_fossil\"}\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    name: str\n",
    "    dataset: str=None\n",
    "    collection: str=None\n",
    "    resolution: int=None\n",
    "        \n",
    "    num_files: Optional[int]=None\n",
    "    num_classes: Optional[int]=None\n",
    "    class_type: str=\"family\"\n",
    "    path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "                \n",
    "        \n",
    "    def __init__(self, name: str, **kwargs):\n",
    "        self.name = name\n",
    "        parts = self.name.split(\"_\")\n",
    "        self.resolution = int(parts[-1])\n",
    "        if len(parts)==3:\n",
    "            self.dataset = parts[1]\n",
    "            self.collection = \"_\".join(parts[:2])\n",
    "        elif len(parts)==2:    \n",
    "            self.dataset = parts[0]\n",
    "            self.collection = [\"_\".join([c, self.dataset]) for c in fossil_collections.keys()]\n",
    "            \n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        disp = f\"\"\"<{str(type(self)).strip(\"'>\").split('.')[1]}>:\"\"\"\n",
    "        \n",
    "        disp += \"\\nFields:\\n\"\n",
    "        for k in self.__dataclass_fields__.keys():\n",
    "            disp += f\"    {k}: {getattr(self,k)}\\n\"\n",
    "        return disp\n",
    "    \n",
    "\n",
    "DatasetConfig(\"Fossil_512\")\n",
    "\n",
    "class FossilDataset(torchdata.datasets.Files): #ImageDataset):\n",
    "    \n",
    "#     loader: Callable = Image.open\n",
    "    transform = None\n",
    "    target_transform = None\n",
    "    \n",
    "    class_type: str=\"family\"\n",
    "    totensor: Callable = torchvision.transforms.ToTensor()\n",
    "    def __init__(self,\n",
    "                 files: List[Path],\n",
    "                 return_items: List[str] = [\"image\",\"target\",\"path\"],\n",
    "                 image_return_type: str = \"tensor\",\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(files=files, *args, **kwargs)\n",
    "        \n",
    "        self.name = kwargs.get(\"name\",\"\")\n",
    "        self.return_items = return_items\n",
    "        self.image_return_type = image_return_type\n",
    "        \n",
    "        self.samples = [self.parse_item(idx) for idx in range((len(self)))]\n",
    "        self.targets = [sample[1] for sample in self.samples]\n",
    "        self._imgs = None\n",
    "        self.classes = sorted(set(self.targets))\n",
    "        self.class2idx = {name:idx for idx, name in enumerate(self.classes)}\n",
    "        \n",
    "        self.config = DatasetConfig(self.name,\n",
    "                                    class_type=self.class_type,\n",
    "                                    num_files=len(self.files),\n",
    "                                    num_classes=len(self.classes)\n",
    "                                   )\n",
    "\n",
    "    def getitem(self, index: int):\n",
    "        path, family, genus, species, collection, catalog_number = self.samples[index]\n",
    "        img = Image.open(path)\n",
    "        return img, family, path\n",
    "\n",
    "#     @property\n",
    "#     def transform(self) -> Callable:#, img: PIL.Image):\n",
    "#         _transforms = []\n",
    "#         if self.image_return_type == \"tensor\":\n",
    "#             _transforms.append(self.totensor)\n",
    "#         return lambda x: x\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        img, family, path = self.getitem(index)\n",
    "        target = self.class2idx[family]\n",
    "        \n",
    "        if self.image_return_type == \"tensor\":\n",
    "            img = self.totensor(img)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target, path\n",
    "    \n",
    "    \n",
    "    def parse_item(self, index: int):\n",
    "        path = self.files[index]\n",
    "        family, genus, species, collection, catalog_number = path.stem.split(\"_\", maxsplit=4)\n",
    "        return path, family, genus, species, collection, catalog_number\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.config.__repr__()\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "        dataset_dirs = available_datasets[name]\n",
    "        if isinstance(available_datasets[name], str):\n",
    "            dataset_dirs = [available_datasets[name]]\n",
    "        assert isinstance(dataset_dirs, list)\n",
    "        file_list = list(flatten(\n",
    "                            [torchdata.datasets.Files.from_folder(Path(root),\n",
    "                                                                  regex=\"*/*.jpg\").files\n",
    "                             for root in dataset_dirs]\n",
    "                                                    ))\n",
    "        data = FossilDataset(file_list,\n",
    "                             name=name)\n",
    "\n",
    "        return data #.map(lambda x: (torchvision.transforms.ToTensor()(x[0]), x[1]))\n",
    "    \n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "#         dataset_dirs = available_datasets[name]\n",
    "#         if isinstance(available_datasets[name], list):\n",
    "#             file_list = list(flatten(\n",
    "#                                 [torchdata.datasets.Files.from_folder(Path(root),\n",
    "#                                                         regex=\"*/*.jpg\").files\n",
    "#                                for root in available_datasets[name]]\n",
    "#                                                             ))\n",
    "            \n",
    "#         elif isinstance(available_datasets[name], str):\n",
    "#             file_list = torchdata.datasets.Files.from_folder(Path(available_datasets[name]),\n",
    "#                                                              regex=\"*/*.jpg\").files\n",
    "\n",
    "#         data = FossilDataset(file_list,\n",
    "#                                  name=name)\n",
    "\n",
    "#         return data.map(torchvision.transforms.ToTensor())\n",
    "\n",
    "fossil_data = FossilDataset.create_dataset(name=\"Fossil_1024\")\n",
    "fossil_data\n",
    "\n",
    "# fossil_data = FossilDataset.create_dataset(name=\"Florissant_Fossil_1024\")\n",
    "# fossil_data\n",
    "\n",
    "# fossil_data = FossilDataset.create_dataset(name=\"Wilf_Fossil_1024\")\n",
    "# fossil_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51be3b",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Future todo: Separate subclass of simpler Leaves/Fossil Dataset class to allow for more customization of return signatures (allowing dict records instead of tuple, multiple labels per sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf057c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelFossilDataset(FossilDataset): #ImageDataset):\n",
    "    \n",
    "    loader: Callable = Image.open\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    target_transform = None\n",
    "    \n",
    "    class_type: str=\"family\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 files: List[Path],\n",
    "                 return_items: List[str,str] = [\"image\",\"target\",\"path\"],\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(files=files, *args, **kwargs)\n",
    "        \n",
    "        self.name = kwargs.get(\"name\",\"\")\n",
    "        self.return_items = return_items\n",
    "        \n",
    "        self.samples = [self.parse_item(idx) for idx in range((len(self)))]            \n",
    "        self.targets = [sample[1] for sample in self.samples]\n",
    "        self.classes = sorted(set(self.targets))\n",
    "        self.class2idx = {name:idx for idx, name in enumerate(self.classes)}\n",
    "        \n",
    "        self.config = DatasetConfig(self.name,\n",
    "                                    class_type=self.class_type,\n",
    "                                    num_files=len(self.files),\n",
    "                                    num_classes=len(self.classes)\n",
    "                                    )\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def get_item(self, index: int):\n",
    "        item = self.samples[index]\n",
    "        img = self.loader(item[\"path\"])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return self.get_item((img, target, path))\n",
    "    \n",
    "        return Image.open(item[0]), self.class2idx[item[1]]\n",
    "\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.samples[index]\n",
    "        \n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return self.get_item((img, target, path))\n",
    "#         return Image.open(item[0]), self.class2idx[item[1]]\n",
    "    \n",
    "    def parse_item(self, index: int):\n",
    "        path = self.files[index]\n",
    "        family, genus, species, collection, catalog_number = path.stem.split(\"_\", maxsplit=4)\n",
    "        item = {\"path\":path,\n",
    "                \"target\":None,\n",
    "                \"family\":family,\n",
    "                \"genus\":genus,\n",
    "                \"species\":species,\n",
    "                \"collection\":collection,\n",
    "                \"catalog_number\":catalog_number}\n",
    "        item[\"target\"] = item[self.class_type]\n",
    "#         return path, family, genus, species, collection, catalog_number\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.config.__repr__()\n",
    "        \n",
    "#     @property\n",
    "#     def config(self):\n",
    "#         return DatasetConfig(self.name,\n",
    "#                              num_files=len(self.files),\n",
    "#                              num_classes=len(self.classes)\n",
    "#                             )\n",
    "    @classmethod\n",
    "    def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "        dataset_dirs = available_datasets[name]\n",
    "        if isinstance(available_datasets[name], str):\n",
    "            dataset_dirs = [available_datasets[name]]\n",
    "        assert isinstance(dataset_dirs, list)\n",
    "        file_list = list(flatten(\n",
    "                            [torchdata.datasets.Files.from_folder(Path(root),\n",
    "                                                                  regex=\"*/*.jpg\").files\n",
    "                             for root in dataset_dirs]\n",
    "                                                    ))\n",
    "        data = FossilDataset(file_list,\n",
    "                             name=name)\n",
    "\n",
    "        return data.map(lambda x: (torchvision.transforms.ToTensor()(x[0]), x[1]))\n",
    "    \n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "#         dataset_dirs = available_datasets[name]\n",
    "#         if isinstance(available_datasets[name], list):\n",
    "#             file_list = list(flatten(\n",
    "#                                 [torchdata.datasets.Files.from_folder(Path(root),\n",
    "#                                                         regex=\"*/*.jpg\").files\n",
    "#                                for root in available_datasets[name]]\n",
    "#                                                             ))\n",
    "            \n",
    "#         elif isinstance(available_datasets[name], str):\n",
    "#             file_list = torchdata.datasets.Files.from_folder(Path(available_datasets[name]),\n",
    "#                                                              regex=\"*/*.jpg\").files\n",
    "\n",
    "#         data = FossilDataset(file_list,\n",
    "#                                  name=name)\n",
    "\n",
    "#         return data.map(torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a298c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(fossil_data.samples[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fossil_data.class2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad641b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310b72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bebbe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ImageDataset(torchdata.datasets.Files):\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return Image.open(self.files[index])\n",
    "    \n",
    "# #     def __repr__(self):\n",
    "# #         return f\"\"\"{self.kwargs['name']}\"\"\"\n",
    "# #         return f\"\"\"ImageDataset: {self.kwargs['name']}\"\"\"\n",
    "\n",
    "#     def __init__(self, files: List[Path], *args, **kwargs):\n",
    "#         super().__init__(files=files, *args, **kwargs)\n",
    "        \n",
    "# #         self.name = kwargs.get(\"name\",\"\")\n",
    "# #         self.cfg = DatasetConfig(self.name)\n",
    "\n",
    "\n",
    "#     @classmethod\n",
    "#     def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "#         dataset_dirs = available_datasets[name]\n",
    "\n",
    "#         if isinstance(available_datasets[name], str):\n",
    "#             data = ImageDataset.from_folder(Path(available_datasets[name]),\n",
    "#                                             regex=\"*/*.jpg\",\n",
    "#                                             name=name)\n",
    "#         return data.map(torchvision.transforms.ToTensor())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28843f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb3926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092c01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7f024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f317ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85485f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25984b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import os\n",
    "import types\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import torch\n",
    "import timm\n",
    "from rich import print\n",
    "import matplotlib.pyplot as plt\n",
    "from contrastive_learning.data.pytorch.pnas import PNASLightningDataModule\n",
    "from contrastive_learning.data.pytorch.extant import ExtantLightningDataModule\n",
    "from contrastive_learning.data.pytorch.common import DataStageError, LeavesLightningDataModule\n",
    "\n",
    "from lightning_hydra_classifiers.callbacks.wandb_callbacks import WatchModelWithWandb, LogPerClassMetricsToWandb, WandbClassificationCallback # LogConfusionMatrixToWandb\n",
    "from lightning_hydra_classifiers.models.resnet import ResNet, get_scalar_metrics\n",
    "import lightning_hydra_classifiers\n",
    "from torch import nn\n",
    "import inspect\n",
    "\n",
    "import wandb\n",
    "pl.trainer.seed_everything(seed=9)\n",
    "\n",
    "    \n",
    "# class Config:\n",
    "#     pass\n",
    "\n",
    "\n",
    "# config = Config()\n",
    "\n",
    "# # config.model_name = 'resnet50'\n",
    "# # config.dataset_name = 'PNAS_family_100_512'\n",
    "# config.dataset_name = '(Extant-PNAS)_family_10_512'\n",
    "# config.normalize = True\n",
    "# config.num_workers = 4\n",
    "# config.batch_size = 16\n",
    "\n",
    "# config = Box({\n",
    "#     \"dataset\":{\n",
    "#         namef\"PNAS_{label_type}_{pnas_threshold}\"\n",
    "#     }\n",
    "    \n",
    "# })\n",
    "\n",
    "########################################\n",
    "# if 'Extant' in config.dataset_name:\n",
    "#     datamodule = ExtantLightningDataModule(name=config.dataset_name, batch_size=config.batch_size, debug=False, normalize=config.normalize, num_workers=config.num_workers)\n",
    "# elif 'PNAS' in config.dataset_name:\n",
    "#     datamodule = PNASLightningDataModule(name=config.dataset_name, batch_size=config.batch_size, debug=False, normalize=config.normalize, num_workers=config.num_workers)#, normalize=False)#True)\n",
    "# datamodule.setup('fit')\n",
    "# ########################################\n",
    "# num_classes = len(datamodule.classes)\n",
    "# config.num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from box import Box\n",
    "import os\n",
    "\n",
    "os.environ['WANDB_CACHE_DIR'] = \"/media/data/jacob/wandb_cache\"\n",
    "class_type = \"family\"\n",
    "extant_threshold = 10\n",
    "pnas_threshold = 100\n",
    "image_size = 512\n",
    "seed = 257\n",
    "\n",
    "config = Box({})\n",
    "\n",
    "config.datasets = [{\n",
    "                  \"name\": f\"PNAS_{class_type}_{pnas_threshold}_{image_size}\",\n",
    "                  \"batch_size\":32,\n",
    "                  \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "                  \"num_workers\":4,\n",
    "                  \"image_size\":image_size,\n",
    "                  \"channels\":3,\n",
    "                  \"class_type\":class_type,\n",
    "                  \"debug\":False,\n",
    "                  \"normalize\":True,\n",
    "                  \"seed\":seed,\n",
    "                  \"dataset_dir\":None,\n",
    "                  \"predict_on_split\":\"val\",\n",
    "                  },\n",
    "    {\n",
    "                  \"name\":f\"Extant_{class_type}_{extant_threshold}_{image_size}\",  # f\"PNAS_{label_type}_{pnas_threshold}_{image_size}\"\n",
    "                  \"batch_size\":32,\n",
    "                  \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "                  \"num_workers\":4,\n",
    "                  \"image_size\":image_size,\n",
    "                  \"channels\":3,\n",
    "                  \"class_type\":class_type,\n",
    "                  \"debug\":False,\n",
    "                  \"normalize\":True,\n",
    "                  \"seed\":seed,\n",
    "                  \"dataset_dir\":None,\n",
    "                  \"predict_on_split\":\"val\",\n",
    "                  }]\n",
    "\n",
    "\n",
    "\n",
    "config.wandb = {\n",
    "                \"init\":\n",
    "                       {\n",
    "                        \"entity\":\"jrose\",\n",
    "                        \"project\":\"image_classification_datasets\",\n",
    "                        \"job_type\":'create-dataset',\n",
    "                        \"group\":None,\n",
    "                        \"run_dir\":os.environ['WANDB_CACHE_DIR'],\n",
    "                        \"tags\":[d.name for d in config.datasets]\n",
    "                       },\n",
    "                \"artifacts\":\n",
    "                        {\n",
    "                        \"root_dir\":None\n",
    "                        },\n",
    "                \"input_artifacts\":\n",
    "                       [\n",
    "                           {\n",
    "                            \"entity\":\"jrose\",\n",
    "                            \"project\":\"image_classification_datasets\",\n",
    "                            \"name\": config.datasets[0].name,\n",
    "                            \"version\": \"v6\",\n",
    "                            \"type\": \"raw_data\",\n",
    "                            \"root_dir\":None,\n",
    "                            \"uri\":None\n",
    "                           }\n",
    "                       ]\n",
    "}\n",
    "\n",
    "i = 0\n",
    "\n",
    "config.wandb.artifacts.root_dir = os.path.join(config.wandb.init.run_dir,\n",
    "                                               \"artifacts\")\n",
    "\n",
    "config.wandb.input_artifacts[i].uri = \"/\".join([config.wandb.input_artifacts[i].entity,\n",
    "                                                config.wandb.input_artifacts[i].project,\n",
    "                                                config.wandb.input_artifacts[i].name]) \\\n",
    "                                           + f':{config.wandb.input_artifacts[i].version}'\n",
    "\n",
    "\n",
    "config.wandb.input_artifacts[i].root_dir = os.path.join(config.wandb.artifacts.root_dir,\n",
    "                                                        \"datasets\",\n",
    "                                                         config.wandb.input_artifacts[i].name \\\n",
    "                                                         + f':{config.wandb.input_artifacts[i].version}'\n",
    "                                                        )\n",
    "\n",
    "\n",
    "# def fetch_datamodule_from_dataset_artifact(config: Box, run_or_api=None) -> LeavesLightningDataModule:\n",
    "#     run = run_or_api or wandb.Api()\n",
    "#     artifact = run.use_artifact(config.wandb.input_artifact.uri,\n",
    "#                                 type=config.wandb.input_artifact.type)\n",
    "#     dataset_artifact_dir = artifact.download(root=config.wandb.input_artifact.root_dir)\n",
    "\n",
    "\n",
    "#     datamodule = get_datamodule(config.dataset)\n",
    "#     datamodule.setup('fit')\n",
    "#     datamodule.setup('test')\n",
    "#     ########################\n",
    "#     config.model.num_classes = config.dataset.num_classes\n",
    "\n",
    "def fetch_datamodule_from_dataset_artifact(config: Box, run_or_api=None) -> LeavesLightningDataModule:\n",
    "    run = run_or_api or wandb.Api()\n",
    "    artifact = run.use_artifact(config.wandb.input_artifact.uri,\n",
    "                                type=config.wandb.input_artifact.type)\n",
    "    dataset_artifact_dir = artifact.download(root=config.wandb.input_artifact.root_dir)\n",
    "\n",
    "\n",
    "    datamodule = get_datamodule(config.dataset)\n",
    "    datamodule.setup('fit')\n",
    "    datamodule.setup('test')\n",
    "    ########################\n",
    "    config.model.num_classes = config.dataset.num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ac71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageStat\n",
    "import seaborn_image as isns\n",
    "import scipy\n",
    "\n",
    "def image_stat(img: np.ndarray):\n",
    "    if img.ndim==3:\n",
    "        h, w, c = img.shape\n",
    "    else:\n",
    "        h, w, c = (*img.shape, 1)\n",
    "    return {\n",
    "        \"min\":np.min(img),\n",
    "        \"max\":np.max(img),\n",
    "        \"var\":np.var(img),\n",
    "        \"mean\":np.mean(img),\n",
    "        \"mode\":scipy.stats.mode(img,axis=None),\n",
    "        \"height\":h,\n",
    "        \"width\":w,\n",
    "        \"channels\":c,\n",
    "        \"num_pixels\":h*w*c\n",
    "    }\n",
    "\n",
    "\n",
    "def load_and_analyze_image(image_path: str):\n",
    "    img = np.array(Image.open(image_path))\n",
    "    return img, image_stat(img)\n",
    "\n",
    "# def load_analyze_and_save_annotated_image(image_path: str):\n",
    "#     img, stats = load_and_analyze_image(image_path)\n",
    "#     return img, image_stat(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fig2data ( fig ):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a numpy 3D array of RGBA values\n",
    "    \"\"\"\n",
    "    # draw the renderer\n",
    "    fig.canvas.draw()\n",
    " \n",
    "    # Get the RGBA buffer from the figure\n",
    "    w,h = fig.canvas.get_width_height()\n",
    "    buf = np.fromstring(fig.canvas.tostring_argb(), dtype=np.uint8)\n",
    "    buf.shape = ( w, h, 4 )\n",
    " \n",
    "    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n",
    "    buf = np.roll( buf, 3, axis = 2 )\n",
    "    return buf\n",
    "\n",
    " \n",
    "def fig2img ( fig ):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a PIL Image in RGBA format and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a Python Imaging Library ( PIL ) image\n",
    "    \"\"\"\n",
    "    # put the figure pixmap into a numpy array\n",
    "    buf = fig2data(fig)\n",
    "    w, h, d = buf.shape\n",
    "    return Image.frombytes(\"RGBA\", (w, h), buf.tostring())\n",
    "\n",
    "# f = isns.imghist(img_array,\n",
    "#                  describe=True)\n",
    "# results = fig2img(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc10860",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.wandb.input_artifacts[0].uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed4d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def trainvaltest_split(x: Union[List[Any],np.ndarray]=None,\n",
    "#                        y: Union[List[Any],np.ndarray]=None,\n",
    "#                        splits: List[float]=(0.5, 0.2, 0.3),\n",
    "#                        random_state: int=None,\n",
    "#                        stratify: bool=True\n",
    "#                        ) -> Dict[str,Tuple[np.ndarray]]:\n",
    "#     \"\"\"\n",
    "#     Wrapper function to split data into 3 stratified subsets specified by `splits`.\n",
    "    \n",
    "#     User specifies absolute fraction of total requested for each subset (e.g. splits=[0.5, 0.2, 0.3])\n",
    "    \n",
    "#     Function calculates adjusted fractions necessary in order to use sklearn's builtin train_test_split function over a sequence of 2 steps.\n",
    "    \n",
    "#     Step 1: Separate test set from the rest of the data (constituting the union of train + val)\n",
    "    \n",
    "#     Step 2: Separate the train and val sets from the remainder produced by step 1.\n",
    "    \n",
    "    \n",
    "    \n",
    "#     Output:\n",
    "#         Dict: {'train':(x_train, y_train),\n",
    "#                 'val':(x_val_y_val),\n",
    "#                 'test':(x_test, y_test)}\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     assert len(splits) == 3, \"Must provide eactly 3 float values for `splits`\"\n",
    "#     assert np.isclose(np.sum(splits), 1.0), f\"Sum of all splits values {splits} = {np.sum(splits)} must be 1.0\"\n",
    "    \n",
    "#     train_split, val_split, test_split = splits\n",
    "#     val_relative_split = val_split/(train_split + val_split)\n",
    "#     train_relative_split = train_split/(train_split + val_split)\n",
    "    \n",
    "    \n",
    "#     if stratify and (y is None):\n",
    "#         raise ValueError(\"If y is not provided, stratify must be set to False.\")\n",
    "    \n",
    "#     y = np.array(y)\n",
    "#     if x is None:\n",
    "#         x = np.arange(len(y))\n",
    "#     else:\n",
    "#         x = np.array(x)\n",
    "    \n",
    "#     stratify_y = y if stratify else None    \n",
    "#     x_train_val, x_test, y_train_val, y_test = train_test_split(x, y,\n",
    "#                                                         test_size=test_split, \n",
    "#                                                         random_state=random_state,\n",
    "#                                                         stratify=y)\n",
    "#     log.info(f\"(x_train+x_val).shape={x_train_val.shape}, (y_train+y_val).shape={y_train_val.shape}\")\n",
    "#     log.info(f\"x_test.shape={x_test.shape}, y_test.shape={y_test.shape}\")\n",
    "    \n",
    "# #     print(f\"(x_train+x_val).shape={x_train_val.shape}, (y_train+y_val).shape={y_train_val.shape}\")\n",
    "# #     print(f\"x_test.shape={x_test.shape}, y_test.shape={y_test.shape}\")\n",
    "\n",
    "#     stratify_y_train = y_train_val if stratify else None\n",
    "#     x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val,\n",
    "#                                                       test_size=val_relative_split,\n",
    "#                                                       random_state=random_state, \n",
    "#                                                       stratify=y_train_val)\n",
    "    \n",
    "#     x = np.concatenate((x_train, x_val, x_test)).tolist()\n",
    "#     assert len(set(x)) == len(x), f\"[Warning] Check for possible data leakage. len(set(x))={len(set(x))} != len(x)={len(x)}\"\n",
    "    \n",
    "#     log.info(f\"x_train.shape={x_train.shape}, y_train.shape={y_train.shape}\")\n",
    "#     log.info(f\"x_val.shape={x_val.shape}, y_val.shape={y_val.shape}\")\n",
    "    \n",
    "#     log.info(f'Absolute splits: {[train_split, val_split, test_split]}')\n",
    "#     log.info(f'Relative splits: [{train_relative_split:.2f}, {val_relative_split:.2f}, {test_split}]')\n",
    "    \n",
    "#     return {\"train\":(x_train, y_train),\n",
    "#             \"val\":(x_val, y_val),\n",
    "#             \"test\":(x_test, y_test)}\n",
    "\n",
    "#####################\n",
    "\n",
    "# y = data.targets\n",
    "\n",
    "# data_splits = trainvaltest_split(x=None,\n",
    "#                                  y=y,\n",
    "#                                  splits=(0.5, 0.2, 0.3),\n",
    "#                                  random_state=0,\n",
    "#                                  stratify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97846d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(config.wandb.input_artifacts[0].uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec003a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(artifact)\n",
    "# dir(artifact.manifest)\n",
    "# artifact.manifest.entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = artifact.get('dataset/test.table.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bee4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe80c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.data\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame(data=df, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc038fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_samples = data_df\n",
    "samples = list(data_df[['image', 'label']].itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = len(set(wide_samples.label.values))\n",
    "plt.bar(range(num_classes), wide_samples.groupby(\"label\")[\"catalog_number\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18707206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eefb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_mem = data_df.image.apply(lambda x: np.array(x._image))\n",
    "in_mem = data_df.image.apply(lambda x: x._image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bfc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_mem[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(data_df.image[0]._image).shape\n",
    "\n",
    "\n",
    "in_mem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "df = data.get_column('image')\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3451865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c41d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(artifact)\n",
    "\n",
    "downloaded_artifact = artifact.checkout(root=config.wandb.input_artifacts[0].root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.abspath\n",
    "(downloaded_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1351f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contrastive_learning.data.pytorch.pnas import PNASLeavesDataset\n",
    "from contrastive_learning.data.pytorch.extant import ExtantLeavesDataset\n",
    "# from contrastive_learning.data.pytorch.common import DataStageError\n",
    "from paleoai_data.dataset_drivers import base_dataset\n",
    "\n",
    "# Step 1. Instantiate PyTorch Datasets for each of Extant Leaves & PNAS, separately\n",
    "pnas_torch = PNASLeavesDataset(name = f\"PNAS_{label_type}_{pnas_threshold}\",\n",
    "                 split: str=\"train\",\n",
    "                 dataset_dir: Optional[str]=None,\n",
    "                 return_paths: bool=False,)\n",
    "extant_torch = ExtantLeavesDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def create_dataset_by_name(name: str,\n",
    "#                            version: str='v0.2',\n",
    "#                            exclude_classes = ['notcataloged','notcatalogued', 'II. IDs, families uncertain', 'Unidentified']):\n",
    "#     data_df = query_db(version=version, **{'dataset':name})\n",
    "#     dataset = base_dataset.BaseDataset.from_dataframe(df=data_df, name=name, exclude_classes=exclude_classes)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3af695",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=WANDB_PROJECT, job_type=\"model_result_analysis\") as run:\n",
    "    \n",
    "    # Retrieve the original raw dataset\n",
    "    dataset_artifact = run.use_artifact(\"raw_data:latest\")\n",
    "    data_table = dataset_artifact.get(\"raw_examples\")\n",
    "    \n",
    "    # Retrieve the train and test score tables\n",
    "    train_artifact = run.use_artifact(\"train_results:latest\")\n",
    "    train_table = train_artifact.get(\"train_iou_score_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60269f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = PNASLightningDataModule(batch_size=16)\n",
    "# data = ExtantLightningDataModule(batch_size=16, num_workers=12)\n",
    "# data.setup(stage='fit')\n",
    "\n",
    "# data.setup(stage='test')\n",
    "\n",
    "# data.setup(stage=None)\n",
    "\n",
    "# try:\n",
    "#     data.setup(stage='other')\n",
    "#     print('success')\n",
    "# except DataStageError as e:\n",
    "#     print(e.with_traceback(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.setup(stage='fit')\n",
    "train_dataloader = data.get_dataloader(stage='train')\n",
    "val_dataloader = data.get_dataloader(stage='val')\n",
    "data.setup(stage='test')\n",
    "test_dataloader = data.get_dataloader(stage='test')\n",
    "\n",
    "# train_dataloader\n",
    "#         if stage=='train': return self.train_dataloader()\n",
    "#         if stage=='val': return self.val_dataloader()\n",
    "#         if stage=='test': return self.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data.train_dataset.transform = None #data.default_train_transforms() #None\n",
    "x, y = data.train_dataset[0]\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb14f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import ImageOps\n",
    "\n",
    "# print(x.max(), x.min())\n",
    "# plt.imshow(ImageOps.invert(x))#.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_idx = 0\n",
    "\n",
    "data.show_batch('train', batch_idx=batch_idx)\n",
    "# data.show_batch('train', cmap='plasma')\n",
    "plt.savefig(f'ExtantLeaves v0_3 train batch {batch_idx}.png')\n",
    "\n",
    "data.show_batch('val', batch_idx=batch_idx)\n",
    "plt.savefig(f'ExtantLeaves v0_3 val batch {batch_idx}.png')\n",
    "\n",
    "data.show_batch('test', batch_idx=batch_idx)\n",
    "plt.savefig(f'ExtantLeaves v0_3 test batch {batch_idx}.png')\n",
    "# data.show_batch('train', cmap='magma')\n",
    "# data.show_batch('train', cmap='cividis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef900e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9f1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bc059",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = data\n",
    "stage = 'test'\n",
    "batch_idx = 0\n",
    "\n",
    "x, y = self.get_batch(stage=stage, batch_idx=batch_idx)\n",
    "\n",
    "x = x[:12,...]\n",
    "\n",
    "batch_size = x.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(24,24))\n",
    "grid_img = torchvision.utils.make_grid(x, nrow=int(np.ceil(np.sqrt(batch_size))))\n",
    "\n",
    "img_min, img_max = grid_img.min(), grid_img.max()\n",
    "print(img_min, img_max)\n",
    "\n",
    "grid_img = (grid_img - img_min)/(img_max - img_min)\n",
    "img_min, img_max = grid_img.min(), grid_img.max()\n",
    "print(img_min, img_max)\n",
    "\n",
    "\n",
    "\n",
    "print('before:', grid_img.shape)\n",
    "\n",
    "if torch.argmin(torch.Tensor(grid_img.shape)) == 0:\n",
    "    grid_img = grid_img.permute(1,2,0)\n",
    "print('after:', grid_img.shape)\n",
    "\n",
    "img_ax = ax.imshow(grid_img[:,:,0], cmap='viridis')#, vmin = img_min, vmax = img_max)\n",
    "fig.colorbar(img_ax, ax=ax)#)#cax=ax)\n",
    "plt.axis('off')\n",
    "plt.suptitle(f'{stage} batch')\n",
    "#         return fig, ax\n",
    "\n",
    "help(plt.imshow)\n",
    "\n",
    "%debug\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "\n",
    "x.min()\n",
    "\n",
    "plt.imshow(x[1,...].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd1d2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c0686ae52b501c5138d1ad3c292b1aad199ba0a61e288abba1616901d57bf21"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
