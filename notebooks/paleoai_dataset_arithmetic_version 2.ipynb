{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2702c51d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PaleoAI Dataset Arithmetic\n",
    "\n",
    "`paleoai_dataset_arithmetic.ipynb`\n",
    "\n",
    "This notebook is for using pandas dataframes for elegantly adding or subtracting sets of data samples while maintaining unique-specimen constraints based on enforcing uniqueness for a user-specified id column. Using simple operator overloading in Python class definitions, we can perform complex queries with minimal boilerplate.\n",
    "\n",
    "Author: Jacob A Rose  \n",
    "Created on: Monday July 19th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116782c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module;function;builtin_function_or_method;ABCMeta;type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefba588",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Code that has been outsourced to scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99076534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.data.common import CommonDataset, PathSchema\n",
    "from lightning_hydra_classifiers.utils.dataset_management_utils import Extract\n",
    "from lightning_hydra_classifiers.utils.common_utils import LabelEncoder\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import torchdata\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "# a = torchdata.datasets.Files([\"class_0/image_0.jpg\"])\n",
    "\n",
    "CommonDataset.available_datasets\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "from omegaconf import DictConfig\n",
    "import collections\n",
    "\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class BaseDatasetConfig:\n",
    "\n",
    "    def save(self,\n",
    "             path: Union[str, Path]) -> None:\n",
    "#         keys = self.__dataclass_fields__.keys()\n",
    "        cfg = DictConfig({k: getattr(self,k) for k in self.keys()})\n",
    "        Extract.config2yaml(cfg, path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls,\n",
    "             path: Union[str, Path]) -> \"DatasetConfig\":\n",
    "\n",
    "        cfg = Extract.config_from_yaml(path)\n",
    "\n",
    "#         keys = cls.__dataclass_fields__.keys()\n",
    "        cfg = cls(**{k: cfg[k] for k in cls.keys()})\n",
    "        return cfg\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        return cls.__dataclass_fields__.keys()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = f\"{type(self)}\" + \"\\n\"\n",
    "        out += \"\\n\".join([f\"{k}: {getattr(self, k)}\" for k in self.keys()])\n",
    "#         out += f\"\\nroot_dir: {self.root_dir}\"\n",
    "#         out += \"\\nsubset_dirs: \\n\\t\" + '\\n\\t'.join(self.subset_dirs)\n",
    "        return out\n",
    "    \n",
    "@dataclass\n",
    "class DatasetConfig(BaseDatasetConfig):\n",
    "    base_dataset_name: str = \"Extant\"\n",
    "    class_type: str = \"family\"\n",
    "    threshold: int = 10\n",
    "    resolution: int = 512\n",
    "    path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "\n",
    "    @property\n",
    "    def full_name(self):\n",
    "        return f\"{self.base_dataset_name}_{self.class_type}_{self.threshold}_{self.resolution}\"\n",
    "    \n",
    "from functools import cached_property #lru_cache\n",
    "\n",
    "\n",
    "    \n",
    "class ImageFileDatasetConfig(DatasetConfig):\n",
    "    \n",
    "    @property\n",
    "    def root_dir(self):\n",
    "        return CommonDataset.available_datasets[self.full_name]\n",
    "    \n",
    "    def is_valid_subset(self, subset: str):\n",
    "        for s in (\"train\", \"val\", \"test\"):\n",
    "            if s in subset:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def subsets(self):\n",
    "        return [s for s in os.listdir(self.root_dir) if  self.is_valid_subset(s)]\n",
    "    \n",
    "    @property\n",
    "    def subset_dirs(self):\n",
    "        return [os.path.join(self.root_dir, subset) for subset in self.subsets]\n",
    "#         subsets = self.subsets\n",
    "#         return [os.path.join(self.root_dir, subset) for subset in subsets if self.is_valid_subset(subset)]\n",
    "\n",
    "    def locate_files(self) -> Dict[str, List[Path]]:\n",
    "        return Extract.locate_files(self.root_dir)\n",
    "\n",
    "\n",
    "    @cached_property\n",
    "    def num_samples(self):\n",
    "#         subset_dirs = {Path(subset_dir).stem: Path(subset_dir) for subset_dir in self.subset_dirs}\n",
    "        files = {subset: f for subset, f in self.locate_files().items() if self.is_valid_subset(subset)}\n",
    "        return {subset: len(list(f)) for subset, f in files.items()}\n",
    "#         return {subset: len(list(subset_dirs[subset].rglob(\"*/*.jpg\"))) for subset in subset_dirs.keys()}\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = super().__repr__()\n",
    "        out += f\"\\nroot_dir: {self.root_dir}\"\n",
    "        out += \"\\nsubsets: \"\n",
    "        for i, subset in enumerate(self.subsets):\n",
    "            out += '\\n\\t' + f\"{subset}:\"\n",
    "            out += '\\n\\t\\t' + f\"subdir: {self.subset_dirs[i]}\"\n",
    "            out += '\\n\\t\\t' + f\"subset_num_samples: {self.num_samples[subset]}\"\n",
    "#         out += '\\n\\t'.join(self.subset_dirs)\n",
    "#         out += \"\\nsubset_dirs: \\n\\t\" + '\\n\\t'.join(self.subset_dirs)\n",
    "        return out\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CSVDatasetConfig(BaseDatasetConfig):\n",
    "    \n",
    "    full_name: str = None\n",
    "    csv_path: str = None\n",
    "    subset_key: str = \"all\"\n",
    "\n",
    "    def locate_files(self) -> pd.DataFrame: #Dict[str, List[Path]]:\n",
    "        return Extract.df_from_csv(self.csv_path)\n",
    "\n",
    "    @cached_property\n",
    "    def num_samples(self):\n",
    "        return {self.subset_key: len(self.locate_files())}\n",
    "\n",
    "    def __repr__(self):\n",
    "        out = super().__repr__()\n",
    "        out += '\\n' + f\"num_samples: {self.num_samples['all']}\"\n",
    "#         out += '\\n\\t'.join(self.subset_dirs)\n",
    "#         out += \"\\nsubset_dirs: \\n\\t\" + '\\n\\t'.join(self.subset_dirs)\n",
    "        return out\n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SampleSchema:\n",
    "    path : Union[str, Path] = None\n",
    "    family : str = None\n",
    "    genus : str = None\n",
    "    species : str = None\n",
    "    collection : str = None\n",
    "    catalog_number : str = None\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        return list(cls.__dataclass_fields__.keys())\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        return getattr(self, self.keys()[index])\n",
    "        \n",
    "totensor: Callable = torchvision.transforms.ToTensor()\n",
    "toPIL: Callable = torchvision.transforms.ToPILImage(\"RGB\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# class CommonDataArithmetic(torchdata.datasets.Files): # (CommonDataset):\n",
    "class CustomDataset(torchdata.datasets.Files): # (CommonDataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 files: List[Path]=None,\n",
    "                 samples_df: pd.DataFrame=None,\n",
    "                 path_schema: Path = \"{family}_{genus}_{species}_{collection}_{catalog_number}\",\n",
    "                 return_signature: List[str] = [\"image\",\"target\",\"path\"],\n",
    "                 config: Optional[BaseDatasetConfig]=None):\n",
    "        files = files or []\n",
    "        super().__init__(files=files)\n",
    "#         self.samples_df = samples_df\n",
    "        self.path_schema = PathSchema(path_schema)\n",
    "        self.return_signature = collections.namedtuple(\"return_signature\", return_signature)\n",
    "        \n",
    "        self.x_col = \"path\"\n",
    "        self.y_col = \"family\"\n",
    "        self.id_col = \"catalog_number\"\n",
    "        self.config = config or {}\n",
    "        \n",
    "        self.setup(samples_df=samples_df)\n",
    "        \n",
    "        \n",
    "    def fetch_item(self, index: int) -> Tuple[str]:\n",
    "        sample = self.parse_sample(index)\n",
    "        image = Image.open(sample.path)\n",
    "        return self.return_signature(image=image,\n",
    "                                     target=getattr(sample, self.y_col),\n",
    "                                     path=getattr(sample, self.x_col))\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        item = self.parse_sample(index)\n",
    "        image, target, path = item.image, item.target, str(item.path)\n",
    "        target = self.label_encoder.class2idx[target]\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform is not None:\n",
    "#             target = self.target_transform(target)\n",
    "\n",
    "\n",
    "        image = totensor(image)\n",
    "            \n",
    "        return tuple(self.return_signature(image=image, target=target, path=path))\n",
    "    \n",
    "        \n",
    "    def setup(self,\n",
    "              samples_df: pd.DataFrame=None,\n",
    "              label_encoder: LabelEncoder=None,\n",
    "              fit_targets: bool=True):\n",
    "        if samples_df is not None:\n",
    "            self.samples_df = samples_df.convert_dtypes()\n",
    "        self.samples = [self.parse_sample(idx) for idx in range((len(self)))]\n",
    "        self.targets = [sample[1] for sample in self.samples]\n",
    "        self.samples_df = pd.DataFrame(self.samples).convert_dtypes()\n",
    "        \n",
    "        self.label_encoder = label_encoder or LabelEncoder()\n",
    "        if fit_targets:\n",
    "            self.label_encoder.fit(self.targets)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config: DatasetConfig, subset_keys: List[str]=None) -> \"CustomDataset\":\n",
    "        pass\n",
    "        \n",
    "    def parse_sample(self, index: int):\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self):\n",
    "        disp = f\"\"\"<{str(type(self)).strip(\"'>\").split('.')[1]}>:\"\"\"\n",
    "        disp += '\\n\\t' + self.config.__repr__().replace('\\n','\\n\\t')\n",
    "        return disp\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def get_files_from_samples(cls,\n",
    "                               samples: Union[pd.DataFrame, List],\n",
    "                               x_col: Optional[str]=\"path\"):\n",
    "        if isinstance(samples, pd.DataFrame):\n",
    "            if x_col in samples.columns:\n",
    "                files = list(samples[x_col].values)\n",
    "            else:\n",
    "                files = list(samples.iloc[:,0].values)\n",
    "        elif isinstance(samples, list):\n",
    "            files = [s[0] for s in self.samples]\n",
    "            \n",
    "        return files\n",
    "    \n",
    "    def intersection(self, other):\n",
    "        samples_df = self.samples_df\n",
    "        other_df = other.samples_df\n",
    "        \n",
    "        intersection = samples_df.merge(other_df, how='inner', on=self.id_col)\n",
    "        return intersection\n",
    "    \n",
    "    def __add__(self, other):\n",
    "    \n",
    "        intersection = self.intersection(other)[self.id_col].tolist()\n",
    "        samples_df = self.samples_df\n",
    "        \n",
    "        left_union = samples_df[samples_df[self.id_col].apply(lambda x: x in intersection)]\n",
    "        \n",
    "        return left_union\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "    \n",
    "        intersection = self.intersection(other)[self.id_col].tolist()\n",
    "        samples_df = self.samples_df\n",
    "        \n",
    "        remainder = samples_df[samples_df[self.id_col].apply(lambda x: x not in intersection)]\n",
    "        \n",
    "        return remainder\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class ImageFileDataset(CustomDataset):\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config: DatasetConfig, subset_keys: List[str]=None) -> \"CustomDataset\":\n",
    "        files = config.locate_files()\n",
    "        if isinstance(subset_keys, list):\n",
    "            files = {k: files[k] for k in subset_keys}\n",
    "        if len(files.keys())==1: \n",
    "            files = files[subset_keys[0]]\n",
    "        new = cls(files=files,\n",
    "                  path_schema=config.path_schema)\n",
    "        new.config = config\n",
    "        return new\n",
    "    \n",
    "    def parse_sample(self, index: int):\n",
    "        path = self.files[index]\n",
    "        family, genus, species, collection, catalog_number = self.path_schema.parse(path)\n",
    "\n",
    "        return SampleSchema(path=path,\n",
    "                             family=family,\n",
    "                             genus=genus,\n",
    "                             species=species,\n",
    "                             collection=collection,\n",
    "                             catalog_number=catalog_number)\n",
    "\n",
    "\n",
    "class CSVDataset(CustomDataset):\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config: DatasetConfig, subset_keys: List[str]=None) -> Union[Dict[str, \"CSVDataset\"], \"CSVDataset\"]:\n",
    "        files_df = config.locate_files()\n",
    "        if subset_keys is None:\n",
    "            subset_keys = ['all']\n",
    "        if isinstance(subset_keys, list) and isinstance(files_df, dict):\n",
    "            files_df = {k: files_df[k] for k in subset_keys}\n",
    "            new = {k: cls(samples_df=files_df[k]) for k in subset_keys}\n",
    "            for k in subset_keys:\n",
    "                new.config = deepcopy(config)\n",
    "                new.config.subset_key = k\n",
    "#         if len(files_df.keys())==1: \n",
    "        if len(subset_keys)==1:\n",
    "            if isinstance(files_df, dict):\n",
    "                files_df = files_df[subset_keys[0]]\n",
    "            new = cls(samples_df=files_df)\n",
    "            new.config = config\n",
    "            new.config.subset_key = subset_keys[0]\n",
    "        return new\n",
    "    \n",
    "    def setup(self,\n",
    "              samples_df: pd.DataFrame=None,\n",
    "              label_encoder: LabelEncoder=None,\n",
    "              fit_targets: bool=True):\n",
    "        \n",
    "        if samples_df is not None:\n",
    "            self.samples_df = samples_df.convert_dtypes()\n",
    "        \n",
    "\n",
    "        self.files = self.samples_df[self.x_col].apply(lambda x: Path(x)).tolist()\n",
    "        self.samples = [self.parse_sample(idx) for idx in range((len(self)))]\n",
    "        self.targets = [sample[1] for sample in self.samples]       \n",
    "        self.samples_df = pd.DataFrame(self.samples).convert_dtypes()\n",
    "        \n",
    "        self.label_encoder = label_encoder or LabelEncoder()\n",
    "        if fit_targets:\n",
    "            self.label_encoder.fit(self.targets)\n",
    "        \n",
    "    \n",
    "    def parse_sample(self, index: int):\n",
    "        \n",
    "        row = self.samples_df.iloc[index,:].tolist()\n",
    "        path, family, genus, species, collection, catalog_number = row\n",
    "        return SampleSchema(path=path,\n",
    "                             family=family,\n",
    "                             genus=genus,\n",
    "                             species=species,\n",
    "                             collection=collection,\n",
    "                             catalog_number=catalog_number)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def export_dataset_catalog_configuration(output_dir: str = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3\",\n",
    "                                         base_dataset_name = \"Extant\",\n",
    "                                         threshold = 100,\n",
    "                                         resolution = 512,\n",
    "                                         path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"):\n",
    "    \n",
    "    image_file_config = ImageFileDatasetConfig(base_dataset_name = base_dataset_name,\n",
    "                                               class_type = \"family\",\n",
    "                                               threshold = threshold,\n",
    "                                               resolution = resolution,\n",
    "                                               path_schema = path_schema)\n",
    "\n",
    "    out_dir = os.path.join(output_dir, image_file_config.full_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    csv_out_path = os.path.join(out_dir, f\"{image_file_config.full_name}-full_dataset.csv\")\n",
    "    image_file_config_out_path = os.path.join(out_dir,\"ImageFileDataset-config.yaml\")\n",
    "    csv_config_out_path = os.path.join(out_dir,\"CSVDataset-config.yaml\")\n",
    "\n",
    "    dataset = ImageFileDataset.from_config(image_file_config, subset_keys=['all'])\n",
    "    Extract.df2csv(dataset.samples_df,\n",
    "                   path = csv_out_path)\n",
    "    image_file_config.save(image_file_config_out_path)\n",
    "\n",
    "    csv_config = CSVDatasetConfig(full_name = image_file_config.full_name,\n",
    "                                  csv_path = csv_out_path,\n",
    "                                  subset_key = \"all\")\n",
    "\n",
    "    csv_config.save(csv_config_out_path)\n",
    "\n",
    "    print(f\"[FINISHED] DATASET FULL NAME: {csv_config.full_name}\")\n",
    "    print(f\"Newly created dataset assets located at:  {out_dir}\")\n",
    "    \n",
    "    return dataset, image_file_config, csv_config\n",
    "\n",
    "################################################\n",
    "\n",
    "def export_composite_dataset_catalog_configuration(output_dir: str = \".\",\n",
    "                                                   csv_cfg_path_A: str=None,\n",
    "                                                   csv_cfg_path_B: str=None,\n",
    "                                                   composition: str=\"-\") -> Tuple[CSVDataset, CSVDatasetConfig]:\n",
    "    \n",
    "    \n",
    "    csv_config_A = CSVDatasetConfig.load(path = csv_cfg_path_A)\n",
    "    csv_config_B = CSVDatasetConfig.load(path = csv_cfg_path_B)\n",
    "\n",
    "    dataset_A = CSVDataset.from_config(csv_config_A)\n",
    "    dataset_B = CSVDataset.from_config(csv_config_B)\n",
    "\n",
    "    if composition == '-':\n",
    "        dataset_A_minus_B = dataset_B - dataset_A\n",
    "        full_name = f\"{csv_config_A.full_name}_minus_{csv_config_B.full_name}\"\n",
    "        \n",
    "    out_dir = os.path.join(output_dir, full_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    inputs_dir = os.path.join(out_dir, \"inputs\")\n",
    "    os.makedirs(inputs_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(inputs_dir, \"A\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(inputs_dir, \"B\"), exist_ok=True)\n",
    "    shutil.copyfile(csv_cfg_path_A, os.path.join(inputs_dir, \"A\", Path(csv_cfg_path_A).name))\n",
    "    shutil.copyfile(csv_cfg_path_B, os.path.join(inputs_dir, \"B\", Path(csv_cfg_path_B).name))\n",
    "\n",
    "    csv_dataset_pathname = f\"{full_name}-full_dataset\"\n",
    "    csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + \".csv\")\n",
    "    csv_dataset_config_out_path = os.path.join(out_dir,f\"{csv_dataset_pathname}_CSVDataset-config.yaml\")    \n",
    "\n",
    "    Extract.df2csv(dataset_A_minus_B,\n",
    "                   path = csv_dataset_out_path)\n",
    "\n",
    "    A_minus_B_config = CSVDatasetConfig(full_name = full_name,\n",
    "                                        csv_path = csv_dataset_out_path,\n",
    "                                        subset_key = \"all\")\n",
    "    A_minus_B_config.save(csv_dataset_config_out_path)\n",
    "\n",
    "    print(f\"[FINISHED] DATASET: {full_name}\")\n",
    "    print(f\"Newly created dataset assets located at:  {out_dir}\")\n",
    "    \n",
    "    dataset_A_minus_B = CSVDataset.from_config(csv_config_B)\n",
    "    return dataset_A_minus_B, A_minus_B_config\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3\"\n",
    "\n",
    "rm -r \"Extant_family_100_512_minus_PNAS_family_100_512\"\n",
    "rm -r \"Extant_family_10_1024_minus_PNAS_family_100_1024\"\n",
    "rm -r \"Extant_family_10_512_minus_PNAS_family_100_512\"\n",
    "rm -r \"Extant_family_100_1024_minus_PNAS_family_100_1024\"\n",
    "\n",
    "cd \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3\"\n",
    "\n",
    "\n",
    "rm -r \"Extant_family_100_512_w_PNAS_family_100_512\"\n",
    "rm -r \"Extant_family_10_1024_w_PNAS_family_100_1024\"\n",
    "rm -r \"Extant_family_10_512_w_PNAS_family_100_512\"\n",
    "rm -r \"Extant_family_100_1024_w_PNAS_family_100_1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# cd \"/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils\"\n",
    "# python \"make_catalogs.py --extant-pnas\"\n",
    "\n",
    "# output_dir = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3\"\n",
    "\n",
    "# base_names = {\"A\": \"Extant\",\n",
    "#               \"B\": \"PNAS\"}\n",
    "# thresholds = [{\"A\":100,\n",
    "#                \"B\":100},\n",
    "#              {\"A\":10,\n",
    "#                \"B\":100}]\n",
    "# resolutions = [512, 1024]\n",
    "# class_type = \"family\"\n",
    "\n",
    "\n",
    "# for threshold in thresholds:\n",
    "#     for resolution in resolutions:\n",
    "#         dataset_full_names = {\"A\":\"_\".join([base_names[\"A\"], class_type, str(threshold[\"A\"]), str(resolution)]),\n",
    "#                               \"B\":\"_\".join([base_names[\"B\"], class_type, str(threshold[\"B\"]), str(resolution)])}\n",
    "\n",
    "#         csv_cfg_path_A = os.path.join(output_dir, dataset_full_names[\"A\"], \"CSVDataset-config.yaml\")\n",
    "#         csv_cfg_path_B = os.path.join(output_dir, dataset_full_names[\"B\"], \"CSVDataset-config.yaml\")\n",
    "        \n",
    "#         break\n",
    "        \n",
    "        \n",
    "# print(\"A:\", csv_cfg_path_A,\n",
    "#       \"B:\", csv_cfg_path_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a2677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.data.utils import make_catalogs\n",
    "dir(make_catalogs)\n",
    "\n",
    "from lightning_hydra_classifiers.data.utils.make_catalogs import *\n",
    "\n",
    "# A_minus_B_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/Extant_family_100_512_minus_PNAS_family_100_512\")\n",
    "A_minus_B_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/Extant_family_10_512_minus_PNAS_family_100_512\")\n",
    "\n",
    "config_path = list(A_minus_B_dir.glob(\"./*CSVDataset-config.yaml\"))[0]\n",
    "dataset_path = list(A_minus_B_dir.glob(\"./*full_dataset.csv\"))[0]\n",
    "print(\"config path: \\n\\t\", config_path)\n",
    "print(\"dataset path: \\n\\t\", dataset_path)\n",
    "config = CSVDatasetConfig.load(path = config_path)\n",
    "dataset = CSVDataset.from_config(config)\n",
    "\n",
    "print(config)\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13383227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# composition = '-'\n",
    "\n",
    "# csv_config_A = CSVDatasetConfig.load(path = csv_cfg_path_A)\n",
    "# csv_config_B = CSVDatasetConfig.load(path = csv_cfg_path_B)\n",
    "\n",
    "# dataset_A = CSVDataset.from_config(csv_config_A)\n",
    "# dataset_B = CSVDataset.from_config(csv_config_B)\n",
    "\n",
    "# if composition == '-':\n",
    "#     dataset_A_minus_B = dataset_B - dataset_A\n",
    "#     full_name = f\"{csv_config_A.full_name}_minus_{csv_config_B.full_name}\"\n",
    "\n",
    "# print(f\"num_samples A: {len(dataset_A)}\")\n",
    "# print(f\"num_samples B: {len(dataset_B)}\")\n",
    "# print(f\"num_samples A-B: {len(dataset_A_minus_B)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = list(A_minus_B_dir.glob(\"./inputs/A/*CSVDataset-config.yaml\"))[0]\n",
    "# dataset_path = list(A_minus_B_dir.glob(\"./inputs/A/*full_dataset.csv\"))[0]\n",
    "print(\"config path: \\n\\t\", config_path)\n",
    "print(\"dataset path: \\n\\t\", dataset_path)\n",
    "\n",
    "config_A = CSVDatasetConfig.load(path = config_path)\n",
    "dataset_A = CSVDataset.from_config(config_A)\n",
    "\n",
    "print(dataset_A.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = list(A_minus_B_dir.glob(\"./inputs/B/*CSVDataset-config.yaml\"))[0]\n",
    "# dataset_path = list(A_minus_B_dir.glob(\"./inputs/A/*full_dataset.csv\"))[0]\n",
    "print(\"config path: \\n\\t\", config_path)\n",
    "print(\"dataset path: \\n\\t\", dataset_path)\n",
    "\n",
    "config_B = CSVDatasetConfig.load(path = config_path)\n",
    "dataset_B = CSVDataset.from_config(config_B)\n",
    "\n",
    "print(dataset_B.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a78629",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "output_dir = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3\"\n",
    "\n",
    "\n",
    "base_dataset_name = \"Extant\"\n",
    "thresholds = [10,100]\n",
    "resolutions = [512,1024]\n",
    "path_schema = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "\n",
    "for threshold in thresholds:\n",
    "    for resolution in resolutions:\n",
    "        export_dataset_catalog_configuration(output_dir=output_dir,\n",
    "                                             base_dataset_name = base_dataset_name,\n",
    "                                             threshold = threshold,\n",
    "                                             resolution = resolution,\n",
    "                                             path_schema = path_schema)\n",
    "\n",
    "print(f'FINISHED ALL IN Extant')\n",
    "print('=='*15)\n",
    "\n",
    "base_dataset_name = \"PNAS\"\n",
    "thresholds = [100]\n",
    "resolutions = [512,1024]\n",
    "path_schema = \"{family}_{genus}_{species}_{catalog_number}\"\n",
    "for threshold in thresholds:\n",
    "    for resolution in resolutions:\n",
    "        export_dataset_catalog_configuration(output_dir=output_dir,\n",
    "                                             base_dataset_name = base_dataset_name,\n",
    "                                             threshold = threshold,\n",
    "                                             resolution = resolution,\n",
    "                                             path_schema = path_schema)\n",
    "\n",
    "print(f'FINISHED ALL IN PNAS')\n",
    "print('=='*15)\n",
    "\n",
    "\n",
    "output_dir = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3\"\n",
    "\n",
    "base_names = {\"A\": \"Extant\",\n",
    "              \"B\": \"PNAS\"}\n",
    "thresholds = [{\"A\":100,\n",
    "               \"B\":100},\n",
    "             {\"A\":10,\n",
    "               \"B\":100}]\n",
    "resolutions = [512, 1024]\n",
    "\n",
    "class_type = \"family\"\n",
    "\n",
    "\n",
    "for threshold in thresholds:\n",
    "    for resolution in resolutions:\n",
    "        dataset_full_names = {\"A\":\"_\".join([base_names[\"A\"], class_type, str(threshold[\"A\"]), str(resolution)]),\n",
    "                              \"B\":\"_\".join([base_names[\"B\"], class_type, str(threshold[\"B\"]), str(resolution)])}\n",
    "\n",
    "        csv_cfg_path_A = os.path.join(output_dir, dataset_full_names[\"A\"], \"CSVDataset-config.yaml\")\n",
    "        csv_cfg_path_B = os.path.join(output_dir, dataset_full_names[\"B\"], \"CSVDataset-config.yaml\")\n",
    "        dataset, cfg = export_composite_dataset_catalog_configuration(output_dir=output_dir,\n",
    "                                                                      csv_cfg_path_A=csv_cfg_path_A,\n",
    "                                                                      csv_cfg_path_B=csv_cfg_path_B,\n",
    "                                                                      composition=\"-\")\n",
    "        \n",
    "        \n",
    "print(f'FINISHED ALL IN Extant-PNAS')\n",
    "print('=='*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda61c8f",
   "metadata": {},
   "source": [
    "# Back to R & D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6754cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e984cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from lightning_hydra_classifiers.data.utils.make_catalogs import *\n",
    "\n",
    "\n",
    "output_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/\")\n",
    "A_minus_B_dir = Path(output_dir, \"Extant_family_10_512_minus_PNAS_family_100_512\")\n",
    "\n",
    "config_path = list(A_minus_B_dir.glob(\"./CSVDataset-config.yaml\"))[0]\n",
    "dataset_path = list(A_minus_B_dir.glob(\"./*full_dataset.csv\"))[0]\n",
    "print(\"config path: \\n\\t\", config_path)\n",
    "print(\"dataset path: \\n\\t\", dataset_path)\n",
    "config = CSVDatasetConfig.load(path = config_path)\n",
    "dataset = CSVDataset.from_config(config)\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ba123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "from MapExtrackt import FeatureExtractor\n",
    "fe = FeatureExtractor(model)\n",
    "\n",
    "# import torch\n",
    "\n",
    "# def toPIL(img: torch.Tensor, mode=\"RGB\") -> Callable:\n",
    "#     return torchvision.transforms.ToPILImage(mode)(img)\n",
    "\n",
    "# img = toPIL(dataset[0][0])\n",
    "\n",
    "# fe.set_image(img)\n",
    "\n",
    "# gray_img = toPIL(dataset[0][0], \"HSV\")\n",
    "# fe.set_image(gray_img)\n",
    "\n",
    "# fe.display_from_map(layer_no=1)\n",
    "\n",
    "# fe[38,:]\n",
    "\n",
    "# batch_size = 32\n",
    "# num_workers = 0\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "# batch = next(iter(dataloader))\n",
    "# len(dataloader)\n",
    "# for idx, batch in enumerate(iter(dataloader)):\n",
    "\n",
    "# batch_size = 32\n",
    "# num_workers = 0\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, torch, time\n",
    "import numpy as np\n",
    "from rising.loading import DataLoader\n",
    "\n",
    "\n",
    "pin_memory = True\n",
    "batch_size = 64 # 1024 # bigger memory transfers to make their cost more noticable\n",
    "num_workers = 6 # parallel workers to free up the main thread and reduce data decoding overhead\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     pin_memory=pin_memory,\n",
    "#     num_workers=n_workers\n",
    "# )\n",
    "# batch_size = 32\n",
    "# num_workers = 0\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "print('pin_memory:', pin_memory)\n",
    "times = []\n",
    "num_runs = 2\n",
    "\n",
    "def work():\n",
    "    # emulates the CPU work done\n",
    "    time.sleep(0.05)\n",
    "\n",
    "\n",
    "for i in trange(num_runs, position=0):\n",
    "    st = time.time()\n",
    "    for i, batch in tqdm(enumerate(dataloader), leave=False, position=1, total=len(dataloader)):\n",
    "        x, y = batch[:2]\n",
    "        x, y = x.cuda(non_blocking=pin_memory), y.cuda(non_blocking=pin_memory)\n",
    "        work()\n",
    "    times.append(time.time() - st)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{num_runs} Epoch(s):\", \"\\n\"+\"==\"*15)\n",
    "print('Time:')\n",
    "print(f\"\\tAvg: {np.mean(times):.2f} seconds\")\n",
    "print(f\"\\tStd: {np.std(times):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5594663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2b69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253640e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf358393",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Developing train/val/test workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4895022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.data.utils import make_catalogs\n",
    "\n",
    "from lightning_hydra_classifiers.utils.common_utils import (LabelEncoder,\n",
    "                                                            trainval_split,\n",
    "                                                            trainvaltest_split,\n",
    "                                                            plot_split_distributions,\n",
    "                                                            plot_class_distributions)\n",
    "import torchdata\n",
    "from lightning_hydra_classifiers.data.utils.make_catalogs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/\")\n",
    "A_minus_B_dir = Path(output_dir, \"Extant_family_10_512_minus_PNAS_family_100_512\")\n",
    "\n",
    "config_path = list(A_minus_B_dir.glob(\"./CSVDataset-config.yaml\"))[0]\n",
    "dataset_path = list(A_minus_B_dir.glob(\"./*full_dataset.csv\"))[0]\n",
    "print(\"config path: \\n\\t\", config_path)\n",
    "print(\"dataset path: \\n\\t\", dataset_path)\n",
    "config = CSVDatasetConfig.load(path = config_path)\n",
    "dataset = CSVDataset.from_config(config)\n",
    "\n",
    "##########################################\n",
    "print(config)\n",
    "print(dataset)\n",
    "\n",
    "extant_minus_pnas_dataset = dataset\n",
    "data_splits = DataSplitter.create_trainvaltest_splits(data=extant_minus_pnas_dataset,\n",
    "                                                      val_split=0.2,\n",
    "                                                      test_split=\"test\",\n",
    "                                                      shuffle=True,\n",
    "                                                      seed=3654,\n",
    "                                                      stratify=True)\n",
    "\n",
    "\n",
    "\n",
    "config_A_path = list(A_minus_B_dir.glob(\"./inputs/A/*CSVDataset-config.yaml\"))[0]\n",
    "config_B_path = list(A_minus_B_dir.glob(\"./inputs/B/*CSVDataset-config.yaml\"))[0]\n",
    "config_A = CSVDatasetConfig.load(path = config_A_path)\n",
    "config_B = CSVDatasetConfig.load(path = config_B_path)\n",
    "\n",
    "# A_minus_B_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/Extant_family_100_512_minus_PNAS_family_100_512\")\n",
    "# output_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/\")\n",
    "# A_minus_B_dir = Path(output_dir, \"Extant_family_10_512_minus_PNAS_family_100_512\")\n",
    "\n",
    "config_path = list(A_minus_B_dir.glob(\"./CSVDataset-config.yaml\"))[0]\n",
    "dataset_path = list(A_minus_B_dir.glob(\"./*full_dataset.csv\"))[0]\n",
    "print(\"config path: \\n\\t\", config_path)\n",
    "print(\"dataset path: \\n\\t\", dataset_path)\n",
    "A_composed_B_config = CSVDatasetConfig.load(path = config_path)\n",
    "dataset_A_composed_B = CSVDataset.from_config(config)\n",
    "\n",
    "\n",
    "\n",
    "full_name = f\"{config_A.full_name}_w_{config_B.full_name}\"\n",
    "A_in_B_dir = Path(output_dir, full_name)\n",
    "# print(f\"num_samples A_w_B: {len(dataset_A_composed_B)}\")\n",
    "config_path = list(A_in_B_dir.glob(\"./*A_in_B-CSVDataset-config.yaml\"))[0]\n",
    "A_in_B_config = CSVDatasetConfig.load(path = config_path)\n",
    "dataset_A_in_B = CSVDataset.from_config(A_in_B_config)\n",
    "\n",
    "config_path = list(A_in_B_dir.glob(\"./*B_in_A-CSVDataset-config.yaml\"))[0]\n",
    "B_in_A_config = CSVDatasetConfig.load(path = config_path)\n",
    "dataset_B_in_A = CSVDataset.from_config(B_in_A_config)\n",
    "\n",
    "\n",
    "##################################\n",
    "\n",
    "data_splits[\"test\"] = dataset_A_in_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 37\n",
    "for k,v in data_splits.items():\n",
    "    print(k)\n",
    "    display(toPIL(v[idx][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ec783",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 48\n",
    "\n",
    "\n",
    "display(toPIL(dataset_A_in_B[idx][0]))\n",
    "\n",
    "display(toPIL(dataset_B_in_A[idx][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc717c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27435cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dataset_pathname = f\"{config_A.full_name}_in_{config_B.full_name}\"\n",
    "csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + \".csv\")\n",
    "dataset_config_path = os.path.join(A_in_B_dir,f\"CSVDataset-config.yaml\")\n",
    "\n",
    "\n",
    "columns = [\"catalog_number\", *[col for col in dataset_A_composed_B.columns if col.endswith(\"_x\")]]\n",
    "extant_in_pnas = dataset_A_composed_B.reset_index()[columns].sort_values(\"catalog_number\")\n",
    "print(f\"extant_in_pnas.columns: {extant_in_pnas.columns}\")\n",
    "\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {col: col.split(\"_x\")[0] for col in extant_in_pnas.columns})\n",
    "Extract.df2csv(extant_in_pnas,\n",
    "               path = csv_dataset_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088cbc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = list(A_minus_B_dir.glob(\"./inputs/B/*CSVDataset-config.yaml\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pnas_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/PNAS_family_100_512\")\n",
    "\n",
    "# config_path = list(pnas_dir.glob(\"./*CSVDataset-config.yaml\"))[0]\n",
    "# dataset_path = list(pnas_dir.glob(\"./*full_dataset.csv\"))[0]\n",
    "# print(\"config path: \\n\\t\", config_path)\n",
    "# print(\"dataset path: \\n\\t\", dataset_path)\n",
    "pnas_config = CSVDatasetConfig.load(path = config_path)\n",
    "pnas_dataset = CSVDataset.from_config(pnas_config)\n",
    "\n",
    "##########################################\n",
    "print(pnas_config)\n",
    "print(pnas_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b534caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_minus_B_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/Extant_family_100_512_minus_PNAS_family_100_512\")\n",
    "pnas_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/PNAS_family_100_512\")\n",
    "\n",
    "config_path = list(pnas_dir.glob(\"./*CSVDataset-config.yaml\"))[0]\n",
    "dataset_path = list(pnas_dir.glob(\"./*full_dataset.csv\"))[0]\n",
    "print(\"config path: \\n\\t\", config_path)\n",
    "print(\"dataset path: \\n\\t\", dataset_path)\n",
    "pnas_config = CSVDatasetConfig.load(path = config_path)\n",
    "pnas_dataset = CSVDataset.from_config(pnas_config)\n",
    "\n",
    "##########################################\n",
    "print(pnas_config)\n",
    "print(pnas_dataset)\n",
    "\n",
    "\n",
    "\n",
    "# A_minus_B_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/Extant_family_100_512_minus_PNAS_family_100_512\")\n",
    "extant_dir = Path(\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3/Extant_family_10_512\")\n",
    "\n",
    "config_path = list(extant_dir.glob(\"./*CSVDataset-config.yaml\"))[0]\n",
    "dataset_path = list(extant_dir.glob(\"./*full_dataset.csv\"))[0]\n",
    "print(\"config path: \\n\\t\", config_path)\n",
    "print(\"dataset path: \\n\\t\", dataset_path)\n",
    "extant_config = CSVDatasetConfig.load(path = config_path)\n",
    "extant_dataset = CSVDataset.from_config(extant_config)\n",
    "\n",
    "##########################################\n",
    "print(extant_config)\n",
    "print(extant_dataset)\n",
    "\n",
    "\n",
    "# extant_w_pnas = extant_dataset.intersection(pnas_dataset, suffixes=(\"_extant\", \"_pnas\"))\n",
    "# extant_w_pnas = extant_w_pnas.sort_values(extant_dataset.id_col)\n",
    "# extant_w_pnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8baf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_extant_w_pnas_v0_3():\n",
    "    output_dir = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3\"\n",
    "\n",
    "    base_names = {\"A\": \"Extant\",\n",
    "                  \"B\": \"PNAS\"}\n",
    "    thresholds = [{\"A\":100,\n",
    "                   \"B\":100},\n",
    "                 {\"A\":10,\n",
    "                   \"B\":100}]\n",
    "    resolutions = [512, 1024]\n",
    "    class_type = \"family\"\n",
    "\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        for resolution in resolutions:\n",
    "            dataset_full_names = {\"A\":\"_\".join([base_names[\"A\"], class_type, str(threshold[\"A\"]), str(resolution)]),\n",
    "                                  \"B\":\"_\".join([base_names[\"B\"], class_type, str(threshold[\"B\"]), str(resolution)])}\n",
    "\n",
    "            csv_cfg_path_A = os.path.join(output_dir, dataset_full_names[\"A\"], \"CSVDataset-config.yaml\")\n",
    "            csv_cfg_path_B = os.path.join(output_dir, dataset_full_names[\"B\"], \"CSVDataset-config.yaml\")\n",
    "            dataset, cfg = export_composite_dataset_catalog_configuration(output_dir=output_dir,\n",
    "                                                                          csv_cfg_path_A=csv_cfg_path_A,\n",
    "                                                                          csv_cfg_path_B=csv_cfg_path_B,\n",
    "                                                                          composition=\"intersection\")\n",
    "\n",
    "\n",
    "    print(f'FINISHED ALL IN Extant_w_PNAS')\n",
    "    print('=='*15)\n",
    "    \n",
    "    \n",
    "    \n",
    "make_extant_w_pnas_v0_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51465b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extant_df = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Extant-dataset_leavesdb-v0_3.csv\", index_col=0)\n",
    "pnas_train = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100/train.csv\")\n",
    "pnas_test = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100/test.csv\")\n",
    "pnas_df = pd.concat([pnas_train, pnas_test])\n",
    "\n",
    "extant_in_pnas = intersection(data_df=extant_df,\n",
    "                              other_df=pnas_df,\n",
    "                              id_col=\"catalog_number\",\n",
    "                              suffixes=(\"_extant\", \"_pnas\"))\n",
    "\n",
    "# In order to only keep original columns\n",
    "suffixes=(\"_extant\", \"_pnas\")\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code Editor out of Sync\n",
    "Please open your browser JavaScript console for bug report instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d24398c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b389ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=dataset\n",
    "# val_split=0.2\n",
    "# test_split=\"test\"\n",
    "# shuffle=True\n",
    "# seed=3654\n",
    "# stratify=True\n",
    "\n",
    "# if (test_split == \"test\") or (test_split is None):\n",
    "#     train_split = 1 - val_split\n",
    "#     if hasattr(data, f\"test_dataset\"):\n",
    "#         data = getattr(data, f\"train_dataset\")            \n",
    "# elif isinstance(test_split, float):\n",
    "#     train_split = 1 - (test_split + val_split)\n",
    "# else:\n",
    "#     raise ValueError(f\"Invalid split arguments: val_train_split={val_train_split}, test_split={test_split}\")\n",
    "\n",
    "\n",
    "# splits=(train_split, val_split, test_split)\n",
    "# splits = list(filter(lambda x: isinstance(x, float), splits))\n",
    "# y = data.targets\n",
    "\n",
    "# if len(splits)==2:\n",
    "#     data_splits = trainval_split(x=None,\n",
    "#                                  y=y,\n",
    "#                                  val_train_split=splits[-1],\n",
    "#                                  random_state=seed,\n",
    "#                                  stratify=stratify)\n",
    "\n",
    "# else:\n",
    "#     data_splits = trainvaltest_split(x=None,\n",
    "#                                      y=y,\n",
    "#                                      splits=splits,\n",
    "#                                      random_state=seed,\n",
    "#                                      stratify=stratify)\n",
    "\n",
    "# for k,v in data_splits.items():\n",
    "#     print(k, len(v[0]))\n",
    "\n",
    "# dataset_splits={}\n",
    "# for split, (split_idx, split_y) in data_splits.items():\n",
    "#     print(split, len(split_idx))\n",
    "#     dataset_splits[split] = data.filter(indices=split_idx, subset_key=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f73599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_splits[\"train\"]\n",
    "dataset_splits[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a6601",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = data_splits['train'][0], data_splits['val'][0]\n",
    "\n",
    "train_df = extant_minus_pnas.iloc[train_idx,:]\n",
    "val_df = extant_minus_pnas.iloc[val_idx,:]\n",
    "\n",
    "train_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\n",
    "                                                               sample_df=train_df,\n",
    "                                                               config=None,\n",
    "                                                               return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                               subset_key=\"train\")\n",
    "#                                                                subset_key=None) #\"train\")\n",
    "\n",
    "\n",
    "val_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\n",
    "                                                             sample_df=val_df,\n",
    "                                                             config=None,\n",
    "                                                             return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                             subset_key=\"val\")\n",
    "\n",
    "\n",
    "test_dataset_extant_in_pnas = CommonDataset.from_dataframe(sample_df=extant_in_pnas,\n",
    "                                                            config=None,\n",
    "                                                            return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                            subset_key=\"test\")\n",
    "\n",
    "data_splits= {'train':train_dataset_extant_minus_pnas,\n",
    "              'val': val_dataset_extant_minus_pnas,\n",
    "              'test':test_dataset_extant_in_pnas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c826f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fedfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "extant_cfg = ImageFileDatasetConfig(base_dataset_name = \"Extant\",\n",
    "                                    class_type = \"family\",\n",
    "                                    threshold = 10,\n",
    "                                    resolution = \"512\",\n",
    "                                    path_schema = \"{family}_{genus}_{species}_{collection}_{catalog_number}\")\n",
    "\n",
    "\n",
    "pnas_cfg = ImageFileDatasetConfig(base_dataset_name = \"PNAS\",\n",
    "                                    class_type = \"family\",\n",
    "                                    threshold = 100,\n",
    "                                    resolution = \"512\",\n",
    "                                    path_schema = \"{family}_{genus}_{species}_{catalog_number}\")\n",
    "\n",
    "pnas_dataset = ImageFileDataset.from_config(pnas_cfg, subset_keys=['all'])\n",
    "extant_dataset = ImageFileDataset.from_config(extant_cfg, subset_keys=['all'])\n",
    "\n",
    "extant_minus_pnas = extant_dataset - pnas_dataset\n",
    "print(len(pnas_dataset),len(extant_dataset))\n",
    "print(len(extant_minus_pnas))\n",
    "\n",
    "csv_path = f\"{extant_cfg.full_name}_minus_{pnas_cfg.full_name}.csv\"\n",
    "Extract.df2csv(extant_minus_pnas,\n",
    "               path = csv_path)\n",
    "\n",
    "\n",
    "extant_minus_pnas_cfg = CSVDatasetConfig(full_name = Path(csv_path).stem,\n",
    "                                         csv_path = csv_path,\n",
    "                                         subset_key = \"all\")\n",
    "extant_minus_pnas_cfg\n",
    "\n",
    "%%time\n",
    "\n",
    "extant_minus_pnas_dataset = CSVDataset.from_config(extant_minus_pnas_cfg) #, subset_keys=['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2faf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(extant_minus_pnas_dataset)\n",
    "\n",
    "extant_minus_pnas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e59435",
   "metadata": {},
   "outputs": [],
   "source": [
    "extant_minus_pnas_dataset.config.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bb45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extant_minus_pnas_cfg.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39581bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "extant_w_pnas = extant_dataset.intersection(pnas_dataset)\n",
    "extant_w_pnas\n",
    "# print(len(extant_minus_pnas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = intersection[id_col].tolist() #values\n",
    "\n",
    "remainder = samples_df[samples_df[id_col].apply(lambda x: x not in intersection)]\n",
    "remainder\n",
    "\n",
    "    def intersection(self, other):\n",
    "        samples_df = self.samples_df\n",
    "        other_df = other.samples_df\n",
    "        \n",
    "        intersection = samples_df.merge(other_df, how='inner', on=self.id_col)\n",
    "        return intersection\n",
    "    \n",
    "    def __add__(self, other):\n",
    "    \n",
    "        intersection = self.intersection(other)\n",
    "        samples_df = self.samples_df\n",
    "        \n",
    "        left_union = samples_df[samples_df[self.id_col].apply(lambda x: x in intersection[self.id_col])]\n",
    "        \n",
    "        return left_union\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "    \n",
    "        intersection = self.intersection(other)\n",
    "        samples_df = self.samples_df\n",
    "        \n",
    "        remainder = samples_df[samples_df[self.id_col].apply(lambda x: x not in intersection[self.id_col])]\n",
    "        \n",
    "        return remainder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed31af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293f2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnas_cfg.save(\"pnas_config.yaml\")\n",
    "\n",
    "cfg = ImageFileDatasetConfig.load(\"pnas_config.yaml\")\n",
    "# cfg\n",
    "\n",
    "pnas_files = cfg.locate_files()\n",
    "pnas_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnas_dataset = CustomDataset(files=pnas_files['all'],\n",
    "                              path_schema=pnas_cfg.path_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b6f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pnas_dataset_subsets = torchdata.datasets.Files(pnas_files)\n",
    "pnas_dataset = torchdata.datasets.Files(pnas_files['all'])\n",
    "pnas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f49ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extant_cfg.save(\"extant_config.yaml\")\n",
    "\n",
    "cfg = ImageFileDatasetConfig.load(\"extant_config.yaml\")\n",
    "cfg\n",
    "\n",
    "extant_files = cfg.locate_files()\n",
    "extant_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7249f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Extant_family_10_512\"\n",
    "\n",
    "root_dir = CommonDataset.available_datasets[dataset_name]\n",
    "\n",
    "df = Extract.df_from_dir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in df.items():\n",
    "    print(k)\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(Extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15599dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pnas_datamodule.train_dataset\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df39a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "train_df = train_dataset.samples_df\r\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "a = train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665612de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def left_union(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\", suffixes=(\"_x\", \"_y\")) -> pd.DataFrame:\r\n",
    "#     \"\"\"\r\n",
    "#     Return a new dataframe containing all rows from `data_df`, concatenated with any rows that only exist in `other_df`. Any rows that are shared between the 2 default to only including the values from `data_df`.\r\n",
    "    \r\n",
    "#     \"\"\"\r\n",
    "#     return data_df.merge(other_df, how='outer', on=id_col, suffixes=suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "\r\n",
    "def intersection(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\", suffixes=(\"_x\", \"_y\")) -> pd.DataFrame:\r\n",
    "    \"\"\"\r\n",
    "    Return a new dataframe containing only rows that share the same values for `id_col` between `data_df` and `other_df`\r\n",
    "    \r\n",
    "    Equivalent to an AND join between sets\r\n",
    "    \"\"\"\r\n",
    "    return data_df.merge(other_df, how='inner', on=id_col, suffixes=suffixes)\r\n",
    "\r\n",
    "\r\n",
    "def left_exclusive(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\") -> pd.DataFrame:\r\n",
    "    \"\"\"\r\n",
    "    Return a new dataframe containing only rows from `data_df` that do not share an `id_col` value with any row from `other_df`.\r\n",
    "    \r\n",
    "    Equivalent to subtracting the set of `id_col` values in `other_df` from `data_df`\r\n",
    "    \"\"\"\r\n",
    "    omit = list(other_df[id_col].values)\r\n",
    "    \r\n",
    "    return data_df[data_df[id_col].apply(lambda x: x not in omit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extant_df = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Extant-dataset_leavesdb-v0_3.csv\", index_col=0)\r",
    "#  pnas_train = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100/train.csv\")# # pnas_test = pd.read_csv(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100/test.csv\"# \n",
    "# pnas_df = pd.concat([pnas_train, pnas_test]# \n",
    "\r\n",
    "# extant_in_pnas = intersection(data_df=extant# f,\r\n",
    "#                               other_df=pna# df,\r\n",
    "#                               id_col=\"catalog_nu# er\",\r\n",
    "#                               suffixes=(\"_extant\", \"_p# s\"))\r\n",
    "\r\n",
    "# # In order to only keep origina# columns\r\n",
    "# suffixes=(\"_extant\"# \"_pnas\")\r\n",
    "# extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suf# xes[1])])\r\n",
    "# extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extant_minus_pnas -> rows exclusive to extant dataset\r\n",
    "# extant_minus_pnas = left_exclusive(data_df=extant_df,\r\n",
    "#                                    other_df=pnas_df,\r\n",
    "#                                    id_col=\"catalog_number\",\r\n",
    "#                                    suffixes=(\"_extant\", \"_pnas\"))\r\n",
    "\r\n",
    "# # pnas_minus_extant -> rows exclusive to pnas dataset\r\n",
    "# pnas_minus_extant = left_exclusive(data_df=pnas_df,\r\n",
    "#                                    other_df=extant_df,\r\n",
    "#                                    id_col=\"catalog_number\",\r\n",
    "#                                    suffixes=(\"_pnas\", \"_extant\"))\r\n",
    "\r\n",
    "# extant_minus_pnas = left_exclusive(data_df=extant_df, other_df=pnas_df, id_col=\"catalog_number\", suffixes=(\"_extant\", \"_pnas\"))\r\n",
    "\r\n",
    "# pnas_minus_extant = left_exclusive(data_df=pnas_df, other_df=extant_df, id_col=\"catalog_number\", suffixes=(\"_pnas\", \"_extant\"))\r\n",
    "\r\n",
    "# extant_and_pnas = left_union(data_df=extant_df, other_df=pnas_df, id_col=\"catalog_number\", suffixes=(\"_extant\", \"_pnas\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea3ae0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path = \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/catalog_files/extant_family_10_train.csv\"\n",
    "\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a971b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\r\n",
    "import os.path\r\n",
    " \r\n",
    "def initialize_logger():\r\n",
    "    logger = logging.getLogger()\r\n",
    "    logger.setLevel(logging.DEBUG)\r\n",
    "     \r\n",
    "    # create console handler and set level to info\r\n",
    "    handler = logging.StreamHandler()\r\n",
    "    handler.setLevel(logging.INFO)\r\n",
    "    formatter = logging.Formatter(\"%(levelname)s - %(message)s\")\r\n",
    "    handler.setFormatter(formatter)\r\n",
    "    logger.addHandler(handler)\r\n",
    "\r\n",
    "initialize_logger()\r\n",
    "\r\n",
    "import torchdata\r\n",
    "from typing import Union, List, Any, Tuple\r\n",
    "# from collections import Counter\r\n",
    "from lightning_hydra_classifiers.utils import template_utils\r\n",
    "from lightning_hydra_classifiers.utils.common_utils import trainvaltest_split\r\n",
    "import collections\r\n",
    "from omegaconf import OmegaConf, DictConfig\r\n",
    "from lightning_hydra_classifiers.data.common import CommonDataSelect, CommonDataset, LeavesLightningDataModule\r\n",
    "from lightning_hydra_classifiers.data import fossil, extant, pnas\r\n",
    "from rich import print as pp\r\n",
    "import os\r\n",
    "\r\n",
    "from typing import *\r\n",
    "from pathlib import Path\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from IPython.display import display\r\n",
    "\r\n",
    "log = template_utils.get_logger(__name__, level=logging.DEBUG)\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "pd.set_option('display.max_rows', 500)\r\n",
    "pd.set_option('display.max_columns', 500)\r\n",
    "pd.set_option('display.max_colwidth', 200)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "config_dir = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\"\r\n",
    "\r\n",
    "from hydra.experimental import compose, initialize, initialize_config_dir\r\n",
    "from omegaconf import OmegaConf, DictConfig\r\n",
    "os.chdir(config_dir)\r\n",
    "print(f\"cwd = {os.getcwd()}\")\r\n",
    "\r\n",
    "def initialize_config(config_dir: str,\r\n",
    "                      overrides=None):\r\n",
    "    with initialize_config_dir(config_dir=config_dir, job_name=\"multi-gpu_experiment\"):\r\n",
    "\r\n",
    "        cfg = compose(config_name=\"multi-gpu\", overrides=overrides)\r\n",
    "        OmegaConf.set_struct(cfg, False)\r\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'family'\r\n",
    "seed = 5687\r\n",
    "val_train_split = 0.2\r\n",
    "\r\n",
    "# pnas_name = \"PNAS_family_100_512\"\r\n",
    "# extant_name = \"Extant_family_10_512\"\r\n",
    "\r\n",
    "pnas_name = \"PNAS_family_100_1024\"#512\"\r\n",
    "extant_name = \"Extant_family_10_1024\" #512\"\r\n",
    "\r\n",
    "\r\n",
    "## Load primary Extant and PNAS datamodules\r\n",
    "pnas_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=pnas_dataset\",\r\n",
    "                                  \"datamodule=standalone_datamodule\"])\r\n",
    "# pnas_cfg[f\"dataset.config.name\"] = pnas_name\r\n",
    "\r\n",
    "pnas_cfg.datamodule.config.dataset.name = pnas_name\r\n",
    "pp(OmegaConf.to_container(pnas_cfg, resolve=False))\r\n",
    "\r\n",
    "pnas_datamodule = LeavesLightningDataModule(pnas_cfg)#.datamodule.config)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "extant_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=extant_dataset\",\r\n",
    "                                   \"datamodule=standalone_datamodule\"])\r\n",
    "# pnas_cfg[f\"dataset.config.name\"] = pnas_name\r\n",
    "\r\n",
    "extant_cfg.datamodule.config.dataset.name = extant_name\r\n",
    "extant_datamodule = LeavesLightningDataModule(extant_cfg) #.datamodule.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d784f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c22b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.utils.common_utils import LabelEncoder, trainval_split\r\n",
    "from lightning_hydra_classifiers.data.common import CommonDataset, LeavesLightningDataModule, plot_split_distributions\r\n",
    "# default_config = initialize_config(config_dir=config_dir,\r\n",
    "#                                    overrides=[\"datamodule=default_datamodule\"])\r\n",
    "# config = DictConfig({\"datamodule.dataset.name\":\"Extant_family_10_minus_PNAS_family_100_512\"})\r\n",
    "# default_config = DictConfig({'datamodule':LeavesLightningDataModule.default_config()})\r\n",
    "\r\n",
    "# pp(OmegaConf.to_container(datamodule.datamodule_config, resolve=True))\r\n",
    "# default_config = DictConfig({'datamodule':LeavesLightningDataModule.default_config()})\r\n",
    "# user_config = DictConfig({\"datamodule\":\r\n",
    "#                               {\"dataset\":\r\n",
    "#                                    {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\r\n",
    "#                               }\r\n",
    "#                          })\r\n",
    "# pp(OmegaConf.to_container(OmegaConf.merge(default_config, user_config), resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_1024_minus_PNAS_family_100_1024\"\r\n",
    "\r\n",
    "#         config = DictConfig({\"dataset\":\r\n",
    "#                                        {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\r\n",
    "#                             })\r\n",
    "# config.dataset.config.name = \"Extant_family_10_1024_in_PNAS_family_100_1024\"\r\n",
    "datamodule = LeavesLightningDataModule(config=None, #config, #default_config,\r\n",
    "                                       data_dir=output_dir)\r\n",
    "config.hparams.classes = datamodule.classes\r\n",
    "config.hparams.num_classes = len(config.hparams.classes)\r\n",
    "config.dataset.config.classes = datamodule.classes\r\n",
    "config.dataset.config.num_classes = len(config.hparams.classes)\r\n",
    "\r\n",
    "\r\n",
    "data_loader = datamodule.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ac7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09eda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd5a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a664465",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'family'\r\n",
    "seed = 5687\r\n",
    "val_train_split = 0.2\r\n",
    "\r\n",
    "# pnas_name = \"PNAS_family_100_512\"\r\n",
    "# extant_name = \"Extant_family_10_512\"\r\n",
    "\r\n",
    "pnas_name = \"PNAS_family_100_1024\"#512\"\r\n",
    "extant_name = \"Extant_family_10_1024\" #512\"\r\n",
    "\r\n",
    "\r\n",
    "## Load primary Extant and PNAS datamodules\r\n",
    "pnas_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=pnas_dataset\",\r\n",
    "                                  \"datamodule=standalone_datamodule\"])\r\n",
    "# pnas_cfg[f\"dataset.config.name\"] = pnas_name\r\n",
    "\r\n",
    "pnas_cfg.datamodule.config.dataset.name = pnas_name\r\n",
    "pp(OmegaConf.to_container(pnas_cfg, resolve=False))\r\n",
    "\r\n",
    "pnas_datamodule = LeavesLightningDataModule(pnas_cfg)#.datamodule.config)\r\n",
    "\r\n",
    "extant_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=extant_dataset\",\r\n",
    "                                   \"datamodule=standalone_datamodule\"])\r\n",
    "# pnas_cfg[f\"dataset.config.name\"] = pnas_name\r\n",
    "\r\n",
    "extant_cfg.datamodule.config.dataset.name = extant_name\r\n",
    "extant_datamodule = LeavesLightningDataModule(extant_cfg) #.datamodule.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\r\n",
    "\r\n",
    "dataset_name = f\"{extant_name}_minus_{pnas_name}\" # Extant_family_10_512_minus_PNAS_family_100_512\r\n",
    "test_dataset_name = f\"{extant_name}_in_{pnas_name}\"\r\n",
    "output_dir = f\"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/{dataset_name}\"\r\n",
    "\r\n",
    "#########################################\r\n",
    "\r\n",
    "extant_dataset = extant_datamodule.dataset\r\n",
    "pnas_dataset = pnas_datamodule.dataset\r\n",
    "extant_df = extant_dataset.samples_df\r\n",
    "pnas_df = pnas_dataset.samples_df\r\n",
    "\r\n",
    "#########################################\r\n",
    "#########################################\r\n",
    "\r\n",
    "# extant_minus_pnas -> rows exclusive to extant dataset\r\n",
    "extant_minus_pnas = left_exclusive(data_df=extant_df,\r\n",
    "                                   other_df=pnas_df,\r\n",
    "                                   id_col=\"catalog_number\")\r\n",
    "# extant_in_pnas -> rows from extant dataset that share a catalog_number with PNAS\r\n",
    "extant_in_pnas = intersection(data_df=extant_df,\r\n",
    "                              other_df=pnas_df,\r\n",
    "                              id_col=\"catalog_number\",\r\n",
    "                              suffixes=(\"_extant\", \"_pnas\"))\r\n",
    "\r\n",
    "suffixes=(\"_extant\", \"_pnas\")\r\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\r\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})\r\n",
    "\r\n",
    "#########################################\r\n",
    "### GENERATE TRAIN VAL SPLIT INDICES\r\n",
    "#########################################\r\n",
    "\r\n",
    "y = extant_minus_pnas[y_col]\r\n",
    "data_splits = trainval_split(x=None,\r\n",
    "                             y=y,\r\n",
    "                             val_train_split=val_train_split,\r\n",
    "                             random_state=seed,\r\n",
    "                             stratify=True\r\n",
    "                             )\r\n",
    "train_idx, val_idx = data_splits['train'][0], data_splits['val'][0]\r\n",
    "\r\n",
    "#########################################\r\n",
    "### CREATE COMMONDATASETS FROM DATAFRAMES, USING THE SPLIT INDICES\r\n",
    "#########################################\r\n",
    "\r\n",
    "\r\n",
    "train_df = extant_minus_pnas.iloc[train_idx,:]\r\n",
    "val_df = extant_minus_pnas.iloc[val_idx,:]\r\n",
    "\r\n",
    "train_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\r\n",
    "                                                               sample_df=train_df,\r\n",
    "                                                               config=None,\r\n",
    "                                                               return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                               subset_key=\"train\")\r\n",
    "\r\n",
    "val_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\r\n",
    "                                                             sample_df=val_df,\r\n",
    "                                                             config=None,\r\n",
    "                                                             return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                             subset_key=\"val\")\r\n",
    "\r\n",
    "test_dataset_extant_in_pnas = CommonDataset.from_dataframe(sample_df=extant_in_pnas,\r\n",
    "                                                            config=None,\r\n",
    "                                                            return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                            subset_key=\"test\")\r\n",
    "\r\n",
    "data_splits= {'train':train_dataset_extant_minus_pnas,\r\n",
    "              'val': val_dataset_extant_minus_pnas,\r\n",
    "              'test':test_dataset_extant_in_pnas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits['train'].config.name = dataset_name\r\n",
    "data_splits['train'].config.subset_key = \"train\"\r\n",
    "\r\n",
    "data_splits['val'].config.name = dataset_name\r\n",
    "data_splits['val'].config.subset_key = \"val\"\r\n",
    "\r\n",
    "data_splits['test'].config.name = test_dataset_name\r\n",
    "data_splits['test'].config.subset_key = \"test\"\r\n",
    "\r\n",
    "#########################################\r\n",
    "### LABEL ENCODER\r\n",
    "#########################################\r\n",
    "\r\n",
    "replace = {\"Nothofagaceae\": \"Fagaceae\"}\r\n",
    "label_encoder = LabelEncoder(replace=replace) # class2idx)\r\n",
    "label_encoder.fit(data_splits[\"test\"].targets)\r\n",
    "label_encoder.fit(data_splits[\"train\"].targets)\r\n",
    "for d in list(data_splits.values()):\r\n",
    "    d.label_encoder = label_encoder\r\n",
    "\r\n",
    "    \r\n",
    "#########################################\r\n",
    "### EXPORT DATASET CONFIGURATION TO A COMBO OF CSV, JSON, YAML, AND JPG FILES.\r\n",
    "#########################################\r\n",
    "from lightning_hydra_classifiers.data.common import export_dataset_to_csv, import_dataset_from_csv\r\n",
    "# output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_512_minus_PNAS_family_100_512\"\r\n",
    "export_dataset_to_csv(data_splits=data_splits,\r\n",
    "                          label_encoder=label_encoder,\r\n",
    "                          output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e89182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_data_splits, conf = import_dataset_from_csv(data_catalog_dir = output_dir)\r\n",
    "# print(conf)\r\n",
    "# for k,v in loaded_data_splits.items():\r\n",
    "#     print(k, repr(v))\r\n",
    "\r\n",
    "\r\n",
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_1024_minus_PNAS_family_100_1024\"\r\n",
    "\r\n",
    "#         config = DictConfig({\"dataset\":\r\n",
    "#                                        {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\r\n",
    "#                             })\r\n",
    "# config.dataset.config.name = \"Extant_family_10_1024_in_PNAS_family_100_1024\"\r\n",
    "datamodule = LeavesLightningDataModule(config=None, #config, #default_config,\r\n",
    "                                       data_dir=output_dir)\r\n",
    "config.hparams.classes = datamodule.classes\r\n",
    "config.hparams.num_classes = len(config.hparams.classes)\r\n",
    "config.dataset.config.classes = datamodule.classes\r\n",
    "config.dataset.config.num_classes = len(config.hparams.classes)\r\n",
    "\r\n",
    "\r\n",
    "data_loader = datamodule.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a418d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaba64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1669954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_1024_minus_PNAS_family_100_1024\"\r\n",
    "\r\n",
    "config = DictConfig({\"dataset\":\r\n",
    "                               {\"name\":\"Extant_family_10_minus_PNAS_family_100_512\"}\r\n",
    "                    })\r\n",
    "datamodule = LeavesLightningDataModule(config=config, #default_config,\r\n",
    "                                       data_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3856a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def left_exclusive(data_df: pd.DataFrame, other_df: pd.DataFrame, id_col: str=\"catalog_number\") -> pd.DataFrame:\r\n",
    "#     \"\"\"\r\n",
    "#     Return a new dataframe containing only rows from `data_df` that do not share an `id_col` value with any row from `other_df`.\r\n",
    "    \r\n",
    "#     Equivalent to subtracting the set of `id_col` values in `other_df` from `data_df`\r\n",
    "#     \"\"\"\r\n",
    "#     omit = list(other_df[id_col].values)\r\n",
    "    \r\n",
    "#     return data_df[data_df[id_col].apply(lambda x: x not in omit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnas_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=pnas_dataset\"])\r\n",
    "pnas_data = LeavesLightningDataModule(pnas_cfg.datamodule.config)\r\n",
    "\r\n",
    "extant_cfg = initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=extant_dataset\"])\r\n",
    "extant_data = LeavesLightningDataModule(extant_cfg.datamodule.config)\r\n",
    "\r\n",
    "extant_dataset = extant_data.dataset\r\n",
    "pnas_dataset = pnas_data.dataset\r\n",
    "\r\n",
    "print(len(extant_dataset), len(pnas_dataset))\r\n",
    "\r\n",
    "extant_df = extant_dataset.samples_df\r\n",
    "pnas_df = pnas_dataset.samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extant_minus_pnas -> rows exclusive to extant dataset\r\n",
    "extant_minus_pnas = left_exclusive(data_df=extant_df,\r\n",
    "                                   other_df=pnas_df,\r\n",
    "                                   id_col=\"catalog_number\")\r\n",
    "\r\n",
    "# pnas_minus_extant -> rows exclusive to pnas dataset\r\n",
    "# pnas_minus_extant = left_exclusive(data_df=pnas_df,\r\n",
    "#                                    other_df=extant_df,\r\n",
    "#                                    id_col=\"catalog_number\")\r\n",
    "\r\n",
    "\r\n",
    "# print(extant_minus_pnas.shape, pnas_minus_extant.shape)\r\n",
    "\r\n",
    "# pnas_in_extant = intersection(data_df=pnas_df,\r\n",
    "#                               other_df=extant_df,\r\n",
    "#                               id_col=\"catalog_number\",\r\n",
    "#                               suffixes=(\"_pnas\", \"_extant\"))\r\n",
    "\r\n",
    "extant_in_pnas = intersection(data_df=extant_df,\r\n",
    "                              other_df=pnas_df,\r\n",
    "                              id_col=\"catalog_number\",\r\n",
    "                              suffixes=(\"_extant\", \"_pnas\"))\r\n",
    "\r\n",
    "\r\n",
    "# print(extant_in_pnas.shape, pnas_in_extant.shape)\r\n",
    "\r\n",
    "suffixes=(\"_extant\", \"_pnas\")\r\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\r\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})\r\n",
    "\r\n",
    "extant_in_pnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'family'\r\n",
    "seed = 5687\r\n",
    "val_train_split = 0.2\r\n",
    "\r\n",
    "y = extant_minus_pnas[y_col]\r\n",
    "\r\n",
    "data_splits = trainval_split(x=None,\r\n",
    "                             y=y,\r\n",
    "                               val_train_split=val_train_split,\r\n",
    "                               random_state=seed,\r\n",
    "                               stratify=True\r\n",
    "                               )\r\n",
    "\r\n",
    "data_splits['tra\n",
    "\n",
    "\n",
    "\n",
    "train_idx, val_idx = data_splits['train'][0], data_splits['val'][0]\n",
    "\n",
    "train_df = extant_minus_pnas.iloc[train_idx,:]\n",
    "val_df = extant_minus_pnas.iloc[val_idx,:]\n",
    "\n",
    "train_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\n",
    "                                                               sample_df=train_df,\n",
    "                                                               config=None,\n",
    "                                                               return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                               subset_key=\"train\")\n",
    "#                                                                subset_key=None) #\"train\")\n",
    "\n",
    "\n",
    "val_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\n",
    "                                                             sample_df=val_df,\n",
    "                                                             config=None,\n",
    "                                                             return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                             subset_key=\"val\")\n",
    "\n",
    "\n",
    "test_dataset_extant_in_pnas = CommonDataset.from_dataframe(sample_df=extant_in_pnas,\n",
    "                                                            config=None,\n",
    "                                                            return_signature = [\"image\",\"target\",\"path\"],\n",
    "                                                            subset_key=\"test\")\n",
    "\n",
    "data_splits= {'train':train_dataset_extant_minus_pnas,\n",
    "              'val': val_dataset_extant_minus_pnas,\n",
    "              'test':test_dataset_extant_in_pnas}in'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = data_splits['train'][0], data_splits['val'][0]\r\n",
    "\r\n",
    "train_df = extant_minus_pnas.iloc[train_idx,:]\r\n",
    "val_df = extant_minus_pnas.iloc[val_idx,:]\r\n",
    "\r\n",
    "train_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\r\n",
    "                                                               sample_df=train_df,\r\n",
    "                                                               config=None,\r\n",
    "                                                               return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                               subset_key=\"train\")\r\n",
    "#                                                                subset_key=None) #\"train\")\r\n",
    "\r\n",
    "\r\n",
    "val_dataset_extant_minus_pnas = CommonDataset.from_dataframe(\r\n",
    "                                                             sample_df=val_df,\r\n",
    "                                                             config=None,\r\n",
    "                                                             return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                             subset_key=\"val\")\r\n",
    "\r\n",
    "\r\n",
    "test_dataset_extant_in_pnas = CommonDataset.from_dataframe(sample_df=extant_in_pnas,\r\n",
    "                                                            config=None,\r\n",
    "                                                            return_signature = [\"image\",\"target\",\"path\"],\r\n",
    "                                                            subset_key=\"test\")\r\n",
    "\r\n",
    "data_splits= {'train':train_dataset_extant_minus_pnas,\r\n",
    "              'val': val_dataset_extant_minus_pnas,\r\n",
    "              'test':test_dataset_extant_in_pnas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in data_splits.items():    \r\n",
    "#     print(k, v.__repr__())\r\n",
    "\r\n",
    "# import_dataset_from_csv(self, data_dir: str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41303cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58affde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OmegaConf.to_container(test_dataset_extant_in_pnas.config, resolve=True)\r\n",
    "# label_encoder = LabelEncoder() # class2idx)\r\n",
    "# label_encoder.fit(data_splits[\"train\"].targets)\r\n",
    "\r\n",
    "# for d in list(data_splits.values()):\r\n",
    "#     d.label_encoder = label_encoder\r\n",
    "# test_dataset_extant_in_pnas.label_encoder\r\n",
    "# label_encoder = LabelEncoder() # class2idx)\r\n",
    "# label_encoder.fit(data_splits[\"test\"].targets)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# test_dataset_extant_in_pnas.label_encoder\r\n",
    "# label_encoder\r\n",
    "\r\n",
    "# test_df = data_splits[\"test\"].samples_df\r\n",
    "\r\n",
    "# replace = {\"Nothofagaceae\": \"Fagaceae\"}\r\n",
    "# label_encoder = LabelEncoder(replace=replace) # class2idx)\r\n",
    "# label_encoder.fit(data_splits[\"test\"].targets)\r\n",
    "\r\n",
    "# label_encoder.fit(data_splits[\"train\"].targets)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# for d in list(data_splits.values()):\r\n",
    "#     d.label_encoder = label_encoder\r\n",
    "    \r\n",
    "# label_encoder\r\n",
    "# test_df[test_df.family==\"Nothofagaceae\"].replace(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffcfa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import *\r\n",
    "# from omegaconf import DictConfig\r\n",
    "# from pathlib import Path\r\n",
    "# import matplotlib.pyplot as plt\r\n",
    "# pnas_df[pnas_df.catalog_number==\"Wolfe_8535\"]\r\n",
    "\r\n",
    "# set(data_splits[\"test\"].samples_df.family.astype(pd.CategoricalDtype()).cat.categories) - set(pnas_df.family.astype(pd.CategoricalDtype()).cat.categories)\r\n",
    "\r\n",
    "# test_dataset_extant_in_pnas.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34556dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(config: DictConfig, config_path: str):\r\n",
    "    with open(config_path, \"w\") as f:\r\n",
    "        f.write(OmegaConf.to_yaml(config, resolve=True))\r\n",
    "\r\n",
    "def load_config(config_path: str) -> DictConfig:    \r\n",
    "    with open(config_path, \"r\") as f:\r\n",
    "        loaded = OmegaConf.load(f)\r\n",
    "    return loaded\r\n",
    "\r\n",
    "\r\n",
    "def export_image_data_diagnostics(data_splits: Dict[str,CommonDataset],\r\n",
    "                                  output_dir: str='.',\r\n",
    "                                  max_samples: int = 64,\r\n",
    "                                  export_sample_images: bool=True,\r\n",
    "                                  export_class_distribution_plots: bool=True) -> Dict[str,str]:\r\n",
    "    image_paths = {\"images\": {},\r\n",
    "                   \"class_distribution_plots\":{}}\r\n",
    "    \r\n",
    "    image_dir = os.path.join(output_dir, \"images\")\r\n",
    "    plot_dir = os.path.join(output_dir, \"plots\")\r\n",
    "    os.makedirs(image_dir, exist_ok = True)\r\n",
    "    os.makedirs(plot_dir, exist_ok = True)\r\n",
    "\r\n",
    "    if export_sample_images:\r\n",
    "#         subsets = ['train', 'val', 'test']\r\n",
    "        for subset in data_splits.keys():\r\n",
    "            fig, ax = data_splits[subset].show_batch(indices=max_samples, include_colorbar=False,\r\n",
    "                                                     suptitle = f\"subset: {subset}, {max_samples} random images\")\r\n",
    "            img_path = os.path.join(image_dir, f\"subset: {subset}, {max_samples} random images.jpg\")\r\n",
    "            image_paths[\"images\"][subset] = img_path\r\n",
    "            plt.savefig(img_path)\r\n",
    "\r\n",
    "    if export_class_distribution_plots:\r\n",
    "        fig, ax = plot_split_distributions(data_splits=data_splits)\r\n",
    "        class_distribution_plot_path = os.path.join(plot_dir, f\"class_distribution_plots_{[subset for subset in data_splits.keys()]}\")\r\n",
    "        image_paths[\"class_distribution_plots\"][\"all\"] = class_distribution_plot_path\r\n",
    "        plt.savefig(class_distribution_plot_path)\r\n",
    "\r\n",
    "    return image_paths\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def export_dataset_to_csv(data_splits: Dict[str,CommonDataset],\r\n",
    "                          label_encoder: Optional[LabelEncoder]=None,\r\n",
    "                          output_dir: str='.',\r\n",
    "                          export_sample_images: bool=True,\r\n",
    "                          export_class_distribution_plots: bool=True) -> Dict[str,str]:\r\n",
    "    output_paths = {\"tables\":{},\r\n",
    "                    \"class_labels\":{},\r\n",
    "                    \"configs\":{}}\r\n",
    "    os.makedirs(output_dir, exist_ok=True)\r\n",
    "    for k, data in data_splits.items():\r\n",
    "        subset_data_path = os.path.join(output_dir, f\"{k}_data_table.csv\")\r\n",
    "        data.samples_df.to_csv(subset_data_path)\r\n",
    "        output_paths[\"tables\"][k] = subset_data_path\r\n",
    "        \r\n",
    "        if hasattr(data, \"config\"):\r\n",
    "            subset_config_path = os.path.join(output_dir, f\"{k}_config.yaml\")\r\n",
    "            save_config(config=data.config, config_path=subset_config_path)\r\n",
    "            output_paths[\"configs\"][k] = subset_config_path\r\n",
    "        \r\n",
    "        if hasattr(data, 'label_encoder') and (label_encoder is None):\r\n",
    "            subset_label_path = os.path.join(output_dir, k + \"_label_encoder.json\")\r\n",
    "            data.label_encoder.save(subset_label_path)\r\n",
    "            output_paths[\"class_labels\"][k] = subset_data_path\r\n",
    "            \r\n",
    "    if label_encoder is not None:\r\n",
    "        full_label_encoder_path = os.path.join(output_dir, \"label_encoder.json\")\r\n",
    "        label_encoder.save(full_label_encoder_path)\r\n",
    "        output_paths[\"class_labels\"][\"full\"] = full_label_encoder_path\r\n",
    "\r\n",
    "    \r\n",
    "    export_image_data_diagnostics(data_splits=data_splits,\r\n",
    "                                  output_dir=output_dir,\r\n",
    "                                  max_samples = 64,\r\n",
    "                                  export_sample_images=export_sample_images,\r\n",
    "                                  export_class_distribution_plots=export_class_distribution_plots)\r\n",
    "        \r\n",
    "    return output_paths\r\n",
    "    \r\n",
    "\r\n",
    "def import_dataset_from_csv(data_catalog_dir: str) -> Dict[str, CommonDataset]:\r\n",
    "    \r\n",
    "    data_paths = list(Path(data_catalog_dir).glob(\"*.csv\"))\r\n",
    "    config_paths = list(Path(data_catalog_dir).glob(\"*.yaml\"))\r\n",
    "    label_encoder_paths = list(Path(data_catalog_dir).glob(\"*.json\"))\r\n",
    "    \r\n",
    "    assert len(data_paths) == len(config_paths)\r\n",
    "    input_paths = {\"tables\":{},\r\n",
    "                   \"class_labels\":{},\r\n",
    "                   \"configs\":{}}\r\n",
    "    subsets = [\"train\", \"val\", \"test\"]\r\n",
    "    for subset in subsets:\r\n",
    "        input_paths[\"tables\"][subset] = [p for p in data_paths if p.stem.startswith(subset)][0]\r\n",
    "        input_paths[\"configs\"][subset] = [p for p in config_paths if p.stem.startswith(subset)][0]\r\n",
    "    \r\n",
    "    if len(label_encoder_paths) == 1:\r\n",
    "        label_encoder = LabelEncoder.load(label_encoder_paths[0])\r\n",
    "    else:\r\n",
    "        raise(f'Currently cannot distinguish between multiple label_encoders, please delete all but 1 in experiment directory. Contents: {label_encoder_paths}')\r\n",
    "    \r\n",
    "    data_splits = {}\r\n",
    "    for subset in subsets:\r\n",
    "        sample_df = pd.read_csv(input_paths[\"tables\"][subset])\r\n",
    "        config = load_config(input_paths[\"configs\"][subset])\r\n",
    "        data_splits[subset] = CommonDataset.from_dataframe(sample_df,\r\n",
    "                                                           config=config)\r\n",
    "        data_splits[subset].label_encoder = label_encoder\r\n",
    "        \r\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_d = data_splits['train']\r\n",
    "\r\n",
    "\r\n",
    "# save_config(config=config, config_path=subset_config_path)\r\n",
    "# loaded = load_config(config_path=subset_config_path)\r\n",
    "\r\n",
    "# pp(OmegaConf.to_container(config, resolve=True))\r\n",
    "\r\n",
    "# pp(OmegaConf.to_container(loaded))\r\n",
    "\r\n",
    "# pp(data_paths, config_paths, label_encoder_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = {\"Nothofagaceae\": \"Fagaceae\"}\r\n",
    "label_encoder = LabelEncoder(replace=replace) # class2idx)\r\n",
    "label_encoder.fit(data_splits[\"test\"].targets)\r\n",
    "label_encoder.fit(data_splits[\"train\"].targets)\r\n",
    "for d in list(data_splits.values()):\r\n",
    "    d.label_encoder = label_encoder\r\n",
    "    \r\n",
    "label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/media/data/jacob/GitHub/prj_fossils_contrastive/notebooks/Extant_family_10_512_minus_PNAS_family_100_512\"\r\n",
    "\r\n",
    "export_dataset_to_csv(data_splits=data_splits,\r\n",
    "                          label_encoder=label_encoder,\r\n",
    "                          output_dir=output_dir)\r\n",
    "\r\n",
    "loaded_data_splits = import_dataset_from_csv(data_catalog_dir = output_dir)\r\n",
    "\r\n",
    "for k,v in loaded_data_splits.items():\r\n",
    "    print(k, repr(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0830d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8bd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d64b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "plt.style.available\r\n",
    "\r\n",
    "# plt.style.use(\"seaborn-notebook\")\r\n",
    "plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90958177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self = data_splits['train']\r\n",
    "# import torch\r\n",
    "# import numpy as np\r\n",
    "# # indices = [0,1, 2,3,4,5]\r\n",
    "# indices = np.array(indices)\r\n",
    "# batch = [self[idx] for idx in indices]\r\n",
    "\r\n",
    "# # y = [batch[idx][1] for idx in indices]\r\n",
    "# y = torch.Tensor(np.array([(item[1]) for item in batch])).to(int)\r\n",
    "# y\r\n",
    "\r\n",
    "# batch = (torch.stack([item[0] for item in batch]),\r\n",
    "#          torch.stack([torch.Tensor(item[1]) for item in batch]))\r\n",
    "# return batch\n",
    "\n",
    "\r\n",
    "\r\n",
    "image_dir = os.path.join(output_dir, \"images\")\r\n",
    "plot_dir = os.path.join(output_dir, \"plots\")\r\n",
    "os.makedirs(image_dir, exist_ok = True)\r\n",
    "os.makedirs(plot_dir, exist_ok = True)\r\n",
    "\r\n",
    "\r\n",
    "image_paths = {\"images\": {},\r\n",
    "               \"class_distribution_plots\":{}}\r\n",
    "\r\n",
    "max_samples = 64\r\n",
    "subsets = ['train', 'val', 'test']\r\n",
    "for subset in subsets:    \r\n",
    "    fig, ax = data_splits[subset].show_batch(indices=max_samples, include_colorbar=False,\r\n",
    "                                             suptitle = f\"subset: {subset}, {max_samples} random images\")\r\n",
    "    img_path = os.path.join(image_dir, f\"subset: {subset}, {max_samples} random images.jpg\")\r\n",
    "    image_paths[\"images\"][subset] = img_path\r\n",
    "    plt.savefig(img_path)\r\n",
    "\r\n",
    "fig, ax = plot_split_distributions(data_splits=data_splits)\r\n",
    "class_distribution_plot_path = os.path.join(plot_dir, f\"class_distribution_plots_{[subset for subset in data_splits.keys()]}\")\r\n",
    "image_paths[\"class_distribution_plots\"][\"all\"] = class_distribution_plot_path\r\n",
    "\r\n",
    "plt.savefig(class_distribution_plot_path)\r\n",
    "\r\n",
    "\r\n",
    "# dir(ax)\r\n",
    "# cb=plt.colorbar()\r\n",
    "# cb.remove()\r\n",
    "# plt.draw()\r\n",
    "\r\n",
    "# plt.subplots_adjust(left=None, bottom=0.0, right=None, top=0.95, wspace=None, hspace=None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c260cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.display_grid(indices=64,\r\n",
    "               label_font_size=\"medium\")\r\n",
    "plt.suptitle(k, fontsize=\"large\")\r\n",
    "rows = 5\r\n",
    "plt.subplots_adjust(left=None, bottom=0.0, right=None, top=0.9, wspace=None, hspace=0.05*rows) #wspace=0.05, hspace=0.1)\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "torch.stack\r\n",
    "\r\n",
    "set(pnas_df.family) #- set(label_encoder.classes[:20])\r\n",
    "\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.style.use('fivethirtyeight')\r\n",
    "sns_context = \"talk\"\r\n",
    "sns_style = \"seaborn-bright\"\r\n",
    "sns.set_context(context=sns_context, font_scale=0.8)\r\n",
    "\r\n",
    "sns.set_palette(\"Accent\")\r\n",
    "# valid contexts = paper, notebook, talk, poster - \r\n",
    "# with notebook being 1:1 and paper being smaller and poster being largest\r\n",
    "# sns.set_style('darkgrid')\r\n",
    "# sns.set_palette('Set2')\r\n",
    "\r\n",
    "# plt.style.use(sns_style)\r\n",
    "fig, ax = plot_split_distributions(data_splits= {'train':train_dataset_extant_minus_pnas,\r\n",
    "                                                 'val': val_dataset_extant_minus_pnas,\r\n",
    "                                                 'test':test_dataset_extant_in_pnas})\r\n",
    "\r\n",
    "# for label in ax[1].xaxis.get_ticklabels()[::2]:\r\n",
    "#     label.set_visible(False)\n",
    "\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "classes = list(set(list(train_df['family'])))#[:100]\r\n",
    "num_classes = len(classes)\r\n",
    "df = pd.DataFrame(np.random.random((num_classes,num_classes)), columns=clasatex\n",
    "\n",
    "# plt.style.use('dark_background')\r\n",
    "plt.style.use('fivethirtyeight')\r\n",
    "\r\n",
    "plt.figure(figsize = (15,15))\r\n",
    "plt.imshow(df.values, cmap=\"BrBG\")\r\n",
    "\r\n",
    "\r\n",
    "label_format = '{:,.0f}'\r\n",
    "\r\n",
    "# nothing done to ax1 as it is a \"control chart.\"\r\n",
    "ax = plt.gca()\r\n",
    "\r\n",
    "\r\n",
    "import matplotlib.ticker as mticker\r\n",
    "\r\n",
    "# fixing yticks with \"set_yticks\"\r\n",
    "# ticks_loc = ax.get_yticks().tolist()\r\n",
    "# ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\r\n",
    "\r\n",
    "# # fixing yticks with matplotlib.ticker \"FixedLocator\"\r\n",
    "# ticks_loc = ax3.get_yticks().tolist()\r\n",
    "# ax3.yaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\r\n",
    "# ax3.set_yticklabels([label_format.format(x) for x in ticks_loc])\r\n",
    "\r\n",
    "# # fixing xticks with FixedLocator but also using MaxNLocator to avoid cramped x-labels\r\n",
    "# ax.xaxis.set_major_locator(mticker.MaxNLocator(75))\r\n",
    "# ticks_loc = ax.get_xticks().tolist()\r\n",
    "# ax.xaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\r\n",
    "# ax.set_xticklabels([label_format.format(x) for x in ticks_loc])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# ax = plt.gca()\r\n",
    "\r\n",
    "# ax.set_xticklabels(classes)\r\n",
    "# plt.xticks(\r\n",
    "# rotation=90, #45, \r\n",
    "# horizontalalignment='right',\r\n",
    "# fontweight='light',\r\n",
    "# fontsize='small'\r\n",
    "# )\r\n",
    "\r\n",
    "\r\n",
    "# plot a heatmap with annotation\r\n",
    "# sns.heatmap(df, annot=True, annot_kws={\"size\": 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pnas_df.shape[0] == 5311, \"Expected full PNAS_family_100 dataset to have 5311 samples\"\r\n",
    "\r\n",
    "assert pnas_minus_extant.shape[0] == 2518\r\n",
    "assert pnas_in_extant.shape[0] == 2793\r\n",
    "\r\n",
    "assert pnas_in_extant.shape[0] == extant_in_pnas.shape[0]\r\n",
    "\r\n",
    "assert pnas_in_extant.merge(pnas_minus_extant, on=\"catalog_number\", how=\"inner\").shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc32775",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnas_df.shape[0]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\r\n",
    "from PIL import Image\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "fig, ax = plt.subplots(1,2, figsize=(24,12))\r\n",
    "extant_in_pnas.iloc[:1,:].apply(lambda x: [ax[0].imshow(Image.open(x.path_extant).convert(\"L\"), cmap=\"gray\"), ax[1].imshow(Image.open(x.path_pnas).convert(\"L\"), cmap=\"gray\")], axis=1)\r\n",
    "ax[0].set_title(\"Extant (No-Crop)\")\r\n",
    "ax[1].set_title(\"PNAS (Cropped)\")\r\n",
    "plt.suptitle(Path(extant_in_pnas.iloc[0,:].path_extant).stem)\r\n",
    "# extant_in_pnas.iloc[:1,:].apply(lambda x: print(type(x)), axis=1)\n",
    "\n",
    "2793+22704\n",
    "\n",
    "# In order to only keep original columns\r\n",
    "suffixes=(\"_extant\", \"_pnas\")\r\n",
    "extant_in_pnas = extant_in_pnas.drop(columns = [c for c in extant_in_pnas.columns if c.endswith(suffixes[1])])\r\n",
    "extant_in_pnas = extant_in_pnas.rename(columns = {c:c.split(suffixes[0])[0] for c in extant_in_pnas.columns})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_df = extant_df\r\n",
    "# other_df = pnas_df\r\n",
    "# id_col = \"catalog_number\"\r\n",
    "\r\n",
    "\r\n",
    "data_df.sort_values(\"catalog_number\")\r\n",
    "intersected = data_df.merge(other_df, on=id_col, how='inner').sort_values(id_col)\n",
    "\n",
    "intersected\n",
    "\n",
    "other_df.sort_values(\"catalog_number\")\n",
    "\n",
    "# extant_minus_pnas\r\n",
    "\r\n",
    "pnas_minus_extant\r\n",
    "pnas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82707495",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(self.samples)\r\n",
    "data_df = data_df.convert_dtypes()\r\n",
    "\r\n",
    "other_df = pd.DataFrame(other.samples)\r\n",
    "other_df = other_df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f474c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94befeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a760e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CommonDataArithmetic(CommonDataset):\r\n",
    "    \r\n",
    "    \r\n",
    "#     @property\r\n",
    "#     def samples_df(self):        \r\n",
    "#         data_df = pd.DataFrame(self.samples)\r\n",
    "#         data_df = data_df.convert_dtypes()\r\n",
    "#         return data_df\r\n",
    "\r\n",
    "    \r\n",
    "# other_df = pd.DataFrame(other.samples)\r\n",
    "# other_df = other_df.convert_dtypes()\r\n",
    "    \r\n",
    "    \r\n",
    "#     def intersection(self, other):\r\n",
    "#         samples_df = self.samples_df\r\n",
    "#         other_df = other.samples_df\r\n",
    "        \r\n",
    "#         intersection = data_df.merge(other_df, how='inner', on=self.id_col)\r\n",
    "#         return intersection\r\n",
    "\r\n",
    "#     def __sub__(self, other)\r\n",
    "    \r\n",
    "#         intersection = self.intersection(other)\r\n",
    "#         samples_df = self.samples_df\r\n",
    "        \r\n",
    "#         remainder = samples_df[samples_df[self.id_col].apply(lambda x: x not in intersection[self.id_col])]\r\n",
    "        \r\n",
    "#  \n",
    "other_df = other_df.convert_dtypes()\r\n",
    "        \r\n",
    "        \r\n",
    "#         init_params = self.init_params\r\n",
    "#         init_params[\"files\"] = data_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f45842",
   "metadata": {},
   "outputs": [],
   "source": [
    "int.__sub__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6543713",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(OmegaConf.to_container(self.config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7877bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_data = data_df.merge(other_df, how='inner', on=self.id_col)\r\n",
    "\r\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86320b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data = extant_train + pnas_train\r\n",
    "concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(concat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29311a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\r\n",
    "import collections\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "sns.set_context(context='talk', font_scale=0.8)\r\n",
    "# sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb0293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_counts(targets: Sequence,\r\n",
    "                         sort_by: Optional[Union[str, bool, Sequence]]=\"count\"\r\n",
    "                        ) -> Dict[str, int]:\r\n",
    "    \r\n",
    "    counts = collections.Counter(targets)\r\n",
    "    if isinstance(sort_by, list):\r\n",
    "        counts = {k: counts[k] for k in sort_by}\r\n",
    "    elif (sort_by == \"count\"):\r\n",
    "        counts = dict(sorted(counts.items(), key = lambda x:x[1], reverse=True))\r\n",
    "    elif (sort_by is True):\r\n",
    "        counts = dict(sorted(counts.items(), key = lambda x:x[0], reverse=True))\r\n",
    "        \r\n",
    "    return counts\r\n",
    "\r\n",
    "def plot_class_distributions(targets: List[Any], \r\n",
    "                             sort_by: Optional[Union[str, bool, Sequence]]=\"count\",\r\n",
    "                             ax=None,\r\n",
    "                             xticklabels: bool=True):\r\n",
    "    \"\"\"\r\n",
    "    Example:\r\n",
    "        counts = plot_class_distributions(targets=data.targets, sort=True)\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    counts = compute_class_counts(targets,\r\n",
    "                                  sort_by=sort_by)\r\n",
    "                        \r\n",
    "    keys = list(counts.keys())\r\n",
    "    values = list(counts.values())\r\n",
    "\r\n",
    "    if ax is None:\r\n",
    "        plt.figure(figsize=(16,12))\r\n",
    "    ax = sns.histplot(x=keys, weights=values, discrete=True, ax=ax)\r\n",
    "    plt.sca(ax)\r\n",
    "    if xticklabels:\r\n",
    "        plt.xticks(\r\n",
    "            rotation=45, \r\n",
    "            horizontalalignment='right',\r\n",
    "            fontweight='light',\r\n",
    "            fontsize='medium'\r\n",
    "        )\r\n",
    "    else:\r\n",
    "        ax.set_xticklabels([])\r\n",
    "    \r\n",
    "    return counts\r\n",
    "\r\n",
    "\r\n",
    "def plot_split_distributions(data_splits: Dict[str, CommonDataset]):\r\n",
    "    \"\"\"\r\n",
    "    Create 3 vertically-stacked count plots of train, val, and test dataset class label distributions\r\n",
    "    \"\"\"\r\n",
    "    assert isinstance(data_splits, dict)\r\n",
    "    num_splits = len(data_splits)\r\n",
    "    \r\n",
    "    if num_splits < 4:\r\n",
    "        rows = num_splits\r\n",
    "        cols = 1\r\n",
    "    else:\r\n",
    "        rows = int(num_splits // 2)\r\n",
    "        cols = int(num_splits % 2)\r\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(16*cols,8*rows))\r\n",
    "    ax = ax.flatten()\r\n",
    "    \r\n",
    "    \r\n",
    "    train_key = [k for k,v in data_splits.items() if \"train\" in k]\r\n",
    "    if len(train_key)==1:\r\n",
    "        train_counts = compute_class_counts(data_splits[train_key[0]].targets,\r\n",
    "                                            sort_by=\"count\")\r\n",
    "    xticklabels=False\r\n",
    "    num_samples = 0\r\n",
    "    counts = {}\r\n",
    "    for i, (k, v) in enumerate(data_splits.items()):\r\n",
    "        if i == len(data_splits)-1:\r\n",
    "            xticklabels=True\r\n",
    "        counts[k] = plot_class_distributions(targets=v.targets, \r\n",
    "                                             sort_by=train_counts,\r\n",
    "                                             ax = ax[i],\r\n",
    "                                             xticklabels=xticklabels)\r\n",
    "        plt.gca().set_title(f\"{k} (n={len(v)})\", fontsize='large')\r\n",
    "        \r\n",
    "        num_samples += len(v)\r\n",
    "    \r\n",
    "    plt.suptitle('-'.join(list(data_splits.keys())) + f\"_splits (total={num_samples})\", fontsize='x-large')\r\n",
    "    plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.07)\r\n",
    "    \r\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.data.common import plot_split_distributions, compute_class_counts\r\n",
    "\r\n",
    "\r\n",
    "data_splits = {\"train\": data.train_dataset,\r\n",
    "               \"val\": data.val_dataset,\r\n",
    "               \"test\": data.test_dataset}\r\n",
    "\r\n",
    "# plot_split_distributions(data_splits=data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in list(data_splits.keys()):\r\n",
    "#     data_splits[k] = pd.DataFrame([data_splits[k]]).assign(split = k)\r\n",
    "    \r\n",
    "# target_splits = pd.concat(list(data_splits.values()))\r\n",
    "# target_splits.reset_index().describe(include='all')\r\n",
    "import numpy as np    \r\n",
    "\r\n",
    "\r\n",
    "# y_col = \"target\"\r\n",
    "y_col = \"family\"\r\n",
    "target_splits = pd.concat([pd.DataFrame(v.targets).assign(split = k) for k, v in data_splits.items()]).rename(columns={0:y_col})\r\n",
    "target_splits.reset_index().describe(include='all')\r\n",
    "\r\n",
    "# pd.DataFrame(target_splits.groupby(\"family\"))\r\n",
    "\r\n",
    "# pd.DataFrame(target_splits.groupby(\"split\"))\r\n",
    "\r\n",
    "pd.DataFrame(target_splits.groupby(\"split\")[\"family\"])#.agg([len]))\r\n",
    "\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "sns.countplot(data=target_splits,\r\n",
    "              x=\"family\",\r\n",
    "              hue=\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3068f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\r\n",
    "import dataclasses\r\n",
    "from omegaconf import DictConfig, OmegaConf\r\n",
    "from rich import print as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8046688",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\r\n",
    "xticklabels = True\r\n",
    "\r\n",
    "sns.set_style('darkgrid')\r\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT_TYPES = DictConfig({\r\n",
    "#     \"unstacked_grouped_countplot\": {\"multiple\":\"dodge\",\r\n",
    "#                                     \"stat\":\"count\", \"kde\":True, \"shrink\":0.95, \"binwidth\":1.5},\r\n",
    "#     \"stacked_filled_grouped_histplot\": {\"multiple\":\"fill\",\r\n",
    "#                                         \"stat\":\"probability\", \"shrink\":0.95, \"binwidth\":0.6},\r\n",
    "#     \"stacked_grouped_countplot\": {\"multiple\":\"stack\",\r\n",
    "#                                   \"stat\":\"count\", \"shrink\":0.9, \"binwidth\":1.5}\r\n",
    "#     })\r\n",
    "        \r\n",
    "# kwargs = PLOT_TYPES\r\n",
    "# pp(dict(kwargs))\r\n",
    "# pp(OmegaConf.to_container(cfg, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def plot_grouped_class_distributions(data: pd.DataFrame,\r\n",
    "                                     x_col: str=\"family\",\r\n",
    "                                     group_col: Optional[str]=None,\r\n",
    "                                     suptitle: Optional[str]=None,\r\n",
    "                                     savefig: Optional[str]=None,\r\n",
    "                                     single_fig_plot: Optional[bool]=True,\r\n",
    "                                     log_dir: Union[Path, str]=\".\",\r\n",
    "                                     height = 13,\r\n",
    "                                     width = 25,\r\n",
    "                                     kwargs: Optional[Dict[str,str]]=None):\r\n",
    "\r\n",
    "    if isinstance(kwargs, dict):\r\n",
    "        kwargs = [kwargs]\r\n",
    "    elif kwargs is None:\r\n",
    "        kwargs = [{\"kwargs\":{}}]\r\n",
    "        \r\n",
    "    default_kwargs = {\"shrink\":0.9, \"binwidth\":3.0}\r\n",
    "    axes = []\r\n",
    "    \r\n",
    "    \r\n",
    "    \r\n",
    "    counts = compute_class_counts(targets=data[x_col],\r\n",
    "                         sort_by=\"count\"\r\n",
    "                        )\r\n",
    "    class_order = list(counts.keys())\r\n",
    "    \r\n",
    "    if single_fig_plot:\r\n",
    "        rows = len(kwargs); cols = 1\r\n",
    "        figsize=(width*cols,height*rows)\r\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=figsize)\r\n",
    "        axes = axes.flatten()\r\n",
    "        \r\n",
    "\r\n",
    "    for i in range(len(kwargs)):\r\n",
    "        kwargs_i = default_kwargs\r\n",
    "        kwargs_i.update(kwargs[i][\"kwargs\"])\r\n",
    "        \r\n",
    "        if single_fig_plot:\r\n",
    "            ax = axes[i]\r\n",
    "            if \"title\" in kwargs[i]:\r\n",
    "                ax.set_title(kwargs[i][\"title\"], fontsize=\"large\")\r\n",
    "            plt.subplots_adjust(bottom=0.05, top=0.96, wspace=None, hspace=0.25)\r\n",
    "            \r\n",
    "        else:\r\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(20,12))\r\n",
    "            axes.append(ax)\r\n",
    "            if \"title\" in kwargs[i]:\r\n",
    "#                 ax.set_title(kwargs[i][\"title\"], fontsize=\"large\")\r\n",
    "                plt.suptitle(kwargs[i][\"title\"], fontsize=\"large\")\r\n",
    "            plt.subplots_adjust(bottom=0.15, top=0.95, wspace=None, hspace=0.3)            \r\n",
    "        \r\n",
    "        ax = sns.histplot(data=data,\r\n",
    "                          x=x_col,\r\n",
    "                          hue=group_col,\r\n",
    "                          ax=ax,\r\n",
    "                          pthresh=0.1,\r\n",
    "                          **kwargs_i)\r\n",
    "\r\n",
    "        plt.sca(ax)\r\n",
    "        sns.despine()\r\n",
    "        xticklabels = bool(data[x_col].nunique() < 100)\r\n",
    "        if xticklabels:\r\n",
    "            plt.xticks(\r\n",
    "                rotation=45, \r\n",
    "                horizontalalignment='right',\r\n",
    "                fontweight='light',\r\n",
    "                fontsize='small'\r\n",
    "            )\r\n",
    "        else:\r\n",
    "            ax.set_xticklabels([])\r\n",
    "            \r\n",
    "            \r\n",
    "        if not single_fig_plot:\r\n",
    "            if \"savefig\" in kwargs[i]:\r\n",
    "                plt.savefig(kwargs[i][\"savefig\"])\r\n",
    "\r\n",
    "    if single_fig_plot:\r\n",
    "        plt.suptitle(suptitle, fontsize=\"x-large\")\r\n",
    "        if isinstance(savefig, (Path, str)):\r\n",
    "            print(f'Saving: savefig={savefig}')\r\n",
    "            plt.savefig(savefig)\r\n",
    "        elif isinstance(suptitle, str):\r\n",
    "            print(f'Saving: suptitle={suptitle}')\r\n",
    "            plt.savefig(os.path.join(log_dir, f\"{suptitle}.png\"))\r\n",
    "    return fig, axes\r\n",
    "\r\n",
    "#         if \"title\" in kwargs[i]:\r\n",
    "#             ax.set_title(kwargs[i][\"title\"], fontsize=\"large\")        \r\n",
    "#     plt.suptitle(suptitle)    \r\n",
    "#     plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490fd2f9",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Latest data distribution plots -- July 18th, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=pnas_dataset\"]),\r\n",
    "            initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=extant_dataset\"]),\r\n",
    "            initialize_config(config_dir=config_dir,\r\n",
    "                        overrides=[\"dataset=fossil_dataset\", \"hparams.image_size=1024\"])\r\n",
    "           ]\r\n",
    "\r\n",
    "logdir = f\"/media/data/jacob/GitHub/lightning-hydra-classifiers/outputs/data_distribution_logs\"\r\n",
    "os.makedirs(logdir, exist_ok=True)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "for i in range(len(configs)):\r\n",
    "\r\n",
    "    cfg = configs[i]\r\n",
    "    data = LeavesLightningDataModule(cfg.datamodule.config)\r\n",
    "    data_splits = {\"train\": data.train_dataset,\r\n",
    "                   \"val\": data.val_dataset,\r\n",
    "                   \"test\": data.test_dataset}\r\n",
    "\r\n",
    "\r\n",
    "    dataset_name = cfg.dataset.config.name\r\n",
    "    label_col = cfg.dataset.config.class_type\r\n",
    "    group_col = \"subset\"\r\n",
    "\r\n",
    "    kwargs_options = [{\"kwargs\":{\"multiple\":\"dodge\", \"stat\":\"count\", \"kde\":True, \"shrink\":0.9, \"binwidth\":2*1.5},\r\n",
    "                       \"title\":f\"{dataset_name}, Per-class countplot, grouped by subset\",\r\n",
    "                       \"savefig\":os.path.join(logdir, f\"{dataset_name}_{label_col}_unstacked_subset_count_distributions.png\")},\r\n",
    "                      {\"kwargs\":{\"multiple\":\"fill\", \"stat\":\"probability\", \"shrink\":0.95, \"binwidth\":2*0.6},\r\n",
    "                       \"title\":f\"{dataset_name}, Per-class filled histograms, grouped by subset\",\r\n",
    "                       \"savefig\":os.path.join(logdir, f\"{dataset_name}_{label_col}_stacked_subset_prior_probabilities.png\")},\r\n",
    "                      {\"kwargs\":{\"multiple\":\"stack\", \"stat\":\"count\", \"shrink\":0.9, \"binwidth\":2*1.5},\r\n",
    "                       \"title\":f\"{dataset_name}, Per-class countplot, grouped by subset\",\r\n",
    "                       \"savefig\":os.path.join(logdir, f\"{dataset_name}_{label_col}_stacked_subset_count_distributions.png\")}\r\n",
    "                     ]\r\n",
    "\r\n",
    "    target_splits = pd.concat([pd.DataFrame(v.targets).assign(**{group_col:k}) for k, v in data_splits.items()\r\n",
    "                              ]).rename(columns={0:label_col})\r\n",
    "\r\n",
    "\r\n",
    "    ### Sort classes by count\r\n",
    "    data_df = target_splits\r\n",
    "    counts = compute_class_counts(targets=data_df[label_col],\r\n",
    "                                  sort_by=\"count\"\r\n",
    "                        )\r\n",
    "    class_order = {label:i for i, label in enumerate(counts.keys())}\r\n",
    "\r\n",
    "    data_df = data_df.assign(family_order = data_df.family.apply(lambda x: class_order[x]))\r\n",
    "    target_splits = data_df.sort_values(by=[\"family_order\"], ascending=True).drop(columns=[\"family_order\"])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    plot_grouped_class_distributions(data=target_splits,\r\n",
    "                                     x_col=label_col,\r\n",
    "                                     group_col=group_col,\r\n",
    "                                     single_fig_plot=False,\r\n",
    "    #                                  suptitle=f\"Dataset: {dataset_name} {label_col} class distributions\",\r\n",
    "                                     log_dir = logdir,\r\n",
    "                                     kwargs = kwargs_options[:])\r\n",
    "\r\n",
    "    plot_grouped_class_distributions(data=target_splits,\r\n",
    "                                     x_col=label_col,\r\n",
    "                                     group_col=group_col,\r\n",
    "                                     single_fig_plot=True,\r\n",
    "                                     suptitle=f\"Dataset={dataset_name} {label_col} class distributions\",\r\n",
    "                                     log_dir = logdir,\r\n",
    "                                     kwargs = kwargs_options[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a49b67",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3793f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494b64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57552d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0572c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "assert isinstance(data_splits, dict)\n",
    "num_splits = len(data_splits)\n",
    "\n",
    "if num_splits < 4:\n",
    "    rows = num_splits\n",
    "    cols = 1\n",
    "else:\n",
    "    rows = int(num_splits // 2)\n",
    "    cols = int(num_splits % 2)\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(16*cols,8*rows))\n",
    "ax = ax.flatten()\n",
    "\n",
    "\n",
    "train_key = [k for k,v in data_splits.items() if \"train\" in k]\n",
    "if len(train_key)==1:\n",
    "    train_counts = compute_class_counts(data_splits[train_key[0]].targets,\n",
    "                                        sort_by=\"count\")\n",
    "xticklabels=False\n",
    "num_samples = 0\n",
    "counts = {}\n",
    "for i, (k, v) in enumerate(data_splits.items()):\n",
    "    if i == len(data_splits)-1:\n",
    "        xticklabels=True\n",
    "    counts[k] = plot_class_distributions(targets=v.targets, \n",
    "                                         sort_by=train_counts,\n",
    "                                         ax = ax[i],\n",
    "                                         xticklabels=xticklabels)\n",
    "    plt.gca().set_title(f\"{k} (n={len(v)})\", fontsize='large')\n",
    "\n",
    "    num_samples += len(v)\n",
    "\n",
    "plt.suptitle('-'.join(list(data_splits.keys())) + f\"_splits (total={num_samples})\", fontsize='x-large')\n",
    "plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.07)\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "width = 0.5\n",
    "\n",
    "temp_summer=[ random.uniform(20,40) for i in range(5)]\n",
    "temp_winter=[ random.uniform(0,10) for i in range(5)]\n",
    "\n",
    "fig=plt.figure(figsize=(10,6))\n",
    "\n",
    "city=['City A','City B','City C','City D','City E']\n",
    "x_pos_summer=list(range(1,6))\n",
    "x_pos_winter=[ i+width for i in x_pos_summer]\n",
    "\n",
    "graph_summer=plt.bar(x_pos_summer, temp_summer, color='tomato', label='Summer', width=width)\n",
    "graph_winter=plt.bar(x_pos_winter, temp_winter, color='dodgerblue', label='Winter', width=width)\n",
    "\n",
    "plt.xticks([i+width/2 for i in x_pos_summer],city)\n",
    "plt.title('City Temperature')\n",
    "plt.ylabel('Temperature ($^\\circ$C)')\n",
    "\n",
    "#Annotating graphs\n",
    "for summer_bar,winter_bar,ts,tw in zip(graph_summer,graph_winter,temp_summer,temp_winter):\n",
    "    plt.text(summer_bar.get_x() + summer_bar.get_width()/2.0,summer_bar.get_height(),'%.2f$^\\circ$C'%ts,ha='center',va='bottom')\n",
    "    plt.text(winter_bar.get_x() + winter_bar.get_width()/2.0,winter_bar.get_height(),'%.2f$^\\circ$C'%tw,ha='center',va='bottom')\n",
    "\n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c57770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b722c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.train_dataset)\n",
    "display(data.val_dataset)\n",
    "display(data.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.train_dataloader()\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dir(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"0\":[0,1,2,3,4], \"1\":[0,1,2,3,4]}).T.to_records()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f93b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91803b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset.plot_trainvaltest_splits(data.train_dataset,\n",
    "                                     data.val_dataset,\n",
    "                                     data.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d33dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "torchvision.transforms.ToPILImage()(data.train_dataset[387][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bec2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pathlib import Path\n",
    "# path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "# # path_schema: str = \"{family}_{genus}_{species}_{catalog_number}\"\n",
    "\n",
    "# # # path_schema = Path(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_1536/train/Fabaceae\")\n",
    "# # filepath = 'Fabaceae_Derris_alborubra_Wolfe_9829.jpg'\n",
    "# sep = \"_\"\n",
    "\n",
    "# # from dataclasses import dataclass\n",
    "# # from typing import *\n",
    "\n",
    "\n",
    "# # @dataclass \n",
    "# # class PathSchema:\n",
    "# #     path_schema: str = Path(\"{family}_{genus}_{species}_{collection}_{catalog_number}\")\n",
    "# #     schema_parts: List[str] = path_schema.split(sep)\n",
    "# #     maxsplit = len(schema_parts) - 2\n",
    "    \n",
    "# #     def parse(self, path: Union[Path, str], sep: str=\"_\"):\n",
    "    \n",
    "# #         parts = Path(path).stem.split(sep, maxsplit=maxsplit)\n",
    "# #         if len(parts) == 5:\n",
    "# #             family, genus, species, collection, catalog_number = parts\n",
    "# #         if len(parts) == 4:\n",
    "# #             family, genus, species, catalog_number = parts\n",
    "# #             collection = catalog_number.split(\"_\")\n",
    "\n",
    "# #         return family, genus, species, collection, catalog_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96698422",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Extant_Leaves/Aizoaceae/Aizoaceae_Galenia_pubescens_Hickey_Hickey_8097.jpg\").stem\n",
    "\n",
    "path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "schema_parts = path_schema.split(sep)\n",
    "maxsplit = len(schema_parts) - 2\n",
    "\n",
    "print(f\"schema_parts={schema_parts}\")\n",
    "print(f\"maxsplit={maxsplit}\")\n",
    "family, genus, species, collection, catalog_number = Path(filepath).stem.split(\"_\", maxsplit=maxsplit)\n",
    "print(family, genus, species, collection, catalog_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59fc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(\"/media/data_cifs/projects/prj_fossils/data/processed_data/data_splits/PNAS_family_100_1536/train/Fabaceae/Fabaceae_Derris_alborubra_Wolfe_9829.jpg\").stem\n",
    "\n",
    "path_schema: str = \"{family}_{genus}_{species}_{catalog_number}\"\n",
    "schema_parts = path_schema.split(sep)\n",
    "maxsplit = len(schema_parts) - 2\n",
    "\n",
    "print(f\"schema_parts={schema_parts}\")\n",
    "print(f\"maxsplit={maxsplit}\")\n",
    "\n",
    "family, genus, species, catalog_number = Path(filepath).stem.split(\"_\", maxsplit=maxsplit)\n",
    "print(family, genus, species, catalog_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toPIL = torchvision.transforms.ToPILImage(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba31041",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06425fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(stage='val')\n",
    "\n",
    "data.show_batch(stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d359f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.train_dataset\n",
    "\n",
    "train_dataloader = data.train_dataloader()\n",
    "\n",
    "train_dataset = data.get_dataset(\"train\")\n",
    "\n",
    "train_dataset.show_batch()\n",
    "\n",
    "# train_dataset = \n",
    "data.get_dataset(\"val\")\n",
    "\n",
    "# train_dataset = \n",
    "data.get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb82c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(OmegaConf.to_container(cfg.dataset, resolve=True))\n",
    "\n",
    "pp(OmegaConf.to_container(cfg.datamodule.config.dataset, resolve=True))\n",
    "\n",
    "pp(OmegaConf.to_container(cfg.datamodule, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Fossil\"\n",
    "pp([k for k in CommonDataset.available_datasets.keys() if dataset_name in k])\n",
    "\n",
    "dataset_name = \"PNAS\"\n",
    "pp([k for k in CommonDataset.available_datasets.keys() if dataset_name in k])\n",
    "\n",
    "dataset_name = \"Extant\"\n",
    "pp([k for k in CommonDataset.available_datasets.keys() if dataset_name in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a18fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Extant_config = OmegaConf.create({\"name\": \"Extant_family_10_512\",\n",
    "                            \"val_split\": 0.2,\n",
    "                            \"test_split\": 0.3,\n",
    "                            \"threshold\": 3,\n",
    "                            \"seed\": 987485,\n",
    "                            \"class_type\": \"family\",\n",
    "                            \"x_col\":\"path\",\n",
    "                            \"y_col\":\"${.class_type}\",\n",
    "                            \"id_col\":\"catalog_number\"\n",
    "})\n",
    "\n",
    "\n",
    "config = OmegaConf.create({\"name\": \"Fossil_512\",\n",
    "                            \"val_split\": 0.2,\n",
    "                            \"test_split\": 0.3,\n",
    "                            \"threshold\": 3,\n",
    "                            \"seed\": 987485,\n",
    "                            \"class_type\": \"family\",\n",
    "                            \"x_col\":\"path\",\n",
    "                            \"y_col\":\"${.class_type}\",\n",
    "                            \"id_col\":\"catalog_number\"\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pp(OmegaConf.to_container(config, resolve=True))\n",
    "data = CommonDataset(config=config,\n",
    "                     files=None,\n",
    "                     class2idx=None)\n",
    "\n",
    "data[1].image\n",
    "print(data.__repr__())\n",
    "\n",
    "data.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2506ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/datamodule/fossil_datamodule.yaml\"\n",
    "config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/multi-gpu.yaml\"\n",
    "config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac95224",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(OmegaConf.to_container(config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89a95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "196013f0",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ba40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Fossil_512\"\n",
    "dataset_dirs = CommonDataSelect.available_datasets[name]\n",
    "\n",
    "dataset_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd289d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CommonDataset.available_datasets.keys()\n",
    "\n",
    "# CommonDataset.available_datasets[\"Fossil_512\"]#.keys()\n",
    "\n",
    "# fossil.available_datasets\n",
    "\n",
    "# d0 = CommonDataSelect.select_dataset_by_name(\"Fossil_512\")\n",
    "\n",
    "# dir(OmegaConf)\n",
    "\n",
    "# config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/datamodule/fossil_datamodule.yaml\"\n",
    "# config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# d1 = torchdata.datasets.Files.from_folder(Path(dataset_dirs[0]), regex=\"*/*.jpg\")\n",
    "d2 = torchdata.datasets.Files.from_folder(Path(dataset_dirs[1]), regex=\"*/*.jpg\")\n",
    "# d2\n",
    "\n",
    "from itertools import repeat, chain\n",
    "from more_itertools import collapse, flatten\n",
    "\n",
    "\n",
    "cls = torchdata.datasets.Files\n",
    "\n",
    "log.info(f\"Concatenating dataset_dirs located at: {dataset_dirs}\")\n",
    "file_list = list(flatten(\n",
    "                    [cls.from_folder(Path(root),\n",
    "                                     regex=\"*/*.jpg\").files\n",
    "                     for root in dataset_dirs]\n",
    "                                            ))\n",
    "data = cls(files=file_list,\n",
    "           name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3314f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = (split[0] for split in data_splits.values())\n",
    "y_train, y_val, y_test = (split[1] for split in data_splits.values())\n",
    "\n",
    "\n",
    "\n",
    "from rich import print as pp\n",
    "\n",
    "\n",
    "pp(data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.2\n",
    "test_split = 0.3\n",
    "\n",
    "train_split = 1 - (val_split + test_split)\n",
    "\n",
    "val_relative_split = val_split/(train_split + val_split)\n",
    "train_relative_split = train_split/(train_split + val_split)\n",
    "random_state = 0\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_split, random_state=random_state, stratify=y)\n",
    "print(f\"x_train.shape={x_train.shape}, x_test.shape={x_test.shape}, y_train.shape={y_train.shape}, y_test.shape={y_test.shape}\")\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_relative_split, random_state=random_state, stratify=y_train)\n",
    "\n",
    "print(f\"x_train.shape={x_train.shape}, x_val.shape={x_val.shape}, y_train.shape={y_train.shape}, y_val.shape={y_val.shape}\")\n",
    "\n",
    "\n",
    "print(f'Absolute splits: {[train_split, val_split, test_split]}')\n",
    "print(f'Relative splits: [{train_relative_split:.2f}, {val_relative_split:.2f}, {test_split}]')\n",
    "\n",
    "print(f'train+val={train_split+val_split} | test={test_split}')\n",
    "print(f'train={train_relative_split:.2f} | val={val_relative_split:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "skf.get_n_splits(x, y)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0663f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(x, y):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(f'y_test: {y_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17876c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    \"\"\"Common base class for all `torchtraining` objects.\n",
    "    Defines default `__str__` and `__repr__`.\n",
    "    Most objects should customize `__str__` according to specific\n",
    "    needs.\n",
    "    Custom objects usually use `yaml.dump` to easily see parameters\n",
    "    and whole pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{type(self).__module__}.{type(self).__name__}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        parameters = \", \".join(\n",
    "            \"{}={}\".format(key, value)\n",
    "            for key, value in self.__dict__.items()\n",
    "            if not key.startswith(\"_\")\n",
    "        )\n",
    "        return \"{}({})\".format(self, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d428ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0378446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata\n",
    "torchdata.datasets.Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56174692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata\n",
    "dir(torchdata.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae2cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1ab0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=3\n",
    "test_split=0.3\n",
    "val_train_split=0.2\n",
    "batch_size=32\n",
    "num_workers=0\n",
    "seed=8567\n",
    "debug=False\n",
    "normalize=True\n",
    "image_size = 'auto'\n",
    "channels=3\n",
    "dataset_dir=None\n",
    "predict_on_split=\"val\"\n",
    "\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85022cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = FossilLightningDataModule(name=dataset_name,\n",
    "                                       threshold=threshold,\n",
    "                                       test_split=test_split,\n",
    "                                       val_train_split=val_train_split,\n",
    "                                       batch_size=batch_size,\n",
    "                                       num_workers=num_workers,\n",
    "                                       seed=seed,\n",
    "                                       debug=debug,\n",
    "                                       normalize=normalize,\n",
    "                                       image_size=image_size,\n",
    "                                       channels=channels,\n",
    "                                       predict_on_split=predict_on_split)\n",
    "\n",
    "datamodule\n",
    "\n",
    "datamodule.setup(\"fit\")\n",
    "datamodule.setup(\"test\")\n",
    "\n",
    "# datamodule.show_batch(\"train\")\n",
    "# datamodule.show_batch(\"val\")\n",
    "# datamodule.show_batch(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/Fossil_512_train-test/2021-07-12/06-03/model/checkpoints/\"\n",
    "ckpt_path = os.path.join(ckpt_dir, \"best_model-epoch-epoch=05--val_loss-val_loss=96.52.ckpt\")\n",
    "\n",
    "print(os.path.isfile(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_state = torch.load(ckpt_path)\n",
    "\n",
    "print(type(ckpt_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd43f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt_state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce946b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt_state['state_dict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransferLearningModel.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5735e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.create_trainvaltest_splits(dataset=data,\n",
    "                                                                  test_split=0.3,\n",
    "                                                                  val_train_split=0.2,\n",
    "                                                                  shuffle=True,\n",
    "                                                                  seed=3654,\n",
    "                                                                  plot_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_dir = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\"\n",
    "\n",
    "from hydra.experimental import compose, initialize, initialize_config_dir\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from rich import print as pp\n",
    "os.chdir(config_dir)\n",
    "\n",
    "# context initialization\n",
    "# with initialize(config_path=\"../configs\", job_name=\"test_app\"):\n",
    "\n",
    "with initialize_config_dir(config_dir=config_dir, job_name=\"multi-gpu_experiment\"):\n",
    "    \n",
    "    cfg = compose(config_name=\"multi-gpu\")\n",
    "#     print(OmegaConf.to_yaml(cfg))\n",
    "    \n",
    "    pp(OmegaConf.to_container(cfg, resolve=True))\n",
    "    \n",
    "    pp(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f990b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns_context = \"talk\"\n",
    "sns_style = \"seaborn-bright\"\n",
    "sns.set_context(context=sns_context, font_scale=0.8)\n",
    "# valid contexts = paper, notebook, talk, poster - \n",
    "# with notebook being 1:1 and paper being smaller and poster being largest\n",
    "plt.style.use(sns_style)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f436a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_class_distributions(targets: List[Any], \n",
    "#                              sort: Union[bool,Sequence]=True,\n",
    "#                              ax=None,\n",
    "#                              xticklabels: bool=True):\n",
    "#     \"\"\"\n",
    "#     Example:\n",
    "#         counts = plot_class_distributions(targets=data.targets, sort=True)\n",
    "#     \"\"\"\n",
    "#     counts = collections.Counter(targets)\n",
    "#     if hasattr(sort, \"__len__\"):\n",
    "#         counts = {k: counts[k] for k in sort}\n",
    "#     elif sort is True:\n",
    "#         counts = dict(sorted(counts.items(), key = lambda x:x[1], reverse=True))\n",
    "\n",
    "#     keys = list(counts.keys())\n",
    "#     values = list(counts.values())\n",
    "\n",
    "#     if ax is None:\n",
    "#         plt.figure(figsize=(16,12))\n",
    "#     ax = sns.histplot(x=keys, weights=values, discrete=True, ax=ax)\n",
    "#     plt.sca(ax)\n",
    "#     if xticklabels:\n",
    "#         plt.xticks(\n",
    "#             rotation=45, \n",
    "#             horizontalalignment='right',\n",
    "#             fontweight='light',\n",
    "#             fontsize='medium'\n",
    "#         )\n",
    "#     else:\n",
    "#         ax.set_xticklabels([])\n",
    "    \n",
    "#     return counts\n",
    "\n",
    "\n",
    "# def plot_trainvaltest_splits(train_data,\n",
    "#                              val_data,\n",
    "#                              test_data):\n",
    "#     \"\"\"\n",
    "#     Create 3 vertically-stacked count plots of train, val, and test dataset class label distributions\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots(3, 1, figsize=(16,8*3))\n",
    "\n",
    "#     train_counts = plot_class_distributions(targets=train_data.targets, sort=True, ax = ax[0], xticklabels=False)\n",
    "#     plt.gca().set_title(f\"train (n={len(train_data)})\", fontsize='large')\n",
    "#     sort_classes = train_counts.keys()\n",
    "\n",
    "#     val_counts = plot_class_distributions(targets=val_data.targets, ax = ax[1], sort=sort_classes, xticklabels=False)\n",
    "#     plt.gca().set_title(f\"val (n={len(val_data)})\", fontsize='large')\n",
    "#     test_counts = plot_class_distributions(targets=test_data.targets, ax = ax[2], sort=sort_classes)\n",
    "#     plt.gca().set_title(f\"test (n={len(test_data)})\", fontsize='large')\n",
    "\n",
    "#     plt.suptitle(f\"Train-Val-Test_splits (total={len(data)})\", fontsize='x-large')\n",
    "\n",
    "#     plt.subplots_adjust(bottom=0.1, top=0.95, wspace=None, hspace=0.07)\n",
    "    \n",
    "#     return fig, ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfd32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4da021",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trainvaltest_splits(train_data,\n",
    "                         val_data,\n",
    "                         test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d0305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853aea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c611a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00784bb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796c392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070b0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd7d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = select_from_dataset(data,\n",
    "                                 indices=train_idx,\n",
    "                                 update_class2idx=False,\n",
    "                                 x_col = 'path',\n",
    "                                 y_col = \"family\")\n",
    "\n",
    "val_data = select_from_dataset(data,\n",
    "                               indices=val_idx,\n",
    "                               update_class2idx=False,\n",
    "                               x_col = 'path',\n",
    "                               y_col = \"family\")\n",
    "val_data\n",
    "\n",
    "test_data = select_from_dataset(data,\n",
    "                                indices=test_idx,\n",
    "                                update_class2idx=False,\n",
    "                                x_col = 'path',\n",
    "                                y_col = \"family\")\n",
    "\n",
    "\n",
    "\n",
    "train_counts = plot_class_distributions(targets=train_data.targets, sort=True)\n",
    "sort_classes = train_counts.keys()\n",
    "val_counts = plot_class_distributions(targets=val_data.targets, sort=sort_classes)\n",
    "test_counts = plot_class_distributions(targets=test_data.targets, sort=sort_classes)\n",
    "\n",
    "\n",
    "train_data = (train_val_samples)\n",
    "train_samples = np.array(train_val_samples)[train_idx]\n",
    "val_samples = np.array(train_val_samples)[val_idx]\n",
    "\n",
    "\n",
    "counts = plot_class_distributions(targets=data.targets, sort=True)\n",
    "\n",
    "train_val_idx.shape\n",
    "test_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 test_split=0.3,\n",
    "                 val_train_split=0.2,\n",
    "                 shuffle: bool=False,\n",
    "                 seed: int=None):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        self.indices = np.arange(range(dataset_size))\n",
    "#         test_split = int(np.floor(test_train_split * dataset_size))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        targets = dataset.targets\n",
    "\n",
    "        train_val_idx, test_idx = train_test_split(\n",
    "                                               indices,\n",
    "                                               test_size=test_split,\n",
    "                                               random_state=seed,\n",
    "                                               shuffle=shuffle,\n",
    "                                               stratify=targets)\n",
    "        \n",
    "            \n",
    "            \n",
    "        train_indices, self.test_indices = self.indices[], self.indices[test_split:]\n",
    "        train_size = len(train_indices)\n",
    "        validation_split = int(np.floor((1 - val_train_split) * train_size))\n",
    "\n",
    "        self.train_indices, self.val_indices = train_indices[ : validation_split], train_indices[validation_split:]\n",
    "\n",
    "        self.train_sampler = SubsetRandomSampler(self.train_indices)\n",
    "        self.val_sampler = SubsetRandomSampler(self.val_indices)\n",
    "        self.test_sampler = SubsetRandomSampler(self.test_indices)\n",
    "\n",
    "    def get_train_split_point(self):\n",
    "        return len(self.train_sampler) + len(self.val_indices)\n",
    "\n",
    "    def get_validation_split_point(self):\n",
    "        return len(self.train_sampler)\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_split(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing train-validation-test dataloaders')\n",
    "        self.train_loader = self.get_train_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.val_loader = self.get_validation_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.test_loader = self.get_test_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_train_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing train dataloader')\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.train_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.train_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_validation_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing validation dataloader')\n",
    "        self.val_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.val_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.val_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_test_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing test dataloader')\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.test_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4add04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.samples)#.iloc[:,0]\n",
    "df = df.assign(sub_dataset = df.apply(lambda x: x[0].parts[-3], axis=1)) #.value_counts()\n",
    "\n",
    "df = df.rename(columns={0:\"path\",\n",
    "                        1:\"family\",\n",
    "                        2:\"genus\",\n",
    "                        3:\"species\",\n",
    "                        4:\"collection\",\n",
    "                        5:\"catalog_number\"})#.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "targets = dataset.targets\n",
    "\n",
    "train_idx, valid_idx = train_test_split(\n",
    "                                        indices,\n",
    "                                        test_size=test_split,\n",
    "                                        random_state=seed,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=targets)\n",
    "\n",
    "print(np.unique(np.array(targets)[train_idx], return_counts=True))\n",
    "print(np.unique(np.array(targets)[valid_idx], return_counts=True))\n",
    "\n",
    "\n",
    "# val_split = 0.2\n",
    "# test_split = 0.3\n",
    "# total = 1.0\n",
    "# trainval_split = total-test_split\n",
    "# print(trainval_split)\n",
    "# print(trainval_split - val_split)\n",
    "# print((val_split/(trainval_split)))# - val_split)\n",
    "\n",
    "(val_split*0.7)# + 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FossilDatasetSubset(FossilDataset):\n",
    "    \n",
    "#     def __init__(self,\n",
    "#                  split\n",
    "#                  files: List[Path]=None,\n",
    "#                  name: Optional[str]=None,\n",
    "#                  return_items: List[str] = [\"image\",\"target\",\"path\"],\n",
    "#                  image_return_type: str = \"tensor\",\n",
    "#                  *args, **kwargs):\n",
    "#                 ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('starting')\n",
    "\n",
    "model = models.resnet18()\n",
    "# inputs = torch.randn(5, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07749f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "batch_size = 64\n",
    "\n",
    "dataloader = DataLoader(data,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "#                         sampler=None,\n",
    "#                         batch_sampler=None,\n",
    "#                         num_workers=0,\n",
    "#                         collate_fn=None,\n",
    "#                         pin_memory=False,\n",
    "#                         drop_last=False,\n",
    "#                         timeout=0,\n",
    "#                         worker_init_fn=None)\n",
    "\n",
    "# idx = [0,10,20,50,100]\n",
    "# idx = 10\n",
    "idx = list(range(0,1000,100))\n",
    "print(len(idx))\n",
    "data.display_grid(idx, repeat_n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data.samples[0][0].parts[-3]\n",
    "\n",
    "df = pd.DataFrame(data.samples)#.iloc[:,0]\n",
    "df = df.assign(sub_dataset = df.apply(lambda x: x[0].parts[-3], axis=1)) #.value_counts()\n",
    "\n",
    "df = df.rename(columns={0:\"path\",\n",
    "                        1:\"family\",\n",
    "                        2:\"genus\",\n",
    "                        3:\"species\",\n",
    "                        4:\"collection\",\n",
    "                        5:\"catalog_number\"})#.value_counts()\n",
    "\n",
    "# df.value_counts().plot(kind='bar')\n",
    "\n",
    "chart = sns.catplot(\n",
    "    data=df, #[data['Year'].isin([1980, 2008])],\n",
    "    x='family',\n",
    "    kind='count',\n",
    "    palette='Set1',\n",
    "    row='sub_dataset',\n",
    "    aspect=3,\n",
    "    height=3\n",
    ")\n",
    "chart.set_xticklabels(rotation=65, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deba325",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sport = (df\n",
    "            .groupby('family')\n",
    "            .filter(lambda x : len(x) > 10)\n",
    "            .groupby(['family', 'genus'])\n",
    "#             .groupby(['genus', 'species'])\n",
    "            .size()\n",
    "            .unstack()\n",
    "           )\n",
    "by_sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bac819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def select_from_indices(data,\n",
    "#                         indices: Sequence,\n",
    "#                         update_class2idx: bool=False,\n",
    "#                         x_col = 'path',\n",
    "#                         y_col = \"family\") -> \"FossilDataset\":\n",
    "#     \"\"\"\n",
    "#     Helper method to create a new FossilDataset containing only samples contained in `indices`\n",
    "#     Useful for producing train/val/test splits\n",
    "    \n",
    "#     \"\"\"\n",
    "#     if update_class2idx:\n",
    "#         class2idx=None\n",
    "#     else:\n",
    "#         class2idx=data.class2idx\n",
    "\n",
    "    \n",
    "#     df = pd.DataFrame(data.samples)\n",
    "#     df = df.rename(columns={0:\"path\",\n",
    "#                             1:\"family\",\n",
    "#                             2:\"genus\",\n",
    "#                             3:\"species\",\n",
    "#                             4:\"collection\",\n",
    "#                             5:\"catalog_number\"})#.value_counts()\n",
    "    \n",
    "#     df = df.iloc[indices,:]\n",
    "    \n",
    "#     files = df[x_col].to_list()\n",
    "\n",
    "#     return FossilDataset(files=files,\n",
    "#                          name=data.name,\n",
    "#                          return_items=data.return_items,\n",
    "#                          image_return_type=data.image_return_type,\n",
    "#                          class2idx=class2idx)\n",
    "\n",
    "\n",
    "\n",
    "# def filter_df_by_threshold(df: pd.DataFrame,\n",
    "#                            threshold: int,\n",
    "#                            y_col: str='family'):\n",
    "#     \"\"\"\n",
    "#     Filter rare classes from dataset in a pd.DataFrame\n",
    "    \n",
    "#     Input:\n",
    "#         df (pd.DataFrame):\n",
    "#             Must contain at least 1 column with name given by `y_col`\n",
    "#         threshold (int):\n",
    "#             Exclude any rows from df that contain a `y_col` value with fewer than `threshold` members in all of df.\n",
    "#         y_col (str): default=\"family\"\n",
    "#             The column in df to look for rare classes to exclude.\n",
    "#     Output:\n",
    "#         (pd.DataFrame):\n",
    "#             Returns a dataframe with the same number of columns as df, and an equal or lower number of rows.\n",
    "#     \"\"\"\n",
    "#     return df.groupby(y_col).filter(lambda x: len(x) >= threshold)\n",
    "\n",
    "\n",
    "\n",
    "# def filter_samples_by_threshold(data: FossilDataset,\n",
    "#                                 threshold: int=1,\n",
    "#                                 update_class2idx: bool=True,\n",
    "#                                 x_col = 'path',\n",
    "#                                 y_col = \"family\") -> FossilDataset:\n",
    "#     if update_class2idx:\n",
    "#         class2idx=None\n",
    "#     else:\n",
    "#         class2idx=data.class2idx\n",
    "\n",
    "        \n",
    "#     df = pd.DataFrame(data.samples)\n",
    "#     df = df.rename(columns={0:\"path\",\n",
    "#                             1:\"family\",\n",
    "#                             2:\"genus\",\n",
    "#                             3:\"species\",\n",
    "#                             4:\"collection\",\n",
    "#                             5:\"catalog_number\"})#.value_counts()\n",
    "    \n",
    "#     df = filter_df_by_threshold(df=df,\n",
    "#                                 threshold=threshold,\n",
    "#                                 y_col=y_col)\n",
    "        \n",
    "#     files = df[x_col].to_list()\n",
    "\n",
    "#     return FossilDataset(files=files,\n",
    "#                          name=data.name,\n",
    "#                          return_items=data.return_items,\n",
    "#                          image_return_type=data.image_return_type,\n",
    "#                          class2idx=class2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset.targets\n",
    "\n",
    "# @classmethod\n",
    "# def create_trainvaltest_splits(cls,\n",
    "#                                dataset,\n",
    "#                                test_split: float=0.3,\n",
    "#                                val_train_split: float=0.2,\n",
    "#                                shuffle: bool=True,\n",
    "#                                seed: int=3654):\n",
    "\n",
    "#     dataset_size = len(dataset)\n",
    "#     indices = np.arange(dataset_size)\n",
    "\n",
    "#     samples = np.array(dataset.samples)\n",
    "#     targets = np.array(dataset.targets)\n",
    "\n",
    "#     train_val_idx, test_idx = train_test_split(\n",
    "#                                                indices,\n",
    "#                                                test_size=test_split,\n",
    "#                                                random_state=seed,\n",
    "#                                                shuffle=shuffle,\n",
    "#                                                stratify=targets)\n",
    "\n",
    "#     train_val_targets = targets[train_val_idx]\n",
    "\n",
    "#     trainval_indices = np.arange(len(train_val_targets))\n",
    "#     train_idx, val_idx = train_test_split(\n",
    "#                                           trainval_indices,\n",
    "#                                           test_size=val_train_split,\n",
    "#                                           random_state=seed,\n",
    "#                                           shuffle=shuffle,\n",
    "#                                           stratify=train_val_targets)\n",
    "\n",
    "#     train_data = data.select_from_indices(indices=train_idx,\n",
    "#                                           update_class2idx=False,\n",
    "#                                           x_col = 'path',\n",
    "#                                           y_col = \"family\")\n",
    "\n",
    "\n",
    "#     val_data = data.select_from_indices(indices=val_idx,\n",
    "#                                         update_class2idx=False,\n",
    "#                                         x_col = 'path',\n",
    "#                                         y_col = \"family\")\n",
    "\n",
    "\n",
    "#     test_data = data.select_from_indices(indices=test_idx,\n",
    "#                                          update_class2idx=False,\n",
    "#                                          x_col = 'path',\n",
    "#                                          y_col = \"family\")\n",
    "\n",
    "\n",
    "#     return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f030ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "g = sns.heatmap(\n",
    "    by_sport, \n",
    "    square=True, # make cells square\n",
    "    cbar_kws={'fraction' : 0.01}, # shrink colour bar\n",
    "    cmap='OrRd', # use orange/red colour map\n",
    "    linewidth=1 # space between cells\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ade18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "g = sns.heatmap(\n",
    "    by_sport, \n",
    "    square=True,\n",
    "    cbar_kws={'fraction' : 0.01},\n",
    "    cmap='OrRd',\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "g.set_yticklabels(g.get_yticklabels(), rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(\n",
    "    data=data[data['Year'].isin([1980, 2008])],\n",
    "    x='Sport',\n",
    "    kind='count',\n",
    "    palette='Set1',\n",
    "    row='Year',\n",
    "    aspect=3,\n",
    "    height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac206ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "count_dist = collections.Counter(data.targets)\n",
    "# count_dist.update(data.targets)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_class_distributions(targets: List[Any])\n",
    "test = count_dist #{1:1,2:1,3:1,4:2,5:3,6:5,7:4,8:2,9:1,10:1}\n",
    "# with matplotlib\n",
    "plt.hist(list(test.keys()), weights=list(test.values()))\n",
    "\n",
    "test = sorted(test.items(), key = lambda x:x[1], reverse=True)\n",
    "\n",
    "test\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# keys = list(test.keys())\n",
    "# values = list(test.values())\n",
    "\n",
    "keys = [i[0] for i in test]\n",
    "values = [i[1] for i in test]\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "chart = sns.histplot(x=keys, weights=values, discrete=True)\n",
    "plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='x-large'  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# chart.set_xticklabels(\n",
    "#     chart.get_xticklabels(), \n",
    "#     rotation=45, \n",
    "#     horizontalalignment='right',\n",
    "#     fontweight='light',\n",
    "#     fontsize='x-large'\n",
    "    \n",
    "# )\n",
    "\n",
    "# None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b395972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with seaborn (use hist_kws to send arugments to plt.hist, used underneath)\n",
    "sns.distplot(range(len(list(test.keys()))), hist_kws={\"weights\":list(test.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0062ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "\n",
    "max_batches = 10\n",
    "\n",
    "reporter = MemReporter()\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(i, len(batch), batch[0].shape)\n",
    "\n",
    "    print('========= before backward =========')\n",
    "    reporter.report()\n",
    "    out = model(batch[0])\n",
    "    \n",
    "    loss = nn.functional.cross_entropy(out, batch[1])\n",
    "    loss.backward()\n",
    "    print('========= after backward =========')\n",
    "    reporter.report()\n",
    "    \n",
    "    if i>=max_batches:\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077fb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec1da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762de834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dde5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter = MemReporter(model)\n",
    "\n",
    "\n",
    "print('========= before loop =========')\n",
    "reporter.report()\n",
    "for batch in data[]\n",
    "out.backward()\n",
    "print('========= after backward =========')\n",
    "reporter.report()\n",
    "###################################################\n",
    "import torch\n",
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "lstm = torch.nn.LSTM(1024, 1024).cuda()\n",
    "reporter = MemReporter(lstm)\n",
    "reporter.report(verbose=True)\n",
    "inp = torch.Tensor(10, 10, 1024).cuda()\n",
    "out, _ = lstm(inp)\n",
    "out.mean().backward()\n",
    "reporter.report(verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "# with torch.autograd.profiler.profile() as prof:\n",
    "    print('starting')\n",
    "    inputs = torch.randn(5,3,224,224, device='cuda')\n",
    "    print('half way')\n",
    "    outputs = inputs + torch.randn(5,3,224,224, device='cuda')\n",
    "    print('ending')\n",
    "    \n",
    "# print(prof)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ba59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_memlab import MemReporter\n",
    "# linear = torch.nn.Linear(1024, 1024).cuda()\n",
    "# linear2 = torch.nn.Linear(1024, 1024).cuda()\n",
    "\n",
    "\n",
    "# def inner():\n",
    "#     torch.nn.Linear(100, 100).cuda()\n",
    "\n",
    "# def outer():\n",
    "#     linear = torch.nn.Linear(100, 100).cuda()\n",
    "#     linear2 = torch.nn.Linear(100, 100).cuda()\n",
    "#     inner()\n",
    "# reporter = MemReporter()\n",
    "# reporter.report()\n",
    "\n",
    "linear = torch.nn.Linear(1024, 1024).cuda()\n",
    "inp = torch.Tensor(512, 1024).cuda()\n",
    "# pass in a model to automatically infer the tensor names\n",
    "reporter = MemReporter(linear)\n",
    "out = linear(inp).mean()\n",
    "print('========= before backward =========')\n",
    "reporter.report()\n",
    "out.backward()\n",
    "print('========= after backward =========')\n",
    "reporter.report()\n",
    "###################################################\n",
    "import torch\n",
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "lstm = torch.nn.LSTM(1024, 1024).cuda()\n",
    "reporter = MemReporter(lstm)\n",
    "reporter.report(verbose=True)\n",
    "inp = torch.Tensor(10, 10, 1024).cuda()\n",
    "out, _ = lstm(inp)\n",
    "out.mean().backward()\n",
    "reporter.report(verbose=True)\n",
    "\n",
    "# import torch\n",
    "# from pytorch_memlab import LineProfiler\n",
    "\n",
    "# def inner():\n",
    "#     torch.nn.Linear(100, 100).cuda()\n",
    "\n",
    "# def outer():\n",
    "#     linear = torch.nn.Linear(100, 100).cuda()\n",
    "#     linear2 = torch.nn.Linear(100, 100).cuda()\n",
    "#     inner()\n",
    "\n",
    "# with LineProfiler(outer, inner) as prof:\n",
    "#     outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5026b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pytorch_memlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4dd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.print_stats()\n",
    "\n",
    "dir(prof)\n",
    "# type(prof)\n",
    "\n",
    "prof.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473111d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "# with torch.autograd.profiler.profile() as prof:\n",
    "    print('starting')\n",
    "    inputs = torch.randn(5,3,224,224, device='cuda')\n",
    "    print('half way')\n",
    "    outputs = inputs + torch.randn(5,3,224,224, device='cuda')\n",
    "    print('ending')\n",
    "    \n",
    "# print(prof)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "\n",
    "# if i % 1000 == 0:\n",
    "#     print(\"Iteration: {}, memory: {}\".format(i, psutil.virtual_memory()))\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a69b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.profiler.PyTorchProfiler(dirpath=None,\n",
    "#                             filename=None,\n",
    "#                             group_by_input_shapes=False,\n",
    "#                             emit_nvtx=False,\n",
    "#                             export_to_chrome=True,\n",
    "#                             row_limit=20,\n",
    "#                             sort_by_key=None,\n",
    "#                             record_functions=None,\n",
    "#                             record_module_names=True,\n",
    "#                             profiled_functions=None,\n",
    "#                             output_filename=None, \n",
    "#                             **profiler_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = [0,1,2,3,4]\n",
    "# idx = 10\n",
    "# data.display_grid(idx, repeat_n=5)\n",
    "data.display_grid(idx, repeat_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec581430",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "self=data\n",
    "indices=10\n",
    "repeat_n=5\n",
    "from itertools import repeat, chain\n",
    "from more_itertools import collapse\n",
    "import random\n",
    "indices = random.sample(range(self.num_samples), indices)\n",
    "idx = collapse((repeat(i,repeat_n) for i in indices))\n",
    "\n",
    "# print([i for i in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee43db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self = data\n",
    "# indices = idx\n",
    "\n",
    "# if isinstance(indices, int):\n",
    "#     indices = random.sample(range(self.num_samples), indices)\n",
    "# indices = list(indices)\n",
    "# images = [self[idx][0] for idx in indices]\n",
    "# labels = [self.classes[self[idx][1]] for idx in indices]\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a28a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.display_grid(idx)\n",
    "plt.suptitle(f\"{idx} random images\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338c3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ec1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e07d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7301ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from PIL.Image import Image as PilImage\n",
    "# import textwrap, os\n",
    "\n",
    "# def display_images(\n",
    "#     images: [PilImage], \n",
    "#     columns=5, max_images=15,\n",
    "#     width=20, height=8,    \n",
    "#     label_wrap_length=50, \n",
    "#     label_font_size=8):\n",
    "\n",
    "#     if not images:\n",
    "#         print(\"No images to display.\")\n",
    "#         return \n",
    "\n",
    "#     if len(images) > max_images:\n",
    "#         print(f\"Showing {max_images} images of {len(images)}:\")\n",
    "#         images=images[0:max_images]\n",
    "\n",
    "#     rows = int(len(images)/columns)\n",
    "        \n",
    "#     height = max(height, rows * height)\n",
    "#     plt.figure(figsize=(width, height))\n",
    "#     for i, image in enumerate(images):\n",
    "\n",
    "#         plt.subplot(rows + 1, columns, i + 1)\n",
    "#         plt.imshow(image)\n",
    "\n",
    "#         if hasattr(image, 'filename'):\n",
    "#             title=image.filename\n",
    "#             if title.endswith(\"/\"): title = title[0:-1]\n",
    "#             title=os.path.basename(title)\n",
    "#             title=textwrap.wrap(title, label_wrap_length)\n",
    "#             title=\"\\n\".join(title)\n",
    "#             plt.title(title, fontsize=label_font_size); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23adf800",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[5][0]\n",
    "\n",
    "import random\n",
    "num_display = 12\n",
    "\n",
    "indices = random.sample(range(data.num_samples), num_display)\n",
    "\n",
    "indices\n",
    "\n",
    "# indices = range(0,4)\n",
    "\n",
    "display_images(\n",
    "    images = [data[idx][0] for idx in indices],\n",
    "    columns=5, max_images=15,\n",
    "    width=20, height=8,    \n",
    "    label_wrap_length=50, \n",
    "    label_font_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643597f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94753914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def parse_item(self, index: int):\n",
    "#         path = self.files[index]\n",
    "#         family, genus, species, collection, catalog_number = path.stem.split(\"_\", maxsplit=4)\n",
    "#         item = {\"path\":path,\n",
    "#                 \"target\":None,\n",
    "#                 \"family\":family,\n",
    "#                 \"genus\":genus,\n",
    "#                 \"species\":species,\n",
    "#                 \"collection\":collection,\n",
    "#                 \"catalog_number\":catalog_number}\n",
    "#         item[\"target\"] = item[self.class_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767ff9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e481144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14efa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e94679",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Previous Fossil class def code, now relocated to fossil.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86162f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import flatten\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58269455",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets = {\n",
    "    \"Wilf_Fossil_512\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Wilf_Fossil\",\n",
    "    \"Wilf_Fossil_1024\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Wilf_Fossil\",\n",
    "    \"Wilf_Fossil_1536\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Wilf_Fossil\",\n",
    "    \"Wilf_Fossil_2048\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Wilf_Fossil\",\n",
    "    \n",
    "    \"Florissant_Fossil_512\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Florissant_Fossil\",\n",
    "    \"Florissant_Fossil_1024\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Florissant_Fossil\",\n",
    "    \"Florissant_Fossil_1536\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Florissant_Fossil\",\n",
    "    \"Florissant_Fossil_2048\": \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Florissant_Fossil\"\n",
    "}\n",
    "\n",
    "available_datasets[\"Fossil_512\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Wilf_Fossil\",\n",
    "                                    \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_512/Florissant_Fossil\"]\n",
    "available_datasets[\"Fossil_1024\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Wilf_Fossil\",\n",
    "                                     \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1024/Florissant_Fossil\"]\n",
    "available_datasets[\"Fossil_1536\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Wilf_Fossil\",\n",
    "                                     \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_1536/Florissant_Fossil\"]\n",
    "available_datasets[\"Fossil_2048\"] = [\"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Wilf_Fossil\",\n",
    "                                     \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v0_3/Fossil/ccrop_2048/Florissant_Fossil\"]\n",
    "\n",
    "fossil_collections = {\"Florissant\":\"florissant_fossil\",\n",
    "                      \"Wilf\":\"wilf_fossil\"}\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    name: str\n",
    "    dataset: str=None\n",
    "    collection: str=None\n",
    "    resolution: int=None\n",
    "        \n",
    "    num_files: Optional[int]=None\n",
    "    num_classes: Optional[int]=None\n",
    "    class_type: str=\"family\"\n",
    "    path_schema: str = \"{family}_{genus}_{species}_{collection}_{catalog_number}\"\n",
    "                \n",
    "        \n",
    "    def __init__(self, name: str, **kwargs):\n",
    "        self.name = name\n",
    "        parts = self.name.split(\"_\")\n",
    "        self.resolution = int(parts[-1])\n",
    "        if len(parts)==3:\n",
    "            self.dataset = parts[1]\n",
    "            self.collection = \"_\".join(parts[:2])\n",
    "        elif len(parts)==2:    \n",
    "            self.dataset = parts[0]\n",
    "            self.collection = [\"_\".join([c, self.dataset]) for c in fossil_collections.keys()]\n",
    "            \n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        disp = f\"\"\"<{str(type(self)).strip(\"'>\").split('.')[1]}>:\"\"\"\n",
    "        \n",
    "        disp += \"\\nFields:\\n\"\n",
    "        for k in self.__dataclass_fields__.keys():\n",
    "            disp += f\"    {k}: {getattr(self,k)}\\n\"\n",
    "        return disp\n",
    "    \n",
    "\n",
    "DatasetConfig(\"Fossil_512\")\n",
    "\n",
    "class FossilDataset(torchdata.datasets.Files): #ImageDataset):\n",
    "    \n",
    "#     loader: Callable = Image.open\n",
    "    transform = None\n",
    "    target_transform = None\n",
    "    \n",
    "    class_type: str=\"family\"\n",
    "    totensor: Callable = torchvision.transforms.ToTensor()\n",
    "    def __init__(self,\n",
    "                 files: List[Path],\n",
    "                 return_items: List[str] = [\"image\",\"target\",\"path\"],\n",
    "                 image_return_type: str = \"tensor\",\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(files=files, *args, **kwargs)\n",
    "        \n",
    "        self.name = kwargs.get(\"name\",\"\")\n",
    "        self.return_items = return_items\n",
    "        self.image_return_type = image_return_type\n",
    "        \n",
    "        self.samples = [self.parse_item(idx) for idx in range((len(self)))]\n",
    "        self.targets = [sample[1] for sample in self.samples]\n",
    "        self._imgs = None\n",
    "        self.classes = sorted(set(self.targets))\n",
    "        self.class2idx = {name:idx for idx, name in enumerate(self.classes)}\n",
    "        \n",
    "        self.config = DatasetConfig(self.name,\n",
    "                                    class_type=self.class_type,\n",
    "                                    num_files=len(self.files),\n",
    "                                    num_classes=len(self.classes)\n",
    "                                   )\n",
    "\n",
    "    def getitem(self, index: int):\n",
    "        path, family, genus, species, collection, catalog_number = self.samples[index]\n",
    "        img = Image.open(path)\n",
    "        return img, family, path\n",
    "\n",
    "#     @property\n",
    "#     def transform(self) -> Callable:#, img: PIL.Image):\n",
    "#         _transforms = []\n",
    "#         if self.image_return_type == \"tensor\":\n",
    "#             _transforms.append(self.totensor)\n",
    "#         return lambda x: x\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        img, family, path = self.getitem(index)\n",
    "        target = self.class2idx[family]\n",
    "        \n",
    "        if self.image_return_type == \"tensor\":\n",
    "            img = self.totensor(img)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target, path\n",
    "    \n",
    "    \n",
    "    def parse_item(self, index: int):\n",
    "        path = self.files[index]\n",
    "        family, genus, species, collection, catalog_number = path.stem.split(\"_\", maxsplit=4)\n",
    "        return path, family, genus, species, collection, catalog_number\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.config.__repr__()\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "        dataset_dirs = available_datasets[name]\n",
    "        if isinstance(available_datasets[name], str):\n",
    "            dataset_dirs = [available_datasets[name]]\n",
    "        assert isinstance(dataset_dirs, list)\n",
    "        file_list = list(flatten(\n",
    "                            [torchdata.datasets.Files.from_folder(Path(root),\n",
    "                                                                  regex=\"*/*.jpg\").files\n",
    "                             for root in dataset_dirs]\n",
    "                                                    ))\n",
    "        data = FossilDataset(file_list,\n",
    "                             name=name)\n",
    "\n",
    "        return data #.map(lambda x: (torchvision.transforms.ToTensor()(x[0]), x[1]))\n",
    "    \n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "#         dataset_dirs = available_datasets[name]\n",
    "#         if isinstance(available_datasets[name], list):\n",
    "#             file_list = list(flatten(\n",
    "#                                 [torchdata.datasets.Files.from_folder(Path(root),\n",
    "#                                                         regex=\"*/*.jpg\").files\n",
    "#                                for root in available_datasets[name]]\n",
    "#                                                             ))\n",
    "            \n",
    "#         elif isinstance(available_datasets[name], str):\n",
    "#             file_list = torchdata.datasets.Files.from_folder(Path(available_datasets[name]),\n",
    "#                                                              regex=\"*/*.jpg\").files\n",
    "\n",
    "#         data = FossilDataset(file_list,\n",
    "#                                  name=name)\n",
    "\n",
    "#         return data.map(torchvision.transforms.ToTensor())\n",
    "\n",
    "fossil_data = FossilDataset.create_dataset(name=\"Fossil_1024\")\n",
    "fossil_data\n",
    "\n",
    "# fossil_data = FossilDataset.create_dataset(name=\"Florissant_Fossil_1024\")\n",
    "# fossil_data\n",
    "\n",
    "# fossil_data = FossilDataset.create_dataset(name=\"Wilf_Fossil_1024\")\n",
    "# fossil_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9286b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Future todo: Separate subclass of simpler Leaves/Fossil Dataset class to allow for more customization of return signatures (allowing dict records instead of tuple, multiple labels per sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelFossilDataset(FossilDataset): #ImageDataset):\n",
    "    \n",
    "    loader: Callable = Image.open\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    target_transform = None\n",
    "    \n",
    "    class_type: str=\"family\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 files: List[Path],\n",
    "                 return_items: List[str,str] = [\"image\",\"target\",\"path\"],\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(files=files, *args, **kwargs)\n",
    "        \n",
    "        self.name = kwargs.get(\"name\",\"\")\n",
    "        self.return_items = return_items\n",
    "        \n",
    "        self.samples = [self.parse_item(idx) for idx in range((len(self)))]            \n",
    "        self.targets = [sample[1] for sample in self.samples]\n",
    "        self.classes = sorted(set(self.targets))\n",
    "        self.class2idx = {name:idx for idx, name in enumerate(self.classes)}\n",
    "        \n",
    "        self.config = DatasetConfig(self.name,\n",
    "                                    class_type=self.class_type,\n",
    "                                    num_files=len(self.files),\n",
    "                                    num_classes=len(self.classes)\n",
    "                                    )\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def get_item(self, index: int):\n",
    "        item = self.samples[index]\n",
    "        img = self.loader(item[\"path\"])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return self.get_item((img, target, path))\n",
    "    \n",
    "        return Image.open(item[0]), self.class2idx[item[1]]\n",
    "\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.samples[index]\n",
    "        \n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return self.get_item((img, target, path))\n",
    "#         return Image.open(item[0]), self.class2idx[item[1]]\n",
    "    \n",
    "    def parse_item(self, index: int):\n",
    "        path = self.files[index]\n",
    "        family, genus, species, collection, catalog_number = path.stem.split(\"_\", maxsplit=4)\n",
    "        item = {\"path\":path,\n",
    "                \"target\":None,\n",
    "                \"family\":family,\n",
    "                \"genus\":genus,\n",
    "                \"species\":species,\n",
    "                \"collection\":collection,\n",
    "                \"catalog_number\":catalog_number}\n",
    "        item[\"target\"] = item[self.class_type]\n",
    "#         return path, family, genus, species, collection, catalog_number\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.config.__repr__()\n",
    "        \n",
    "#     @property\n",
    "#     def config(self):\n",
    "#         return DatasetConfig(self.name,\n",
    "#                              num_files=len(self.files),\n",
    "#                              num_classes=len(self.classes)\n",
    "#                             )\n",
    "    @classmethod\n",
    "    def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "        dataset_dirs = available_datasets[name]\n",
    "        if isinstance(available_datasets[name], str):\n",
    "            dataset_dirs = [available_datasets[name]]\n",
    "        assert isinstance(dataset_dirs, list)\n",
    "        file_list = list(flatten(\n",
    "                            [torchdata.datasets.Files.from_folder(Path(root),\n",
    "                                                                  regex=\"*/*.jpg\").files\n",
    "                             for root in dataset_dirs]\n",
    "                                                    ))\n",
    "        data = FossilDataset(file_list,\n",
    "                             name=name)\n",
    "\n",
    "        return data.map(lambda x: (torchvision.transforms.ToTensor()(x[0]), x[1]))\n",
    "    \n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "#         dataset_dirs = available_datasets[name]\n",
    "#         if isinstance(available_datasets[name], list):\n",
    "#             file_list = list(flatten(\n",
    "#                                 [torchdata.datasets.Files.from_folder(Path(root),\n",
    "#                                                         regex=\"*/*.jpg\").files\n",
    "#                                for root in available_datasets[name]]\n",
    "#                                                             ))\n",
    "            \n",
    "#         elif isinstance(available_datasets[name], str):\n",
    "#             file_list = torchdata.datasets.Files.from_folder(Path(available_datasets[name]),\n",
    "#                                                              regex=\"*/*.jpg\").files\n",
    "\n",
    "#         data = FossilDataset(file_list,\n",
    "#                                  name=name)\n",
    "\n",
    "#         return data.map(torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23631b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(fossil_data.samples[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fossil_data.class2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f5f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f5558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4711c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ImageDataset(torchdata.datasets.Files):\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return Image.open(self.files[index])\n",
    "    \n",
    "# #     def __repr__(self):\n",
    "# #         return f\"\"\"{self.kwargs['name']}\"\"\"\n",
    "# #         return f\"\"\"ImageDataset: {self.kwargs['name']}\"\"\"\n",
    "\n",
    "#     def __init__(self, files: List[Path], *args, **kwargs):\n",
    "#         super().__init__(files=files, *args, **kwargs)\n",
    "        \n",
    "# #         self.name = kwargs.get(\"name\",\"\")\n",
    "# #         self.cfg = DatasetConfig(self.name)\n",
    "\n",
    "\n",
    "#     @classmethod\n",
    "#     def create_dataset(cls, name: str) -> \"ImageDataset\":\n",
    "#         dataset_dirs = available_datasets[name]\n",
    "\n",
    "#         if isinstance(available_datasets[name], str):\n",
    "#             data = ImageDataset.from_folder(Path(available_datasets[name]),\n",
    "#                                             regex=\"*/*.jpg\",\n",
    "#                                             name=name)\n",
    "#         return data.map(torchvision.transforms.ToTensor())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce580412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013691a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae843bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf76db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f0292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d67b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fd227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import os\n",
    "import types\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import torch\n",
    "import timm\n",
    "from rich import print\n",
    "import matplotlib.pyplot as plt\n",
    "from contrastive_learning.data.pytorch.pnas import PNASLightningDataModule\n",
    "from contrastive_learning.data.pytorch.extant import ExtantLightningDataModule\n",
    "from contrastive_learning.data.pytorch.common import DataStageError, LeavesLightningDataModule\n",
    "\n",
    "from lightning_hydra_classifiers.callbacks.wandb_callbacks import WatchModelWithWandb, LogPerClassMetricsToWandb, WandbClassificationCallback # LogConfusionMatrixToWandb\n",
    "from lightning_hydra_classifiers.models.resnet import ResNet, get_scalar_metrics\n",
    "import lightning_hydra_classifiers\n",
    "from torch import nn\n",
    "import inspect\n",
    "\n",
    "import wandb\n",
    "pl.trainer.seed_everything(seed=9)\n",
    "\n",
    "    \n",
    "# class Config:\n",
    "#     pass\n",
    "\n",
    "\n",
    "# config = Config()\n",
    "\n",
    "# # config.model_name = 'resnet50'\n",
    "# # config.dataset_name = 'PNAS_family_100_512'\n",
    "# config.dataset_name = '(Extant-PNAS)_family_10_512'\n",
    "# config.normalize = True\n",
    "# config.num_workers = 4\n",
    "# config.batch_size = 16\n",
    "\n",
    "# config = Box({\n",
    "#     \"dataset\":{\n",
    "#         namef\"PNAS_{label_type}_{pnas_threshold}\"\n",
    "#     }\n",
    "    \n",
    "# })\n",
    "\n",
    "########################################\n",
    "# if 'Extant' in config.dataset_name:\n",
    "#     datamodule = ExtantLightningDataModule(name=config.dataset_name, batch_size=config.batch_size, debug=False, normalize=config.normalize, num_workers=config.num_workers)\n",
    "# elif 'PNAS' in config.dataset_name:\n",
    "#     datamodule = PNASLightningDataModule(name=config.dataset_name, batch_size=config.batch_size, debug=False, normalize=config.normalize, num_workers=config.num_workers)#, normalize=False)#True)\n",
    "# datamodule.setup('fit')\n",
    "# ########################################\n",
    "# num_classes = len(datamodule.classes)\n",
    "# config.num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092819c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from box import Box\n",
    "import os\n",
    "\n",
    "os.environ['WANDB_CACHE_DIR'] = \"/media/data/jacob/wandb_cache\"\n",
    "class_type = \"family\"\n",
    "extant_threshold = 10\n",
    "pnas_threshold = 100\n",
    "image_size = 512\n",
    "seed = 257\n",
    "\n",
    "config = Box({})\n",
    "\n",
    "config.datasets = [{\n",
    "                  \"name\": f\"PNAS_{class_type}_{pnas_threshold}_{image_size}\",\n",
    "                  \"batch_size\":32,\n",
    "                  \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "                  \"num_workers\":4,\n",
    "                  \"image_size\":image_size,\n",
    "                  \"channels\":3,\n",
    "                  \"class_type\":class_type,\n",
    "                  \"debug\":False,\n",
    "                  \"normalize\":True,\n",
    "                  \"seed\":seed,\n",
    "                  \"dataset_dir\":None,\n",
    "                  \"predict_on_split\":\"val\",\n",
    "                  },\n",
    "    {\n",
    "                  \"name\":f\"Extant_{class_type}_{extant_threshold}_{image_size}\",  # f\"PNAS_{label_type}_{pnas_threshold}_{image_size}\"\n",
    "                  \"batch_size\":32,\n",
    "                  \"val_split\":None, # TODO specify split explicitly in wandb report\n",
    "                  \"num_workers\":4,\n",
    "                  \"image_size\":image_size,\n",
    "                  \"channels\":3,\n",
    "                  \"class_type\":class_type,\n",
    "                  \"debug\":False,\n",
    "                  \"normalize\":True,\n",
    "                  \"seed\":seed,\n",
    "                  \"dataset_dir\":None,\n",
    "                  \"predict_on_split\":\"val\",\n",
    "                  }]\n",
    "\n",
    "\n",
    "\n",
    "config.wandb = {\n",
    "                \"init\":\n",
    "                       {\n",
    "                        \"entity\":\"jrose\",\n",
    "                        \"project\":\"image_classification_datasets\",\n",
    "                        \"job_type\":'create-dataset',\n",
    "                        \"group\":None,\n",
    "                        \"run_dir\":os.environ['WANDB_CACHE_DIR'],\n",
    "                        \"tags\":[d.name for d in config.datasets]\n",
    "                       },\n",
    "                \"artifacts\":\n",
    "                        {\n",
    "                        \"root_dir\":None\n",
    "                        },\n",
    "                \"input_artifacts\":\n",
    "                       [\n",
    "                           {\n",
    "                            \"entity\":\"jrose\",\n",
    "                            \"project\":\"image_classification_datasets\",\n",
    "                            \"name\": config.datasets[0].name,\n",
    "                            \"version\": \"v6\",\n",
    "                            \"type\": \"raw_data\",\n",
    "                            \"root_dir\":None,\n",
    "                            \"uri\":None\n",
    "                           }\n",
    "                       ]\n",
    "}\n",
    "\n",
    "i = 0\n",
    "\n",
    "config.wandb.artifacts.root_dir = os.path.join(config.wandb.init.run_dir,\n",
    "                                               \"artifacts\")\n",
    "\n",
    "config.wandb.input_artifacts[i].uri = \"/\".join([config.wandb.input_artifacts[i].entity,\n",
    "                                                config.wandb.input_artifacts[i].project,\n",
    "                                                config.wandb.input_artifacts[i].name]) \\\n",
    "                                           + f':{config.wandb.input_artifacts[i].version}'\n",
    "\n",
    "\n",
    "config.wandb.input_artifacts[i].root_dir = os.path.join(config.wandb.artifacts.root_dir,\n",
    "                                                        \"datasets\",\n",
    "                                                         config.wandb.input_artifacts[i].name \\\n",
    "                                                         + f':{config.wandb.input_artifacts[i].version}'\n",
    "                                                        )\n",
    "\n",
    "\n",
    "# def fetch_datamodule_from_dataset_artifact(config: Box, run_or_api=None) -> LeavesLightningDataModule:\n",
    "#     run = run_or_api or wandb.Api()\n",
    "#     artifact = run.use_artifact(config.wandb.input_artifact.uri,\n",
    "#                                 type=config.wandb.input_artifact.type)\n",
    "#     dataset_artifact_dir = artifact.download(root=config.wandb.input_artifact.root_dir)\n",
    "\n",
    "\n",
    "#     datamodule = get_datamodule(config.dataset)\n",
    "#     datamodule.setup('fit')\n",
    "#     datamodule.setup('test')\n",
    "#     ########################\n",
    "#     config.model.num_classes = config.dataset.num_classes\n",
    "\n",
    "def fetch_datamodule_from_dataset_artifact(config: Box, run_or_api=None) -> LeavesLightningDataModule:\n",
    "    run = run_or_api or wandb.Api()\n",
    "    artifact = run.use_artifact(config.wandb.input_artifact.uri,\n",
    "                                type=config.wandb.input_artifact.type)\n",
    "    dataset_artifact_dir = artifact.download(root=config.wandb.input_artifact.root_dir)\n",
    "\n",
    "\n",
    "    datamodule = get_datamodule(config.dataset)\n",
    "    datamodule.setup('fit')\n",
    "    datamodule.setup('test')\n",
    "    ########################\n",
    "    config.model.num_classes = config.dataset.num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageStat\n",
    "import seaborn_image as isns\n",
    "import scipy\n",
    "\n",
    "def image_stat(img: np.ndarray):\n",
    "    if img.ndim==3:\n",
    "        h, w, c = img.shape\n",
    "    else:\n",
    "        h, w, c = (*img.shape, 1)\n",
    "    return {\n",
    "        \"min\":np.min(img),\n",
    "        \"max\":np.max(img),\n",
    "        \"var\":np.var(img),\n",
    "        \"mean\":np.mean(img),\n",
    "        \"mode\":scipy.stats.mode(img,axis=None),\n",
    "        \"height\":h,\n",
    "        \"width\":w,\n",
    "        \"channels\":c,\n",
    "        \"num_pixels\":h*w*c\n",
    "    }\n",
    "\n",
    "\n",
    "def load_and_analyze_image(image_path: str):\n",
    "    img = np.array(Image.open(image_path))\n",
    "    return img, image_stat(img)\n",
    "\n",
    "# def load_analyze_and_save_annotated_image(image_path: str):\n",
    "#     img, stats = load_and_analyze_image(image_path)\n",
    "#     return img, image_stat(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fig2data ( fig ):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a numpy 3D array of RGBA values\n",
    "    \"\"\"\n",
    "    # draw the renderer\n",
    "    fig.canvas.draw()\n",
    " \n",
    "    # Get the RGBA buffer from the figure\n",
    "    w,h = fig.canvas.get_width_height()\n",
    "    buf = np.fromstring(fig.canvas.tostring_argb(), dtype=np.uint8)\n",
    "    buf.shape = ( w, h, 4 )\n",
    " \n",
    "    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n",
    "    buf = np.roll( buf, 3, axis = 2 )\n",
    "    return buf\n",
    "\n",
    " \n",
    "def fig2img ( fig ):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a PIL Image in RGBA format and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a Python Imaging Library ( PIL ) image\n",
    "    \"\"\"\n",
    "    # put the figure pixmap into a numpy array\n",
    "    buf = fig2data(fig)\n",
    "    w, h, d = buf.shape\n",
    "    return Image.frombytes(\"RGBA\", (w, h), buf.tostring())\n",
    "\n",
    "# f = isns.imghist(img_array,\n",
    "#                  describe=True)\n",
    "# results = fig2img(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e03224",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.wandb.input_artifacts[0].uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6660d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def trainvaltest_split(x: Union[List[Any],np.ndarray]=None,\n",
    "#                        y: Union[List[Any],np.ndarray]=None,\n",
    "#                        splits: List[float]=(0.5, 0.2, 0.3),\n",
    "#                        random_state: int=None,\n",
    "#                        stratify: bool=True\n",
    "#                        ) -> Dict[str,Tuple[np.ndarray]]:\n",
    "#     \"\"\"\n",
    "#     Wrapper function to split data into 3 stratified subsets specified by `splits`.\n",
    "    \n",
    "#     User specifies absolute fraction of total requested for each subset (e.g. splits=[0.5, 0.2, 0.3])\n",
    "    \n",
    "#     Function calculates adjusted fractions necessary in order to use sklearn's builtin train_test_split function over a sequence of 2 steps.\n",
    "    \n",
    "#     Step 1: Separate test set from the rest of the data (constituting the union of train + val)\n",
    "    \n",
    "#     Step 2: Separate the train and val sets from the remainder produced by step 1.\n",
    "    \n",
    "    \n",
    "    \n",
    "#     Output:\n",
    "#         Dict: {'train':(x_train, y_train),\n",
    "#                 'val':(x_val_y_val),\n",
    "#                 'test':(x_test, y_test)}\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     assert len(splits) == 3, \"Must provide eactly 3 float values for `splits`\"\n",
    "#     assert np.isclose(np.sum(splits), 1.0), f\"Sum of all splits values {splits} = {np.sum(splits)} must be 1.0\"\n",
    "    \n",
    "#     train_split, val_split, test_split = splits\n",
    "#     val_relative_split = val_split/(train_split + val_split)\n",
    "#     train_relative_split = train_split/(train_split + val_split)\n",
    "    \n",
    "    \n",
    "#     if stratify and (y is None):\n",
    "#         raise ValueError(\"If y is not provided, stratify must be set to False.\")\n",
    "    \n",
    "#     y = np.array(y)\n",
    "#     if x is None:\n",
    "#         x = np.arange(len(y))\n",
    "#     else:\n",
    "#         x = np.array(x)\n",
    "    \n",
    "#     stratify_y = y if stratify else None    \n",
    "#     x_train_val, x_test, y_train_val, y_test = train_test_split(x, y,\n",
    "#                                                         test_size=test_split, \n",
    "#                                                         random_state=random_state,\n",
    "#                                                         stratify=y)\n",
    "#     log.info(f\"(x_train+x_val).shape={x_train_val.shape}, (y_train+y_val).shape={y_train_val.shape}\")\n",
    "#     log.info(f\"x_test.shape={x_test.shape}, y_test.shape={y_test.shape}\")\n",
    "    \n",
    "# #     print(f\"(x_train+x_val).shape={x_train_val.shape}, (y_train+y_val).shape={y_train_val.shape}\")\n",
    "# #     print(f\"x_test.shape={x_test.shape}, y_test.shape={y_test.shape}\")\n",
    "\n",
    "#     stratify_y_train = y_train_val if stratify else None\n",
    "#     x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val,\n",
    "#                                                       test_size=val_relative_split,\n",
    "#                                                       random_state=random_state, \n",
    "#                                                       stratify=y_train_val)\n",
    "    \n",
    "#     x = np.concatenate((x_train, x_val, x_test)).tolist()\n",
    "#     assert len(set(x)) == len(x), f\"[Warning] Check for possible data leakage. len(set(x))={len(set(x))} != len(x)={len(x)}\"\n",
    "    \n",
    "#     log.info(f\"x_train.shape={x_train.shape}, y_train.shape={y_train.shape}\")\n",
    "#     log.info(f\"x_val.shape={x_val.shape}, y_val.shape={y_val.shape}\")\n",
    "    \n",
    "#     log.info(f'Absolute splits: {[train_split, val_split, test_split]}')\n",
    "#     log.info(f'Relative splits: [{train_relative_split:.2f}, {val_relative_split:.2f}, {test_split}]')\n",
    "    \n",
    "#     return {\"train\":(x_train, y_train),\n",
    "#             \"val\":(x_val, y_val),\n",
    "#             \"test\":(x_test, y_test)}\n",
    "\n",
    "#####################\n",
    "\n",
    "# y = data.targets\n",
    "\n",
    "# data_splits = trainvaltest_split(x=None,\n",
    "#                                  y=y,\n",
    "#                                  splits=(0.5, 0.2, 0.3),\n",
    "#                                  random_state=0,\n",
    "#                                  stratify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964bd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(config.wandb.input_artifacts[0].uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4df0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60211211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(artifact)\n",
    "# dir(artifact.manifest)\n",
    "# artifact.manifest.entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83557cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = artifact.get('dataset/test.table.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.data\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame(data=df, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d47e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_samples = data_df\n",
    "samples = list(data_df[['image', 'label']].itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d96b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = len(set(wide_samples.label.values))\n",
    "plt.bar(range(num_classes), wide_samples.groupby(\"label\")[\"catalog_number\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915da655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc94b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_mem = data_df.image.apply(lambda x: np.array(x._image))\n",
    "in_mem = data_df.image.apply(lambda x: x._image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33760cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_mem[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbefd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(data_df.image[0]._image).shape\n",
    "\n",
    "\n",
    "in_mem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11022e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "df = data.get_column('image')\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51aa1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ed908",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(artifact)\n",
    "\n",
    "downloaded_artifact = artifact.checkout(root=config.wandb.input_artifacts[0].root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.abspath\n",
    "(downloaded_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af28a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contrastive_learning.data.pytorch.pnas import PNASLeavesDataset\n",
    "from contrastive_learning.data.pytorch.extant import ExtantLeavesDataset\n",
    "# from contrastive_learning.data.pytorch.common import DataStageError\n",
    "from paleoai_data.dataset_drivers import base_dataset\n",
    "\n",
    "# Step 1. Instantiate PyTorch Datasets for each of Extant Leaves & PNAS, separately\n",
    "pnas_torch = PNASLeavesDataset(name = f\"PNAS_{label_type}_{pnas_threshold}\",\n",
    "                 split: str=\"train\",\n",
    "                 dataset_dir: Optional[str]=None,\n",
    "                 return_paths: bool=False,)\n",
    "extant_torch = ExtantLeavesDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def create_dataset_by_name(name: str,\n",
    "#                            version: str='v0.2',\n",
    "#                            exclude_classes = ['notcataloged','notcatalogued', 'II. IDs, families uncertain', 'Unidentified']):\n",
    "#     data_df = query_db(version=version, **{'dataset':name})\n",
    "#     dataset = base_dataset.BaseDataset.from_dataframe(df=data_df, name=name, exclude_classes=exclude_classes)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f25291",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=WANDB_PROJECT, job_type=\"model_result_analysis\") as run:\n",
    "    \n",
    "    # Retrieve the original raw dataset\n",
    "    dataset_artifact = run.use_artifact(\"raw_data:latest\")\n",
    "    data_table = dataset_artifact.get(\"raw_examples\")\n",
    "    \n",
    "    # Retrieve the train and test score tables\n",
    "    train_artifact = run.use_artifact(\"train_results:latest\")\n",
    "    train_table = train_artifact.get(\"train_iou_score_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581faec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = PNASLightningDataModule(batch_size=16)\n",
    "# data = ExtantLightningDataModule(batch_size=16, num_workers=12)\n",
    "# data.setup(stage='fit')\n",
    "\n",
    "# data.setup(stage='test')\n",
    "\n",
    "# data.setup(stage=None)\n",
    "\n",
    "# try:\n",
    "#     data.setup(stage='other')\n",
    "#     print('success')\n",
    "# except DataStageError as e:\n",
    "#     print(e.with_traceback(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888173b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.setup(stage='fit')\n",
    "train_dataloader = data.get_dataloader(stage='train')\n",
    "val_dataloader = data.get_dataloader(stage='val')\n",
    "data.setup(stage='test')\n",
    "test_dataloader = data.get_dataloader(stage='test')\n",
    "\n",
    "# train_dataloader\n",
    "#         if stage=='train': return self.train_dataloader()\n",
    "#         if stage=='val': return self.val_dataloader()\n",
    "#         if stage=='test': return self.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data.train_dataset.transform = None #data.default_train_transforms() #None\n",
    "x, y = data.train_dataset[0]\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522093f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import ImageOps\n",
    "\n",
    "# print(x.max(), x.min())\n",
    "# plt.imshow(ImageOps.invert(x))#.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee464fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_idx = 0\n",
    "\n",
    "data.show_batch('train', batch_idx=batch_idx)\n",
    "# data.show_batch('train', cmap='plasma')\n",
    "plt.savefig(f'ExtantLeaves v0_3 train batch {batch_idx}.png')\n",
    "\n",
    "data.show_batch('val', batch_idx=batch_idx)\n",
    "plt.savefig(f'ExtantLeaves v0_3 val batch {batch_idx}.png')\n",
    "\n",
    "data.show_batch('test', batch_idx=batch_idx)\n",
    "plt.savefig(f'ExtantLeaves v0_3 test batch {batch_idx}.png')\n",
    "# data.show_batch('train', cmap='magma')\n",
    "# data.show_batch('train', cmap='cividis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bdf174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc10d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc86fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = data\n",
    "stage = 'test'\n",
    "batch_idx = 0\n",
    "\n",
    "x, y = self.get_batch(stage=stage, batch_idx=batch_idx)\n",
    "\n",
    "x = x[:12,...]\n",
    "\n",
    "batch_size = x.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(24,24))\n",
    "grid_img = torchvision.utils.make_grid(x, nrow=int(np.ceil(np.sqrt(batch_size))))\n",
    "\n",
    "img_min, img_max = grid_img.min(), grid_img.max()\n",
    "print(img_min, img_max)\n",
    "\n",
    "grid_img = (grid_img - img_min)/(img_max - img_min)\n",
    "img_min, img_max = grid_img.min(), grid_img.max()\n",
    "print(img_min, img_max)\n",
    "\n",
    "\n",
    "\n",
    "print('before:', grid_img.shape)\n",
    "\n",
    "if torch.argmin(torch.Tensor(grid_img.shape)) == 0:\n",
    "    grid_img = grid_img.permute(1,2,0)\n",
    "print('after:', grid_img.shape)\n",
    "\n",
    "img_ax = ax.imshow(grid_img[:,:,0], cmap='viridis')#, vmin = img_min, vmax = img_max)\n",
    "fig.colorbar(img_ax, ax=ax)#)#cax=ax)\n",
    "plt.axis('off')\n",
    "plt.suptitle(f'{stage} batch')\n",
    "#         return fig, ax\n",
    "\n",
    "help(plt.imshow)\n",
    "\n",
    "%debug\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "\n",
    "x.min()\n",
    "\n",
    "plt.imshow(x[1,...].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132f169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c0686ae52b501c5138d1ad3c292b1aad199ba0a61e288abba1616901d57bf21"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('sequoia': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd01c0686ae52b501c5138d1ad3c292b1aad199ba0a61e288abba1616901d57bf21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
