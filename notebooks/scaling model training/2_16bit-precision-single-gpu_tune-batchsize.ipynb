{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3df382",
   "metadata": {},
   "source": [
    "## Notebook 2.\n",
    "\n",
    "### 16bit precision on a single gpu -- include a batch_size tuning step before training then testing\n",
    "\n",
    "Created by: Jacob A Rose  \n",
    "Created on: Monday July 5th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b440f",
   "metadata": {},
   "source": [
    "### Scaling model training series\n",
    "\n",
    "A collection of notebooks meant to demonstrate minimal-complexity examples for:\n",
    "* Integrating new training methods for scaling up experiments to large numbers in parallel &\n",
    "* Making maximum use of hardware resources\n",
    "\n",
    "1. 16bit precision, single gpu, train -> test\n",
    "2. 16bit precision, single gpu, batch_size tune -> train -> test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389597c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1c2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from pytorch_lightning.metrics.classification import Accuracy\n",
    "# from lightning_hydra_classifiers.models.modules.simple_dense_net import SimpleDenseNet\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Replace default file cloud urls from Yann Lecun's website to offiial aws s3 bucket\n",
    "# new_mirror = 'https://ossci-datasets.s3.amazonaws.com/mnist'\n",
    "# MNIST.resources = [\n",
    "#                    ('/'.join([new_mirror, url.split('/')[-1]]), md5)\n",
    "#                    for url, md5 in MNIST.resources\n",
    "#                    ]\n",
    "\n",
    "if 'TOY_DATA_DIR' not in os.environ: \n",
    "    os.environ['TOY_DATA_DIR'] = \"/media/data_cifs/projects/prj_fossils/data/toy_data\"\n",
    "        \n",
    "default_root_dir = os.environ['TOY_DATA_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/experiment/2_16bit-precision-single-gpu_tune-batchsize.yaml\"\n",
    "    config = read_hydra_config(config_dir = str(Path(config_path).parent),\n",
    "                               job_name=\"test_app\",\n",
    "                               config_name=Path(config_path).stem)\n",
    "    \n",
    "    template_utils.extras(config)\n",
    "    \n",
    "    if \"seed\" in config:\n",
    "        pl.seed_everything(config.seed)\n",
    "    \n",
    "    datamodule, config = configure_datamodule(config)\n",
    "    \n",
    "    model = configure_model(config)\n",
    "    \n",
    "#     model = CoolSystem()\n",
    "\n",
    "    trainer = configure_trainer(config)\n",
    "    \n",
    "#     trainer = pl.Trainer(gpus=1, \n",
    "#                          precision=16,\n",
    "#                          progress_bar_refresh_rate=5,\n",
    "#                          max_epochs=10)\n",
    "    \n",
    "    \n",
    "    bsz_tuner = trainer.tune(model, datamodule=datamodule)\n",
    "#     best_bsz = model.hparams.batch_size\n",
    "    best_bsz = model.batch_size\n",
    "    \n",
    "    print(f'[START] Training with tuned batch_size = {best_bsz}')\n",
    "    \n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    \n",
    "    test_results = trainer.test(datamodule=datamodule)\n",
    "    \n",
    "    return test_results, best_bsz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece21fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function definitions\n",
    "\n",
    "1. Configure logger (using python's logging module)\n",
    "2. Configure experiment Config (using hydra + omegaconf.DictConfig)\n",
    "3. Configure datamodule (using custom LightningDataModule)\n",
    "4. Configure model (using custom LightningModule)\n",
    "5. Configure trainer (using pl.Trainer, as well as pytorch lightning loggers & callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bca571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from lightning_hydra_classifiers.utils import template_utils\n",
    "import hydra\n",
    "from hydra.experimental import compose, initialize_config_dir\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_standard_python_logger(name: str='notebook'):\n",
    "    \"\"\"\n",
    "    Set up the standard python logging module for command line debugging\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "        level=logging.INFO,\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        stream=sys.stdout,\n",
    "    )\n",
    "    log = logging.getLogger(name)\n",
    "    \n",
    "    return log\n",
    "\n",
    "log = get_standard_python_logger(name='notebook')\n",
    "\n",
    "\n",
    "def read_hydra_config(config_dir: str,\n",
    "                      job_name: str=\"test_app\",\n",
    "                      config_name: str=\"experiment\") -> DictConfig:\n",
    "    \"\"\"\n",
    "    Read a yaml config file from disk using hydra and return as a DictConfig.\n",
    "    \"\"\"\n",
    "    os.chdir(config_dir)\n",
    "    with initialize_config_dir(config_dir=config_dir, job_name=job_name):\n",
    "        cfg = compose(config_name=config_name)\n",
    "        \n",
    "    if cfg.get(\"print_config\"):\n",
    "        template_utils.print_config(cfg, resolve=True)        \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ceae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_datamodule(config: DictConfig) -> pl.LightningDataModule:\n",
    "    log.info(f\"Instantiating datamodule <{config.datamodule._target_}>\")\n",
    "    datamodule: pl.LightningDataModule = hydra.utils.instantiate(config.datamodule)\n",
    "        \n",
    "    try:\n",
    "        datamodule.setup(stage=\"fit\")\n",
    "        config.datamodule.classes = datamodule.classes\n",
    "        config.datamodule.num_classes = len(config.datamodule.classes)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "        \n",
    "    return datamodule, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_model(config: DictConfig) -> pl.LightningModule:\n",
    "    log.info(f\"Instantiating model <{config.model._target_}>\")\n",
    "    model: pl.LightningModule = hydra.utils.instantiate(config.model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf196f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# from pytorch_lightning import LightningModule, LightningDataModule, Callback, Trainer\n",
    "# from pytorch_lightning.loggers import LightningLoggerBase\n",
    "# from pytorch_lightning import seed_everything\n",
    "\n",
    "def configure_trainer(config: DictConfig) -> pl.Trainer:\n",
    "\n",
    "    # Init Lightning callbacks\n",
    "    callbacks: List[pl.Callback] = []\n",
    "    if \"callbacks\" in config:\n",
    "        for cb_name, cb_conf in config[\"callbacks\"].items():\n",
    "            if \"_target_\" in cb_conf:\n",
    "                log.info(f\"Instantiating callback <{cb_conf._target_}>\")\n",
    "                if cb_name == \"wandb\":\n",
    "                    callbacks.append(hydra.utils.instantiate(cb_conf, config=OmegaConf.to_container(config, resolve=True)))\n",
    "                else:\n",
    "                    callbacks.append(hydra.utils.instantiate(cb_conf))\n",
    "\n",
    "    # Init Lightning loggers\n",
    "    logger: List[pl.loggers.LightningLoggerBase] = []\n",
    "    if \"logger\" in config:\n",
    "        for _, lg_conf in config[\"logger\"].items():\n",
    "            if \"_target_\" in lg_conf:\n",
    "                log.info(f\"Instantiating logger <{lg_conf._target_}>\")\n",
    "                logger.append(hydra.utils.instantiate(lg_conf))\n",
    "\n",
    "\n",
    "    log.info(f\"Instantiating trainer <{config.trainer._target_}>\")\n",
    "    trainer: pl.Trainer = hydra.utils.instantiate(config.trainer,\n",
    "                                                  callbacks=callbacks,\n",
    "                                                  logger=logger,\n",
    "                                                  _convert_=\"partial\")\n",
    "        \n",
    "    return trainer\n",
    "\n",
    "# trainer = configure_trainer(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a56c7f",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc3efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --port 0 --logdir lightning_logs/\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde172a",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892350bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CoolSystem(pl.LightningModule):\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  classes: int=10,\n",
    "#                  batch_size: int=32):\n",
    "#         super().__init__()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.save_hyperparameters()\n",
    "\n",
    "#         self.l1 = torch.nn.Linear(28 * 28, self.hparams.classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_hat = self(x)\n",
    "#         loss = F.cross_entropy(y_hat, y)\n",
    "#         tensorboard_logs = {'train_loss': loss}\n",
    "#         return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         MNIST(default_root_dir, train=True, download=True, transform=transforms.ToTensor())\n",
    "#         MNIST(default_root_dir, train=False, download=True,  transform=transforms.ToTensor())\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return self.trainer.datamodule.train_dataloader()\n",
    "#         mnist_train = MNIST(default_root_dir, train=True, download=False, transform=transforms.ToTensor())\n",
    "#         loader = DataLoader(mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "#         return loader\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return self.trainer.datamodule.test_dataloader()\n",
    "#         mnist_test = MNIST(default_root_dir, train=False, download=False, transform=transforms.ToTensor())\n",
    "#         loader = DataLoader(mnist_test, batch_size=self.batch_size, num_workers=4)\n",
    "#         return loader\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich import print as pp\n",
    "# import yaml\n",
    "# def read_config(path:  str) -> dict:\n",
    "#     try:\n",
    "#         with open (path, 'r') as file:\n",
    "#             config = yaml.safe_load(file)\n",
    "#         return config\n",
    "#     except Exception as e:\n",
    "#         print('Error reading the config file')\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f40233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/experiment/2_16bit-precision-single-gpu_tune-batchsize.yaml\"\n",
    "# config = read_hydra_config(config_dir = str(Path(config_path).parent),\n",
    "#                            job_name=\"test_app\",\n",
    "#                            config_name=Path(config_path).stem)\n",
    "\n",
    "# template_utils.extras(config)\n",
    "\n",
    "# if \"seed\" in config:\n",
    "#     pl.seed_everything(config.seed)\n",
    "\n",
    "# datamodule, config = configure_datamodule(config)\n",
    "\n",
    "\n",
    "# dir(datamodule)\n",
    "\n",
    "# datamodule.batch_size=16\n",
    "\n",
    "# datamodule.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead91e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/experiment/2_16bit-precision-single-gpu_tune-batchsize.yaml\"\n",
    "# config = read_hydra_config(config_dir = str(Path(config_path).parent),\n",
    "#                            job_name=\"test_app\",\n",
    "#                            config_name=Path(config_path).stem)\n",
    "\n",
    "# template_utils.extras(config)\n",
    "# # OmegaConf.set_struct(config, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
