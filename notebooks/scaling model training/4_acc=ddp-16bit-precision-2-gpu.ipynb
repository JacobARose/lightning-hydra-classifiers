{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad51cc12",
   "metadata": {},
   "source": [
    "## Notebook 4. (Must be run as python script)\n",
    "\n",
    "file: `4_acc=ddp-16bit-precision-2-gpu.ipynb`\n",
    "### ddp - 16bit-precision -- 2-gpu\n",
    "\n",
    "<!-- tune-batchsize -- tune_lr -- include a batch_size tuning step, then a learning_rate tuning step (both using only 'dp'), before training (using 'ddp') then testing -->\n",
    "\n",
    "Created by: Jacob A Rose  \n",
    "Created on: Wednesday July 7th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83573f9e",
   "metadata": {},
   "source": [
    "### Scaling model training series\n",
    "\n",
    "A collection of notebooks meant to demonstrate minimal-complexity examples for:\n",
    "* Integrating new training methods for scaling up experiments to large numbers in parallel &\n",
    "* Making maximum use of hardware resources\n",
    "\n",
    "1. 16bit precision, single gpu, train -> test\n",
    "2. 16bit precision, single gpu, batch_size tune -> train -> test\n",
    "3. 16bit precision, single gpu, batch_size tune -> lr tune -> train -> test\n",
    "4. ddp, 16bit precision, 2x gpus, batch_size tune -> lr tune -> train -> test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ff158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ffb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# available_datasets = {'1':1,\n",
    "#                      '2':2}\n",
    "\n",
    "# class TestClass:\n",
    "    \n",
    "# #     @property\n",
    "# #     def available_datasets(self):\n",
    "# #         return available_datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     @property\n",
    "#     def available_datasets(self):\n",
    "#         \"\"\"\n",
    "#         Subclasses must define this property\n",
    "#         Must return a dict mapping dataset key names to their absolute paths on disk.\n",
    "#         \"\"\"\n",
    "#         return available_datasets\n",
    "        \n",
    "#     @available_datasets.setter\n",
    "#     def available_datasets(self, new):\n",
    "#         \"\"\"\n",
    "#         Subclasses must define this property\n",
    "#         Must return a dict mapping dataset key names to their absolute paths on disk.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             available_datasets.update(new)\n",
    "#         except:\n",
    "#             raise Exception\n",
    "\n",
    "# obj = TestClass()\n",
    "\n",
    "# print(obj.available_datasets)\n",
    "\n",
    "# obj.available_datasets['3'] = 3\n",
    "\n",
    "# print(obj.available_datasets)\n",
    "\n",
    "# obj.available_datasets = {'4':4,\n",
    "#                           \"5\":5}\n",
    "\n",
    "# print(obj.available_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from pytorch_lightning.metrics.classification import Accuracy\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import rich\n",
    "# from rich import pretty\n",
    "# rich.pretty.install()\n",
    "\n",
    "if 'TOY_DATA_DIR' not in os.environ: \n",
    "    os.environ['TOY_DATA_DIR'] = \"/media/data_cifs/projects/prj_fossils/data/toy_data\"\n",
    "        \n",
    "default_root_dir = os.environ['TOY_DATA_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "def main(config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/experiment/3_16bit_precision-single_gpu--tune_batchsize--tune_lr.yaml\"):\n",
    "    \n",
    "    \n",
    "    config = read_hydra_config(config_dir = str(Path(config_path).parent),\n",
    "                               job_name=\"test_app\",\n",
    "                               config_name=Path(config_path).stem)\n",
    "    template_utils.extras(config)\n",
    "    \n",
    "    if \"seed\" in config:\n",
    "        pl.seed_everything(config.seed)\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "#     from IPython.core import debugger\n",
    "#     debugger.set_trace()\n",
    "    \n",
    "#############################################################################\n",
    "    if os.path.isfile(config.hparams_log_path):\n",
    "        config.hparams.update(OmegaConf.load(config.hparams_log_path))\n",
    "        if isinstance(config.hparams.batch_size, int):\n",
    "            config.tuner.scale_batch_size.tuned = True\n",
    "        if isinstance(config.hparams.lr, float):\n",
    "            config.tuner.lr_find.tuned = True\n",
    "        os.makedirs(config.log_dir, exist_ok=True)\n",
    "        shutil.copyfile(config.hparams_log_path, Path(config.log_dir, Path(config.hparams_log_path).name))\n",
    "    else:\n",
    "        os.makedirs(Path(config.hparams_log_path).parent, exist_ok=True)\n",
    "#############################################################################\n",
    "#     if not config.tuner.scale_batch_size.tuned:\n",
    "#         config.hparams.batch_size = config.tuner.scale_batch_size.kwargs.init_val\n",
    "        \n",
    "#     if not config.tuner.lr_find.tuned:\n",
    "#         config.hparams.lr = config.tuner.lr_find.kwargs.min_lr\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    for k,v in config.hparams.items():\n",
    "        print(k, v, type(v))\n",
    "\n",
    "#############################################################################\n",
    "#     if config.hparams.batch_size is None:\n",
    "    if not config.tuner.scale_batch_size.tuned:\n",
    "        log.info(f\"<----------Initiating auto scale_batch_size tuning----------->\")\n",
    "        log.info(f\"Tuning kwargs:\")\n",
    "        log.info(OmegaConf.to_yaml(config.tuner.scale_batch_size.kwargs))\n",
    "        # TODO: create hparams dataclass for checkpointing these tuned parameters\n",
    "        \n",
    "        config.hparams.batch_size = config.tuner.scale_batch_size.kwargs.init_val        \n",
    "        config.hparams.lr = config.tuner.lr_find.kwargs.min_lr\n",
    "        datamodule, config = configure_datamodule(config)    \n",
    "        template_utils.print_config(config, resolve=True)\n",
    "        model = configure_model(config)\n",
    "        tuner = configure_tuner(config)\n",
    "\n",
    "        \n",
    "        best_bsz = tuner.scale_batch_size(model,\n",
    "                                          datamodule=datamodule,\n",
    "                                          **config.tuner.scale_batch_size.kwargs)\n",
    "        config.hparams.batch_size = best_bsz\n",
    "        \n",
    "        log.info(\"<----------scale_batch_size-Results----------->\")\n",
    "        log.info(f'Best batch_size={best_bsz}')\n",
    "        \n",
    "        del datamodule, model, tuner        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "#     datamodule.batch_size = config.hparams.batch_size\n",
    "#     model.batch_size = config.hparams.batch_size\n",
    "    if not config.tuner.lr_find.tuned:\n",
    "        config.hparams.lr = \"default\"\n",
    "    OmegaConf.save(config.hparams, resolve=True, f=config.hparams_log_path)\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "    if not config.tuner.lr_find.tuned:\n",
    "        config.hparams.lr = config.tuner.lr_find.kwargs.min_lr\n",
    "        log.info(f\"<----------Initiating learning_rate tuning----------->\")\n",
    "        log.info(f\"Tuning kwargs:\")\n",
    "        log.info(OmegaConf.to_yaml(config.tuner.lr_find.kwargs))\n",
    "        \n",
    "#         config.hparams.lr = config.tuner.lr_find.kwargs.min_lr\n",
    "        datamodule, config = configure_datamodule(config)    \n",
    "        template_utils.print_config(config, resolve=True)\n",
    "        model = configure_model(config)\n",
    "        tuner = configure_tuner(config)\n",
    "\n",
    "        \n",
    "        lr_finder = tuner.lr_find(model=model,\n",
    "                                  datamodule=datamodule,\n",
    "                                  **config.tuner.lr_find.kwargs)\n",
    "\n",
    "        log.info(\"<----------lr_finder-Results----------->\")\n",
    "        log.info(f\"Best lr: {lr_finder.results}\")\n",
    "\n",
    "        fig = lr_finder.plot(suggest=True)\n",
    "        plt.savefig(Path(config.hparams_log_path).parent / \"lr_tuning_plot.png\")\n",
    "        config.hparams.lr = lr_finder.suggestion()\n",
    "   \n",
    "        del datamodule, model, tuner        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "#     model.lr = config.hparams.lr\n",
    "    OmegaConf.save(config.hparams, resolve=True, f=config.hparams_log_path)\n",
    "    \n",
    "    \n",
    "#############################################################################\n",
    "\n",
    "    datamodule, config = configure_datamodule(config)    \n",
    "    template_utils.print_config(config, resolve=True)\n",
    "    model = configure_model(config)\n",
    "        \n",
    "    trainer = configure_trainer(config)\n",
    "    print(f'[START] Training with tuned batch_size = {config.hparams.batch_size}')\n",
    "    print(f'and tuned learning rate = {config.hparams.lr}')\n",
    "    \n",
    "    trainer.fit(model, datamodule=datamodule)    \n",
    "    test_results = trainer.test(datamodule=datamodule)\n",
    "    \n",
    "    return test_results, config.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc4dde",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function definitions\n",
    "\n",
    "1. Configure logger (using python's logging module)\n",
    "2. Configure experiment Config (using hydra + omegaconf.DictConfig)\n",
    "3. Configure datamodule (using custom LightningDataModule)\n",
    "4. Configure model (using custom LightningModule)\n",
    "5. Configure trainer (using pl.Trainer, as well as pytorch lightning loggers & callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from lightning_hydra_classifiers.utils import template_utils\n",
    "import hydra\n",
    "from hydra.experimental import compose, initialize_config_dir\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from pathlib import Path\n",
    "from rich.logging import RichHandler\n",
    "\n",
    "def get_standard_python_logger(name: str='notebook',\n",
    "                               log_path: str=None):\n",
    "    \"\"\"\n",
    "    Set up the standard python logging module for command line debugging\n",
    "    \"\"\"\n",
    "    if log_path is None:\n",
    "        log_path = f\"./{name}.log\"\n",
    "    else:\n",
    "        os.makedirs(log_path, exist_ok=True)\n",
    "        log_path = str(Path(log_path, name)) + '.log'\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "        level=logging.INFO,\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "    log = logging.getLogger(name)\n",
    "    \n",
    "#     sys.stdout = open(log_path, 'a')\n",
    "    \n",
    "#     fh = logging.FileHandler(log_path)\n",
    "#     f = logging.Formatter('%(message)s')\n",
    "#     fh.setFormatter(f)\n",
    "#     fh.setLevel(logging.INFO)\n",
    "    rh = RichHandler(rich_tracebacks=True)\n",
    "    rh.setLevel(logging.INFO)\n",
    "    \n",
    "#     log.addHandler(fh)\n",
    "    log.addHandler(rh)\n",
    "    return log\n",
    "\n",
    "\n",
    "# log = get_standard_python_logger(name='notebook_experiment')\n",
    "\n",
    "\n",
    "def read_hydra_config(config_dir: str,\n",
    "                      job_name: str=\"test_app\",\n",
    "                      config_name: str=\"experiment\") -> DictConfig:\n",
    "    \"\"\"\n",
    "    Read a yaml config file from disk using hydra and return as a DictConfig.\n",
    "    \"\"\"\n",
    "    os.chdir(config_dir)\n",
    "    with initialize_config_dir(config_dir=config_dir, job_name=job_name):\n",
    "        cfg = compose(config_name=config_name)\n",
    "        \n",
    "    if cfg.get(\"print_config\"):\n",
    "        template_utils.print_config(cfg, resolve=True)        \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_datamodule(config: DictConfig) -> pl.LightningDataModule:\n",
    "    log.info(f\"Instantiating datamodule <{config.datamodule._target_}>\")\n",
    "    datamodule: pl.LightningDataModule = hydra.utils.instantiate(config.datamodule)\n",
    "        \n",
    "    try:\n",
    "        datamodule.setup(stage=\"fit\")\n",
    "        config.hparams.classes = datamodule.classes\n",
    "        config.hparams.num_classes = len(config.hparams.classes)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "        \n",
    "    return datamodule, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_model(config: DictConfig) -> pl.LightningModule:\n",
    "    log.info(f\"Instantiating model <{config.model._target_}>\")\n",
    "    model: pl.LightningModule = hydra.utils.instantiate(config.model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0090370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_tuner(config: DictConfig) -> pl.tuner.tuning.Tuner:\n",
    "    config = OmegaConf.create(config)\n",
    "#     trainer_config = OmegaConf.masked_copy(config,['trainer'])\n",
    "#     tuner_config = OmegaConf.masked_copy(config,['tuner'])\n",
    "    if 'ddp' in config.trainer.accelerator:\n",
    "        config.trainer.accelerator = 'dp'\n",
    "        \n",
    "    trainer: pl.Trainer = configure_trainer(config) #hydra.utils.instantiate(trainer_config)\n",
    "    tuner: pl.tuner.tuning.Tuner = hydra.utils.instantiate(config.tuner.instantiate, trainer=trainer)\n",
    "    \n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ef6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# from pytorch_lightning import LightningModule, LightningDataModule, Callback, Trainer\n",
    "# from pytorch_lightning.loggers import LightningLoggerBase\n",
    "# from pytorch_lightning import seed_everything\n",
    "\n",
    "def configure_trainer(config: DictConfig) -> pl.Trainer:\n",
    "\n",
    "    # Init Lightning callbacks\n",
    "    callbacks: List[pl.Callback] = []\n",
    "    if \"callbacks\" in config:\n",
    "        for cb_name, cb_conf in config[\"callbacks\"].items():\n",
    "            if \"_target_\" in cb_conf:\n",
    "                log.info(f\"Instantiating callback <{cb_conf._target_}>\")\n",
    "                if cb_name == \"wandb\":\n",
    "                    callbacks.append(hydra.utils.instantiate(cb_conf, config=OmegaConf.to_container(config, resolve=True)))\n",
    "                else:\n",
    "                    callbacks.append(hydra.utils.instantiate(cb_conf))\n",
    "\n",
    "    # Init Lightning loggers\n",
    "    logger: List[pl.loggers.LightningLoggerBase] = []\n",
    "    if \"logger\" in config:\n",
    "        for _, lg_conf in config[\"logger\"].items():\n",
    "            if \"_target_\" in lg_conf:\n",
    "                log.info(f\"Instantiating logger <{lg_conf._target_}>\")\n",
    "                logger.append(hydra.utils.instantiate(lg_conf))\n",
    "\n",
    "\n",
    "    log.info(f\"Instantiating trainer <{config.trainer._target_}>\")\n",
    "    trainer: pl.Trainer = hydra.utils.instantiate(config.trainer,\n",
    "                                                  callbacks=callbacks,\n",
    "                                                  logger=logger,\n",
    "                                                  _convert_=\"partial\")\n",
    "        \n",
    "    return trainer\n",
    "\n",
    "# trainer = configure_trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs/experiment/3_16bit_precision-single_gpu--tune_batchsize--tune_lr.yaml\"\n",
    "\n",
    "log = get_standard_python_logger(name=Path(config_path).stem,\n",
    "                                 log_path=Path(config_path).parent / \"experiment_logs\") # 'notebook_experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0f4a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --port 0 --logdir lightning_logs/\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad48bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip list\n",
    "#| grep tornado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35eea1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b9f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# config = read_hydra_config(config_dir = str(Path(config_path).parent),\n",
    "#                            job_name=\"test_app\",\n",
    "#                            config_name=Path(config_path).stem)\n",
    "\n",
    "# template_utils.extras(config)\n",
    "\n",
    "\n",
    "\n",
    "# log.info(f\"<----------Initiating auto scale_batch_size tuning----------->\")\n",
    "# log.info(f\"Tuning kwargs:\")\n",
    "# # log.info(f\"Tuning kwargs:\\n{OmegaConf.to_yaml(config.tuner.scale_batch_size.kwargs, resolve=True)}\")\n",
    "# log.info(OmegaConf.to_container(config.tuner.scale_batch_size.kwargs, resolve=True))\n",
    "# # OmegaConf.to_container(\n",
    "\n",
    "# # OmegaConf.to_yaml(\n",
    "\n",
    "# import rich\n",
    "\n",
    "# name = Path(config_path).stem\n",
    "# log_path=Path(config_path).parent / \"experiment_logs\"\n",
    "\n",
    "\n",
    "# if log_path is None:\n",
    "#     log_path = f\"./{name}.log\"\n",
    "# else:\n",
    "#     os.makedirs(log_path, exist_ok=True)\n",
    "#     log_path = str(Path(log_path, name)) + '.log'\n",
    "\n",
    "# file = open(log_path, 'w')\n",
    "# template_utils.print_config(config, resolve=True, file=file)\n",
    "# file.close()\n",
    "# rich.print\n",
    "# rich.print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
