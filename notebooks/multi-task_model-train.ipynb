{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333c21e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# multi-task_model-train\n",
    "\n",
    "`multi-task_model-train.ipynb`\n",
    "\n",
    "End of August attempts to create good model training workflows for multi-task experiments\n",
    "\n",
    "Author: Jacob A Rose  \n",
    "Created on: Monday August 29th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d88d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summarize catalog with skimpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas_log\n",
    "# from skimpy import skim\n",
    "# import pandas as pd\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "# path = \"/media/data_cifs/projects/prj_fossils/data/processed_data/leavesdb-v1_0-prerelease/leavesdb-v1_0-release/catalogs/JR_parsed_catalogs/Master_catalog_leavesdb-v1.0.csv\"\n",
    "# with pandas_log.enable(verbose=True, calculate_memory=True):\n",
    "#     df = pd.read_csv(path)\n",
    "# #     skim(df)\n",
    "#     df = df.convert_dtypes()\n",
    "\n",
    "\n",
    "# df = df.astype({'Order':\"category\",\n",
    "#                 'Family':\"category\",\n",
    "#                 'Genus':\"category\",\n",
    "#                 'species':\"category\",\n",
    "#                 'old_Family':\"category\",\n",
    "#                 'dataset':\"category\",\n",
    "#                 'collection_type':\"category\",\n",
    "#                 'data_subset':\"category\"})\n",
    "# skim(df)\n",
    "# df.head(5)\n",
    "\n",
    "# with pandas_log.enable():\n",
    "#     res = (df.query(\"dataset=='Extant_Leaves'\")\n",
    "# #              .query(\"type_1=='fire' or type_2=='fire'\")\n",
    "# #              .drop(\"legendary\", axis=1)\n",
    "# #              .nsmallest(1,\"total\")\n",
    "#           )\n",
    "# res\n",
    "# # df = df.convert_dtypes()\n",
    "# # for col in df.columns:\n",
    "# #     print(\"=\"*20)\n",
    "# #     print(f\"col name: {col}\")\n",
    "# #     print(f\"Type: {df[col].dtype}\")\n",
    "# #     print(f\"nunique: {df[col].nunique()}\")\n",
    "# #     print(f\"mem_usage: {df[col].memory_usage()}\")\n",
    "# # df.memory_usage()\n",
    "# # df.info()\n",
    "# for col in df.columns:\n",
    "#     print(col, df[col].iloc[0], type(df[col].iloc[0]))\n",
    "# df.reset_index().rename(columns={\"index\":\"idx\"}).loc[0,\"idx\"]#[0]#.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from IPython.core.display import HTML\n",
    "# import base64\n",
    "# # import pandas as pd\n",
    "# from io import BytesIO\n",
    "# # pd.set_option('display.max_colwidth', None)\n",
    "# import pandas as pd\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_colwidth', 200)\n",
    "# # def path_to_image_html(path):\n",
    "# #     return '<img src=\"'+ str(path) + '\" width=\"60\" >'\n",
    "# import PIL\n",
    "# from pathlib import Path\n",
    "# from lightning_hydra_classifiers.scripts.multitask.train import MultiTaskDataModule, LitMultiTaskModule, ImagePredictionLogger, train_task, cmdline_args, CIFAR10DataModule, run_multitask_test, load_data_and_model, load_data\n",
    "# from omegaconf import OmegaConf as oc\n",
    "# from typing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510b896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def image_formatter(img):\n",
    "#     def image_base64(img):\n",
    "#         if isinstance(img, (Path, str)):\n",
    "#             img = PIL.Image.open(img)\n",
    "#         with BytesIO() as buffer:\n",
    "#             img.save(buffer, 'jpeg')\n",
    "#             return base64.b64encode(buffer.getvalue()).decode()\n",
    "        \n",
    "#     return f'<img src=\"data:image/jpeg;base64,{image_base64(img)}\">'\n",
    "\n",
    "\n",
    "# def df_embed_paths2imgs(df: pd.DataFrame,\n",
    "#                         file_path: str, \n",
    "#                         path_col: str=\"path\",\n",
    "#                         display: bool=False\n",
    "#                        ) -> Optional[HTML]:\n",
    "#     if not file_path.endswith(\".html\"):\n",
    "#         file_path = f\"{file_path}.html\"\n",
    "#     df.to_html(file_path,\n",
    "#                escape=False,\n",
    "#                formatters={path_col:image_formatter}\n",
    "#               )\n",
    "#     if display:\n",
    "#         return HTML(filename=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed7ad5",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Visualize image data embedded within a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef024ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning_hydra_classifiers.utils.report_utils import df_embed_paths2imgs\n",
    "# import pandas_log\n",
    "# pandas_log.enable()\n",
    "# config = oc.create({\n",
    "#                     \"data\":{\n",
    "#                             \"batch_size\":32,\n",
    "#                             \"image_size\":512,\n",
    "#                             \"image_buffer_size\":0,\n",
    "#                             \"num_workers\":6,\n",
    "#                             \"pin_memory\":False\n",
    "#                     },\n",
    "#                     \"model\":{},\n",
    "#                     \"debug\":False\n",
    "# })\n",
    "\n",
    "# task_id = 0\n",
    "# datamodule = load_data(config=config,\n",
    "#                        task_id=task_id)\n",
    "# config.model.num_classes = datamodule.num_classes\n",
    "# config.data.full_name = datamodule.train_dataset.config.full_name\n",
    "\n",
    "# dataset = datamodule.test_dataset\n",
    "\n",
    "# # print(f\"datamodule.num_classes: {datamodule.num_classes}\")\n",
    "# print(f\"dataset.label_encoder.num_classes: {dataset.label_encoder.num_classes}\")\n",
    "# print(f\"dataset.config: {str(dataset.config)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a023aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Resample balanced k per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc88576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dataset.samples_df.head(100)\n",
    "\n",
    "\n",
    "# with pandas_log.enable(verbose=True, calculate_memory=True):\n",
    "#     resampled_df = df.groupby(\"family\").sample(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fa28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# full_name = config.data.full_name\n",
    "# with pandas_log.enable(verbose=True, calculate_memory=True):\n",
    "#     df = dataset.samples_df.head(100)\n",
    "#     df_html  = df_embed_paths2imgs(df=df,\n",
    "#                                    file_path=config.data.full_name,\n",
    "#                                    path_col=\"path\",\n",
    "#                                    display=True)\n",
    "# # HTML(df.to_html(f\"{name}.html\", escape=False,formatters=dict(path=image_formatter)))\n",
    "# # df.to_html('webpage.html',escape=False, formatters=dict(Country=path_to_image_html))\n",
    "\n",
    "# from rich import print as pp\n",
    "# pp(config)\n",
    "# data = datamodule.current_task['test']\n",
    "# data\n",
    "# data.samples_df\n",
    "# # output_dir: str = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v1_0\"\n",
    "# # image_file_config = ImageFileDatasetConfig(base_dataset_name = \"Fossil\",\n",
    "# #                                            class_type = \"family\",\n",
    "# #                                            threshold = None,\n",
    "# #                                            resolution = 512,\n",
    "# #                                            version=\"v1_0\")\n",
    "# # dataset = ImageFileDataset.from_config(image_file_config, subset_keys=['all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db169313",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports & definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef854030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# # pretty print all cell's output and not just the last one\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# import os\n",
    "# if 'TOY_DATA_DIR' not in os.environ: \n",
    "#     os.environ['TOY_DATA_DIR'] = \"/media/data_cifs/projects/prj_fossils/data/toy_data\"\n",
    "# default_root_dir = os.environ['TOY_DATA_DIR']\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# import pandas as pd\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_colwidth', 200)\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "# import pytorch_lightning as pl\n",
    "# import timm\n",
    "# from rich import print as pp\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from munch import Munch\n",
    "# from lightning_hydra_classifiers.data.datasets.common import toPIL\n",
    "# from lightning_hydra_classifiers.data.utils.make_catalogs import *\n",
    "# from lightning_hydra_classifiers.utils.metric_utils import get_per_class_metrics, get_scalar_metrics\n",
    "# from lightning_hydra_classifiers.utils.logging_utils import get_wandb_logger\n",
    "# import wandb\n",
    "# torch.manual_seed(17)\n",
    "\n",
    "# from lightning_hydra_classifiers.scripts.multitask.train import MultiTaskDataModule, LitMultiTaskModule, ImagePredictionLogger, train_task, cmdline_args, CIFAR10DataModule, run_multitask_test, load_data_and_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a6ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70fdcea2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LRTunerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f47c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from omegaconf import OmegaConf\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "# @dataclass\n",
    "# class LRTunerConfig:\n",
    "    \n",
    "#     min_lr: float = 1e-08\n",
    "#     max_lr: float = 1.0\n",
    "#     num_training: int = 100\n",
    "#     mode: str = 'exponential'\n",
    "#     early_stop_threshold: float = 4.0\n",
    "\n",
    "\n",
    "# lr_tuner_config = OmegaConf.structured(LRTunerConfig())\n",
    "# lr_tuner_config\n",
    "\n",
    "# from rich import print as pp\n",
    "# pp(type(lr_tuner_config))\n",
    "############################\n",
    "\n",
    "\n",
    "# import pl_bolts\n",
    "\n",
    "# # pl_bolts.callbacks.TrainingDataMonitor()\n",
    "# # pl.callbacks.LearningRateMonitor\n",
    "# # pl_bolts.callbacks.ModuleDataMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d8d62",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate image dataset mean & std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106014b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from tqdm.auto import tqdm\n",
    "# # batch_size = 2\n",
    "# # loader = DataLoader(image_data, \n",
    "# #                     batch_size = batch_size,\n",
    "# #                     num_workers=1)\n",
    "\n",
    "# from torch import nn\n",
    "# from lightning_hydra_classifiers.utils.template_utils import get_logger\n",
    "# logger = get_logger(name=__name__)\n",
    "\n",
    "# class ImageStatsAccumulator(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Calculates a dataset-wide set of image statistics.\n",
    "#     Currently:\n",
    "#         - per-channel pixel mean\n",
    "#         - per-channel pixel std\n",
    "#         - per-channel pixel min\n",
    "#         - per-channel pixel max\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, name: str=\"image_stats\", cache_dir: str=None, clear_cache: bool=True):\n",
    "#         self.pixel_count = torch.zeros(1)\n",
    "#         self.image_count = torch.zeros(1)\n",
    "#         self.sum_of_pixels = torch.zeros(3)\n",
    "#         self.sum_of_square_pixels = torch.zeros(3)\n",
    "#         self.resolution = torch.zeros(1)\n",
    "# #         self.channels = torch.Tensor(3)\n",
    "        \n",
    "#         self.global_max_pixel = torch.Tensor([-float(\"inf\")]*3)\n",
    "#         self.global_min_pixel = torch.Tensor([float(\"inf\")]*3)\n",
    "        \n",
    "#         self.name = name\n",
    "#         self.cache_dir = cache_dir\n",
    "#         self.cache_path = os.path.join(self.cache_dir, self.name + \".pth\")\n",
    "# #         self.clear_cache = clear_cache\n",
    "#         if os.path.isfile(self.cache_path):\n",
    "#             if clear_cache:\n",
    "#                 logger.warning(f\"Removing prior cache prior to stats update.\")\n",
    "#                 os.remove(self.cache_path)\n",
    "#             else:\n",
    "#                 self.load_from_cache()\n",
    "\n",
    "        \n",
    "#     def compute(self):        \n",
    "#         global_mean = self.sum_of_pixels / self.pixel_count\n",
    "#         global_var = (self.sum_of_square_pixels / self.pixel_count) - global_mean**2\n",
    "#         global_std = torch.sqrt(global_var)\n",
    "#         return global_mean, global_std, self.global_min_pixel, self.global_max_pixel\n",
    "    \n",
    "    \n",
    "#     def update_step(self, batch):\n",
    "#         images = batch[0]\n",
    "#         b, c, h, w = images.shape\n",
    "#         nb_pixels = b * h * w\n",
    "#         if self.resolution == 0:\n",
    "#             self.resolution = h\n",
    "#         assert self.resolution == h == w\n",
    "        \n",
    "#         batch_max = torch.amax(images, dim=(0,2,3))\n",
    "#         batch_min = torch.amin(images, dim=(0,2,3))\n",
    "        \n",
    "#         self.global_max_pixel = torch.amax(torch.stack([self.global_max_pixel, batch_max]), dim=0)\n",
    "#         self.global_min_pixel = torch.amin(torch.stack([self.global_min_pixel, batch_min]), dim=0)\n",
    "        \n",
    "        \n",
    "#         self.sum_of_pixels        += torch.sum(images,    dim=[0, 2, 3])\n",
    "#         self.sum_of_square_pixels += torch.sum(images**2, dim=[0, 2, 3])\n",
    "#         self.pixel_count          += nb_pixels\n",
    "#         self.image_count          += b\n",
    "        \n",
    "#     def update(self, loader):\n",
    "\n",
    "#         for batch in tqdm(loader):\n",
    "#             self.update_step(batch)\n",
    "#         global_mean, global_std, global_min, global_max = self.compute()\n",
    "#         logger.info(f\"[FINISHED] Calculating full dataset stats from a collection of {self.image_count} images at resolution {self.resolution}\")\n",
    "#         logger.info(f\"Global channel-wise mean = {global_mean}\")\n",
    "#         logger.info(f\"Global channel-wise std = {global_std}\")\n",
    "#         logger.info(f\"Global channel-wise pixel min = {global_min}\")\n",
    "#         logger.info(f\"Global channel-wise pixel max = {global_max}\")\n",
    "\n",
    "#         return global_mean, global_std, global_min, global_max\n",
    "    \n",
    "#     def state_dict(self):\n",
    "#         global_mean, global_std, _, _ = self.compute()\n",
    "#         return {\"global_mean\": global_mean,\n",
    "#                 \"global_std\": global_std,\n",
    "#                 \"pixel_count\": self.pixel_count,\n",
    "#                 \"image_count\": self.image_count,\n",
    "#                 \"sum_of_pixels\": self.sum_of_pixels,\n",
    "#                 \"sum_of_square_pixels\": self.sum_of_square_pixels,\n",
    "#                 \"resolution\": self.resolution,\n",
    "#                 \"global_max_pixel\": self.global_max_pixel,\n",
    "#                 \"global_min_pixel\": self.global_min_pixel}\n",
    "    \n",
    "#     def load_state_dict(self, state_dict):\n",
    "#         self.pixel_count = state_dict[\"pixel_count\"]\n",
    "#         self.image_count = state_dict[\"image_count\"]\n",
    "#         self.sum_of_pixels = state_dict[\"sum_of_pixels\"]\n",
    "#         self.sum_of_square_pixels = state_dict[\"sum_of_square_pixels\"]\n",
    "#         self.resolution = state_dict[\"resolution\"] \n",
    "#         self.global_max_pixel = state_dict[\"global_max_pixel\"]\n",
    "#         self.global_min_pixel = state_dict[\"global_min_pixel\"]\n",
    "        \n",
    "#         global_mean, global_std, _, _ = self.compute()\n",
    "#         assert torch.all(global_mean == state_dict[\"global_mean\"])\n",
    "#         assert torch.all(global_std == state_dict[\"global_std\"])    \n",
    "    \n",
    "#     def cache(self):\n",
    "#         if not isinstance(self.cache_dir, (str, Path)):\n",
    "#             return\n",
    "# #         self.cache_path = os.path.join(self.cache_dir, self.name + \".pth\")\n",
    "#         torch.save(self.state_dict(), self.cache_path)\n",
    "#         logger.info(f\"Updated Image Stats cache located at: {self.cache_path}\")\n",
    "\n",
    "#     def load_from_cache(self):\n",
    "#         if not isinstance(self.cache_dir, (str, Path)):\n",
    "#             logger.warning(f\"No cache detected. No-op.\")\n",
    "#             return\n",
    "# #         self.cache_path = os.path.join(self.cache_dir, self.name + \".pth\")\n",
    "#         if os.path.isfile(self.cache_path):            \n",
    "#             self.load_state_dict(torch.load(self.cache_path))\n",
    "#             logger.info(f\"Loaded Image Stats from cache located at: {self.cache_path}\")\n",
    "#         else:\n",
    "#             logger.warning(f\"No cache detected. No-op.\")\n",
    "        \n",
    "    \n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return \"ImageStatsAccumulator:\\n\" + str('\\n'.join([f\"{k}: {v}\" for k,v in self.state_dict().items()]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f9735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # datamodule.train_dataset.transform = torchvision.transforms.ToTensor()\n",
    "# # loader = datamodule.train_dataloader()\n",
    "# # iter_loader = iter(loader)\n",
    "# # batches = (next(iter_loader) for i in range(4))\n",
    "# # stats = ImageStatsAccumulator()\n",
    "# # results = stats.update(batches)#loader)\n",
    "# datamodule = MultiTaskDataModule(batch_size=64,\n",
    "#                                  task_id=0,\n",
    "#                                  image_size=512, #model.config.image_size,\n",
    "#                                  image_buffer_size=0, #32,\n",
    "#                                  num_workers=4,\n",
    "#                                  pin_memory=False)\n",
    "# datamodule.setup()\n",
    "\n",
    "# datamodule.train_dataset.transform = torchvision.transforms.ToTensor()\n",
    "# loader = datamodule.train_dataloader()\n",
    "\n",
    "# stats = ImageStatsAccumulator(cache_dir=\"\")\n",
    "\n",
    "# results = stats.update(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5178742",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Load model and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7875ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# args, config = cmdline_args([\"--load_from_checkpoint\", \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-PNAS-to-PNAS_efficientnet_b1_imagenet_weights/checkpoints/task_0/epoch=08-val_loss=1.3451-val_acc=0.6087.ckpt\"])\n",
    "\n",
    "# model = LitMultiTaskModule(config.model)\n",
    "# model.label_encoder = datamodule.label_encoder\n",
    "\n",
    "\n",
    "# ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-PNAS-to-PNAS_efficientnet_b1_imagenet_weights/checkpoints/task_0/epoch=08-val_loss=1.3451-val_acc=0.6087.ckpt\"\n",
    "\n",
    "# ckpt = torch.load(ckpt_path, map_location=\"cuda:0\")\n",
    "# ckpt.keys()\n",
    "# for k, v in ckpt.items():\n",
    "#     if \"state\" not in k:\n",
    "#         print(k, v)\n",
    "\n",
    "# ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-PNAS-to-PNAS_efficientnet_b3_imagenet_weights/task_0/checkpoints/task_0/epoch=14-val_loss=nan-val_acc=0.6974.ckpt\"\n",
    "# ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-PNAS-to-PNAS_efficientnet_b1_imagenet_weights/checkpoints/task_0/epoch=08-val_loss=1.3451-val_acc=0.6087.ckpt\"\n",
    "\n",
    "# args, config = cmdline_args([\"-ckpt\", ckpt_path, \"-res\", \"512\"])\n",
    "# args, config = cmdline_args([\"-res\", \"512\"])\n",
    "# model = LitMultiTaskModule.load_from_checkpoint(config.ckpt_path)\n",
    "\n",
    "# datamodule = MultiTaskDataModule(batch_size=config.data.batch_size,\n",
    "#                                  task_id=0,\n",
    "#                                  image_size=model.config.image_size,\n",
    "#                                  image_buffer_size=config.data.image_buffer_size,\n",
    "#                                  num_workers=config.data.num_workers,\n",
    "#                                  pin_memory=config.data.pin_memory)\n",
    "\n",
    "# state_dict = torch.load(ckpt_path, map_location=\"cuda:0\")\n",
    "# print(state_dict.keys())\n",
    "\n",
    "# for k,v in state_dict.items():\n",
    "#     if k != \"state_dict\":\n",
    "#         if isinstance(v, dict):\n",
    "#             print(k, v.keys())\n",
    "#         elif k == \"optimizer_states\":\n",
    "#             for i in v:\n",
    "#                 print(k, i.keys())\n",
    "#         else:\n",
    "#             print(k, v)\n",
    "# #     else:\n",
    "# #         print(k, v.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4cb0c2",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# sample test function\n",
    "\n",
    "source: https://github.com/pytorch/hydra-torch/blob/examples/mnist_01/examples/mnist_00.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd1b4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Current Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64955610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "if 'TOY_DATA_DIR' not in os.environ: \n",
    "    os.environ['TOY_DATA_DIR'] = \"/media/data_cifs/projects/prj_fossils/data/toy_data\"\n",
    "default_root_dir = os.environ['TOY_DATA_DIR']\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 600)\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "from rich import print as pp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from munch import Munch\n",
    "from lightning_hydra_classifiers.data.datasets.common import toPIL\n",
    "from lightning_hydra_classifiers.data.utils.make_catalogs import *\n",
    "from lightning_hydra_classifiers.utils.metric_utils import get_per_class_metrics, get_scalar_metrics\n",
    "from lightning_hydra_classifiers.utils.logging_utils import get_wandb_logger\n",
    "import wandb\n",
    "torch.manual_seed(17)\n",
    "\n",
    "from lightning_hydra_classifiers.experiments.multitask.modules import MultiTaskClassifierConfig, ClassifierConfig, BackboneConfig, LitMultiTaskModuleConfig\n",
    "\n",
    "from lightning_hydra_classifiers.scripts.multitask.train import MultiTaskDataModule, LitMultiTaskModule, ImagePredictionLogger, train_task,  CIFAR10DataModule, run_multitask_test, load_data_and_model, load_data, resolve_config\n",
    "\n",
    "from dataclasses import dataclass, asdict, is_dataclass\n",
    "import json\n",
    "\n",
    "from lightning_hydra_classifiers.utils.template_utils import get_logger\n",
    "logger = get_logger(name=__name__)\n",
    "\n",
    "from lightning_hydra_classifiers.experiments.transfer_experiment import * #TransferExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa56527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "overrides = ['model/backbone=efficientnet_b3',\"data=extant_to_fossil\", \"trainer.max_epochs=2\", \"data.batch_size=16\", \"trainer.precision=16\"]#, \"trainer.gpus='3'\"]\n",
    "\n",
    "config = ETL.load_hydra_config(config_name = \"config\",\n",
    "                              config_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/configs\",\n",
    "                              overrides=overrides)\n",
    "\n",
    "task_id = 0\n",
    "# pp(OmegaConf.to_container(cfg, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3700c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model\n",
    "\n",
    "# LitMultiTaskModule.load_state_dict\n",
    "\n",
    "# loaded_model = LitMultiTaskModule.load_from_checkpoint(ckpt_path)\n",
    "# loaded_model.config\n",
    "# loaded_model.set_current_classifier(\"task_1\")\n",
    "# loaded_model\n",
    "# outcome = loaded_model.load_state_dict(checkpoint['state_dict'])\n",
    "# ckpt_path = \"//media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-to-Fossil-512-transfer_benchmark/replicate_1/results/checkpoints/epoch=03-val_loss=5.558-val_acc=0.114.ckpt\"\n",
    "\n",
    "ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-to-PNAS-512-transfer_benchmark/replicate_1/results/checkpoints/epoch=05-val_loss=0.205-val_acc=0.977.ckpt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "# ckpt_path = \"//media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-to-Fossil-512-transfer_benchmark/replicate_1/results/checkpoints/epoch=03-val_loss=5.558-val_acc=0.114.ckpt\"\n",
    "\n",
    "# ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-to-PNAS-512-transfer_benchmark/replicate_1/results/checkpoints/epoch=07-val_loss=1.179-val_acc=0.610.ckpt\"\n",
    "\n",
    "ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-to-PNAS-512-transfer_benchmark/replicate_1/results/checkpoints/epoch=05-val_loss=0.205-val_acc=0.977.ckpt\"\n",
    "\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "checkpoint['hparams_name']\n",
    "checkpoint['hyper_parameters']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bbdaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-to-PNAS-512-transfer_benchmark/replicate_1/results/checkpoints/epoch=05-val_loss=0.205-val_acc=0.977.ckpt\"\n",
    "\n",
    "\n",
    "config.model.ckpt_path = ckpt_path\n",
    "datamodule, model, config = load_data_and_model(config=config, task_id=task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.backbone\n",
    "dict(model.state_dict()).keys()\n",
    "\n",
    "# dict(backbone.state_dict()).keys()\n",
    "# dict(backbone_sequential.state_dict()).keys()\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16142e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "\n",
    "trainer_config = resolve_config(config.trainer)\n",
    "\n",
    "\n",
    "logger = []\n",
    "for _, lg_conf in config[\"logger\"].items():\n",
    "    if \"_target_\" in lg_conf:\n",
    "#         logging.info(f\"Instantiating logger <{lg_conf._target_}>\")\n",
    "        logger.append(hydra.utils.instantiate(lg_conf))\n",
    "\n",
    "\n",
    "# trainer_config['callbacks'] = callbacks\n",
    "trainer_config['logger'] = logger\n",
    "\n",
    "\n",
    "\n",
    "trainer: pl.Trainer = hydra.utils.instantiate(trainer_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4450e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d98a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO (monday, 2:55 pm) Save the heckpoint after 1 epoch of fit finishes successfully\n",
    "\n",
    "trainer.save_checkpoint('test_ckpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0fd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385561e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_path, map_location='cuda')\n",
    "ckpt['state_dict'].keys()\n",
    "\n",
    "model.state_dict().keys()\n",
    "\n",
    "dict(model.named_parameters()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7277c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_from_checkpoint(ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# data_loader = datamodule.test_dataloader()\n",
    "# results = []\n",
    "\n",
    "\n",
    "\n",
    "results = trainer.test(model=model, datamodule=datamodule)\n",
    "\n",
    "# total = len(data_loader)\n",
    "# for i, batch in enumerate(tqdm(iter(data_loader), total=total)):\n",
    "# #     x, y = batch[:2]\n",
    "    \n",
    "#     results.append(model.test_step(batch, i))\n",
    "\n",
    "# model.test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# results[0].keys()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "per_class = {k:v for k,v in results[0].items() if \"per_class\" in k}\n",
    "\n",
    "for k,v in per_class.items():\n",
    "\n",
    "    if np.isnan(v):\n",
    "        per_class[k] = 0\n",
    "#     print(k, v, type(v), )\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "x= np.stack(list(per_class.keys()))\n",
    "y= np.stack(list(per_class.values()))\n",
    "print(y.shape)\n",
    "sns.barplot(x=x, y=y)\n",
    "# dir(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2196d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.scripts.multitask.train import *\n",
    "# from lightning_hydra_classifiers.models.heads import ClassifierHead\n",
    "from lightning_hydra_classifiers.models.backbones import backbone\n",
    "\n",
    "AVAILABLE_GLOBAL_POOL_LAYERS = {\"avg\":nn.AdaptiveAvgPool2d,\n",
    "                                \"max\":nn.AdaptiveMaxPool2d}\n",
    "\n",
    "\n",
    "GlobalPoolFactory = AVAILABLE_GLOBAL_POOL_LAYERS[config.model.backbone.global_pool_type]\n",
    "backbone = backbone.build_model(model_name=config.model.backbone.backbone_name,\n",
    "                                     pretrained=config.model.backbone.pretrained,\n",
    "#                                              num_classes=num_classes,\n",
    "#                                              global_pool_type=config.global_pool_type,\n",
    "                                     drop_rate=config.model.backbone.drop_rate)\n",
    "\n",
    "# self.out_features = self.backbone.out_features\n",
    "# self.global_pool = GlobalPoolFactory(1) #self.out_features//4) #self.backbone.global_pool\n",
    "\n",
    "        \n",
    "\n",
    "# backbone_sequential = nn.Sequential(*list(backbone.backbone.children()))\n",
    "from collections import OrderedDict\n",
    "backbone_sequential = nn.Sequential(OrderedDict(backbone.backbone.named_children()))\n",
    "# backbone_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d975058",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((1,3,512,512))\n",
    "\n",
    "y_logit = backbone(x)\n",
    "\n",
    "y_logit_sequential = backbone_sequential(x)\n",
    "\n",
    "torch.allclose(y_logit, y_logit_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone\n",
    "\n",
    "dict(model.state_dict()).keys()\n",
    "\n",
    "# dict(backbone.state_dict()).keys()\n",
    "# dict(backbone_sequential.state_dict()).keys()\n",
    "\n",
    "\n",
    "\n",
    "# for b_l, b_seq_l in zip(dict(backbone.state_dict()).keys(), dict(backbone_sequential.state_dict()).keys()):\n",
    "#     print(f\"{b_l}, vs. {b_seq_l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67458579",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 5\n",
    "\n",
    "# toPIL(datamodule.test_dataset[8][0])\n",
    "data_iter = iter(datamodule.test_dataloader())\n",
    "data_iter = [next(data_iter) for i in range(num_batches)]\n",
    "preds, targets = []\n",
    "\n",
    "for batch in data_iter:\n",
    "    preds.append(batch)\n",
    "x, y = batch[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_logit = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbbc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['hparams_name']\n",
    "checkpoint['hyper_parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.zeros((9,3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7da746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = torch.cat(self.preds).cpu().numpy()\n",
    "targets = torch.cat(self.targets).cpu().numpy()\n",
    "f1 = f1_score(preds, targets, average=None)\n",
    "r = recall_score(preds, targets, average=None)\n",
    "p = precision_score(preds, targets, average=None)\n",
    "data = [f1, p, r]\n",
    "\n",
    "# set figure size\n",
    "plt.figure(figsize=(14, 3))\n",
    "\n",
    "# set labels size\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# set font size\n",
    "sns.heatmap(\n",
    "    data,\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 10},\n",
    "    fmt=\".3f\",\n",
    "    yticklabels=[\"F1\", \"Precision\", \"Recall\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19de7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510572d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = model.metrics_test\n",
    "metric = model.metrics_test_per_class\n",
    "\n",
    "y_logit[0,:]=0.0\n",
    "y_logit[0,100]=0.999\n",
    "\n",
    "batch_metric = metric(y_logit, y)\n",
    "\n",
    "batch_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f92a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d17ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_metric = metric.compute()\n",
    "\n",
    "\n",
    "print(epoch_metric['test/per_class/F1'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.hist(y_logit.detach().numpy(), bins=50); #datamodule.num_classes//6);\n",
    "plt.title('Histogram of predicted probabilities')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbbe5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.hist(np.argmax(y_logit.detach().numpy(),axis=1), bins=50); #datamodule.num_classes//6);\n",
    "plt.title('Histogram of predicted classes')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc00a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.hist(y.detach().numpy(), bins=datamodule.num_classes)\n",
    "\n",
    "plt.title('Histogram of True labels')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdbfc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp(OmegaConf.to_container(config.data, resolve=True))\n",
    "# pp(OmegaConf.to_container(config.model, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "def log_model_summary(model: nn.Module,\n",
    "                      input_size: Tuple[int],\n",
    "                      full_summary: bool=True,\n",
    "                      working_dir: str=\".\",\n",
    "                      model_name: Optional[str]=None,\n",
    "                      verbose: bool=1):\n",
    "    \"\"\"\n",
    "    produce a text file with the model summary\n",
    "    \n",
    "    TODO: Add this to Eval Plugins\n",
    "    \n",
    "    log_model_summary(model=model,\n",
    "                  working_dir=working_dir,\n",
    "                  input_size=(1, data_config.channels, *data_config.image_size),\n",
    "                  full_summary=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if full_summary:\n",
    "        col_names=(\"kernel_size\", \"input_size\",\"output_size\", \"num_params\", \"mult_adds\")\n",
    "    else:\n",
    "        col_names=(\"input_size\",\"output_size\", \"num_params\")\n",
    "\n",
    "    model_summary = summary(model.cuda(),\n",
    "                            input_size=input_size,\n",
    "                            row_settings=('depth', 'var_names'),\n",
    "                            col_names=col_names,\n",
    "                            verbose=verbose)\n",
    "\n",
    "    if working_dir is None:\n",
    "        return model_summary\n",
    "    \n",
    "    \n",
    "    if (model_name is None) and (hasattr(model, \"name\")):\n",
    "        model_name = model.name\n",
    "    if (model_name is None):\n",
    "        summary_path = os.path.join(working_dir, 'model', f'model_summary.txt')        \n",
    "    else:\n",
    "        summary_path = os.path.join(working_dir, 'model', f'{model_name}_model_summary.txt')\n",
    "    \n",
    "    os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(str(model_summary))\n",
    "        \n",
    "    return model_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_summary = log_model_summary(model=model,\n",
    "                                  input_size=(2,3,512,512),\n",
    "                                  full_summary=True,\n",
    "                                  working_dir=None,\n",
    "                                  model_name=None,\n",
    "                                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8849b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.utils.etl_utils import ETL\n",
    "\n",
    "config = Extant_to_Fossil_ExperimentConfig()\n",
    "\n",
    "ETL.config2yaml(config, \"test.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66750193",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((2,3,512,512), device='cuda')\n",
    "\n",
    "x_hat = model.backbone(x)\n",
    "\n",
    "x_hat.shape\n",
    "\n",
    "from torch import nn\n",
    "pool_size = 2048\n",
    "pool = nn.AdaptiveAvgPool2d(pool_size)\n",
    "x_out = pool(x_hat.cpu())\n",
    "\n",
    "print(f\"Before: {x_hat.shape}\")\n",
    "print(f\"pool: {pool}\")\n",
    "print(f\"After: {x_out.shape}\")\n",
    "\n",
    "pool_size = 1\n",
    "pool = nn.AdaptiveAvgPool2d(pool_size)\n",
    "x_out = pool(x_hat.cpu())\n",
    "\n",
    "print(f\"Before: {x_hat.shape}\")\n",
    "print(f\"pool: {pool}\")\n",
    "print(f\"After: {x_out.shape}\")\n",
    "\n",
    "pool_size = (1,1)\n",
    "pool = nn.AdaptiveAvgPool2d(pool_size)\n",
    "x_out = pool(x_hat.cpu())\n",
    "\n",
    "print(f\"Before: {x_hat.shape}\")\n",
    "print(f\"pool: {pool}\")\n",
    "print(f\"After: {x_out.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "x_pooled = model.global_pool.cpu()(x_hat.cpu())\n",
    "\n",
    "x_pooled.shape\n",
    "\n",
    "model.global_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Extant_to_Fossil_ExperimentConfig()\n",
    "config = Extant_to_PNAS_ExperimentConfig()\n",
    "experiment = TransferExperiment(config)\n",
    "pp((asdict(experiment.config)))\n",
    "\n",
    "task_0 = experiment.setup_task_0()\n",
    "task_1 = experiment.setup_task_1()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LitMultiTaskModuleConfig()\n",
    "\n",
    "args, config = cmdline_args([\"-res\", \"512\", \"-model\", \"resnet50\"])\n",
    "\n",
    "# print(\"OLD CONFIG:\")\n",
    "# print(\"=\"*10)\n",
    "# pp(OmegaConf.to_yaml(config))\n",
    "\n",
    "# dir(hydra.experimental)\n",
    "\n",
    "# import hydra\n",
    "# hydra.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ce796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getattr(config, \"data\")\n",
    "\n",
    "getattr(config.data, \"experiment\", [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef114eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(dict(args.__dict__))\n",
    "\n",
    "pp(OmegaConf.to_container(config))\n",
    "type(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6044948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dictconfig2dataclass(config, dataclass_type: Callable):#MultiTaskExperimentConfig):\n",
    "#     \"\"\"\n",
    "#     Recursively convert DictConfig to a Structured Config specified by the provided dataclass.\n",
    "#     \"\"\"\n",
    "#     kwargs = {}\n",
    "#     for k, field in dataclass_type.__dataclass_fields__.items():\n",
    "#         if is_dataclass(field.type):\n",
    "#             kwargs[k] = dictconfig2dataclass(config=config[k], dataclass_type=field.type)\n",
    "#         else:\n",
    "#             try:\n",
    "#                 kwargs[k] = field.type(**config[k])\n",
    "#             except TypeError as e:\n",
    "#                 kwargs[k] = config[k]\n",
    "#     return dataclass_type(**kwargs)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c960cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.experiments.configs.file_manager import TaskFileSystemConfig, MultiTaskFileSystemConfig\n",
    "from lightning_hydra_classifiers.experiments.configs.utils import hash_utils\n",
    "from lightning_hydra_classifiers.utils.dataset_management_utils import ETL\n",
    "from rich import print as pp\n",
    "from dataclasses import dataclass, asdict, is_dataclass\n",
    "\n",
    "from hydra.experimental import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from lightning_hydra_classifiers.experiments.configs.config import MultiTaskExperimentConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc6d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "\n",
    "hydra.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hashlib import md5\n",
    "\n",
    "cfg = ETL.init_structured_config(config_name = \"multitask_experiment_config\",\n",
    "                                 config_path = None,\n",
    "                                 job_name = \"test_app\",\n",
    "                                 dataclass_type = MultiTaskExperimentConfig,\n",
    "                                 overrides=['++root_dir=~'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.hashname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.hashname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972541c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate=1\n",
    "experiment_name=\"Extant-PNAS-to-PNAS_resnet50_imagenet-pretrained\"\n",
    "root_dir= \"/media/data/jacob/GitHub/lightning-hydra-classifiers/notebooks/experiments_September_2021\"\n",
    "\n",
    "system = MultiTaskFileSystemConfig(replicate=replicate,\n",
    "                                   experiment_name=experiment_name,\n",
    "                                   root_dir=root_dir,\n",
    "                                   hashname=cfg.hashname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(cfg.system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp(cfg.model)\n",
    "pp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cfg.model.optimizer = {cfg.model.optimizer}\")\n",
    "print(f\"hash(cfg.model.optimizer) = {hash_utils.get_hash(cfg.model.optimizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18eaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cfg.model.optimizer = {cfg.model.optimizer}\")\n",
    "print(f\"hash(cfg.model.optimizer) = {hash_utils.get_hash(cfg.model.optimizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"hash(cfg.model) = {hash_utils.get_hash(cfg.model)}\")\n",
    "print(f\"cfg.model = \")\n",
    "pp(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"hash(cfg.model) = {hash_utils.get_hash(cfg.model)}\")\n",
    "print(f\"cfg.model = \")\n",
    "pp(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"hash(cfg) = {hash_utils.get_hash(cfg)}\")\n",
    "print(f\"cfg = \")\n",
    "pp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"hash(cfg) = {hash_utils.get_hash(cfg)}\")\n",
    "print(f\"cfg = \")\n",
    "pp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(cfg.model.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ef78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.model.optimizer.lr = 1e-3\n",
    "# cfg.model.optimizer.betas = (0.8, 0.888)\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# thing = asdict(cfg.model.optimizer)\n",
    "thing = cfg.model.optimizer\n",
    "\n",
    "\n",
    "# hashlib.md5(json.dumps(thing).encode('utf-8'))\n",
    "hash_obj = hashlib.md5(hash_utils.json_dumps(thing).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_obj.digest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(hash_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d232ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cfg.model.optimizer = {cfg.model.optimizer}\")\n",
    "print(f\"hash(cfg.model.optimizer) = {hash_utils.get_hash(asdict(cfg.model.optimizer))}\")\n",
    "# hash_utils.get_hash(cfg.model.optimizer.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98fbb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cfg.model.optimizer = {cfg.model.optimizer}\")\n",
    "print(f\"hash(cfg.model.optimizer) = {hash_utils.get_hash(cfg.model.optimizer)}\")\n",
    "# hash_utils.get_hash(cfg.model.optimizer.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cfg.model.optimizer = {cfg.model.optimizer}\")\n",
    "print(f\"hash(cfg.model.optimizer) = {hash_utils.get_hash(cfg.model.optimizer)}\")\n",
    "# hash_utils.get_hash(cfg.model.optimizer.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cfg.model.optimizer.betas = {cfg.model.optimizer.betas}\")\n",
    "print(f\"hash(cfg.model.optimizer.betas) = {hash_utils.get_hash(cfg.model.optimizer.betas)}\")\n",
    "# hash_utils.get_hash(cfg.model.optimizer.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash(cfg.model.optimizer.betas)# = [0.8, 0.888]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ade58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d18bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hash(cfg.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.experimental import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# from lightning_hydra_classifiers.experiments.multitask.modules import MultiTaskClassifierConfig, ClassifierConfig, BackboneConfig, LitMultiTaskModuleConfig\n",
    "from lightning_hydra_classifiers.experiments.configs.config import MultiTaskExperimentConfig\n",
    "\n",
    "\n",
    "with initialize():\n",
    "    cfg = compose(config_name=\"multitask_experiment_config\")\n",
    "\n",
    "cfg = dictconfig2dataclass(config=cfg, dataclass_type=MultiTaskExperimentConfig)\n",
    "print(\"Structured Config:\")\n",
    "pp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictconfig2dataclass(config, dataclass_type: Callable=MultiTaskExperimentConfig):\n",
    "    kwargs = {}\n",
    "    for k, field in dataclass_type.__dataclass_fields__.items():\n",
    "        if is_dataclass(field.type):\n",
    "#             print(f\"{k} is a dataclass: {config[k]}\")\n",
    "#             print(f\"field.type={field.type}\")\n",
    "            kwargs[k] = dictconfig2dataclass(config=config[k], dataclass_type=field.type)\n",
    "#             print(f\"isinstance(kwargs[k], field.type) : isinstance({kwargs[k]}, {field.type}) = {isinstance(kwargs[k], field.type)}\")\n",
    "#             kwargs[k] = dictconfig2dataclass(config=field, dataclass_type=field.type)\n",
    "        else:\n",
    "            try:\n",
    "                kwargs[k] = field.type(**config[k])\n",
    "            except TypeError as e:\n",
    "                kwargs[k] = config[k]\n",
    "    return dataclass_type(**kwargs)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultitaskExperimentConfig.__dataclass_fields__['model'].type.__dataclass_fields__\n",
    "\n",
    "# out = dictconfig2dataclass(config=cfg.model, dataclass_type=LitMultiTaskModuleConfig)\n",
    "# out = dictconfig2dataclass(config=cfg.model.multitask, dataclass_type=MultiTaskClassifierConfig)\n",
    "out = dictconfig2dataclass(config=cfg, dataclass_type=MultitaskExperimentConfig)\n",
    "pp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataclass_type = MultitaskExperimentConfig.__dataclass_fields__['model'].type.__dataclass_fields__['multitask'].type\n",
    "# dataclass_type = type(struct_cfg.model)\n",
    "\n",
    "# for k, field in dataclass_type.__dataclass_fields__.items():\n",
    "#     print(k, field, field.type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from dataclasses import is_dataclass\n",
    "\n",
    "# is_dataclass(MultitaskExperimentConfig.__dataclass_fields__['model'].type)\n",
    "# MultitaskExperimentConfig.__dataclass_fields__['model'].type\n",
    "\n",
    "# struct_cfg = MultitaskExperimentConfig(**cfg)\n",
    "# print(type(struct_cfg))\n",
    "# pp(struct_cfg)\n",
    "# from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f57ab3",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Config hash utilities (check if i already moved to module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f4f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(cfg.multitask_config.__dataclass_fields__.keys())\n",
    "\n",
    "for task in cfg.multitask_config.__dataclass_fields__.keys():\n",
    "    print(task)\n",
    "    \n",
    "    print(getattr(cfg.multitask_config, task))\n",
    "\n",
    "def dict_drop_empty(pairs):\n",
    "    return dict(\n",
    "        (k, v)\n",
    "        for k, v in pairs\n",
    "        if not (\n",
    "            v is None\n",
    "            or not v and isinstance(v, Collection)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def json_default(thing):\n",
    "    return dataclasses.asdict(thing, dict_factory=dict_drop_empty)\n",
    "\n",
    "\n",
    "def json_dumps(thing):\n",
    "    return json.dumps(\n",
    "        thing,\n",
    "        default=json_default,\n",
    "        ensure_ascii=False,\n",
    "        sort_keys=True,\n",
    "        indent=None,\n",
    "        separators=(',', ':'),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_hash(thing):\n",
    "    \"\"\"\n",
    "    Produce deterministic md5 hash codes from dataclass configs.\n",
    "    \n",
    "    Based on blogpost: https://death.andgravity.com/stable-hashing\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    return hashlib.md5(json_dumps(thing).encode('utf-8')).digest()\n",
    "\n",
    "# print(json_dumps(asdict(cfg)))\n",
    "# print(get_hash(cfg))\n",
    "# cfg.multitask_config.task_0.in_features=1024\n",
    "# cfg.multitask_config.task_0\n",
    "# print(get_hash(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.structured(MultitaskExperimentConfig().backbone_config)\n",
    "\n",
    "LitMultiTaskModuleConfig().backbone_config\n",
    "\n",
    "OmegaConf.structured(LitMultiTaskModuleConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# hydra-torch structured config imports\n",
    "# from hydra_configs.torch.optim import AdadeltaConf\n",
    "# from hydra_configs.torch.optim.lr_scheduler import StepLRConf\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultitaskExperimentConfig:\n",
    "    batch_size: int = 64\n",
    "#     test_batch_size: int = 1000\n",
    "    num_epochs: int = 14\n",
    "    gpus: Any = \"3\"\n",
    "#     dry_run: bool = False\n",
    "    seed: int = 1\n",
    "    model: LitMultiTaskModuleConfig = LitMultiTaskModuleConfig()\n",
    "#     log_interval: int = 10\n",
    "#     save_model: bool = False\n",
    "#     checkpoint_name: str = \"unnamed.pt\"\n",
    "\n",
    "\n",
    "\n",
    "cs = ConfigStore.instance()\n",
    "cs.store(name=\"multitask_experiment\", node=MultitaskExperimentConfig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(cs.load(\"multitask_experiment.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1174a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "dir(dataclasses)\n",
    "\n",
    "dir(cfg)\n",
    "\n",
    "import hashlib\n",
    "dir(hashlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_state_dict(state_dict):\n",
    "    s = 0\n",
    "    for _, v in state_dict.items():\n",
    "        s += v.sum()\n",
    "    return s\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "def log_model_summary(model: nn.Module,\n",
    "                      input_size: Tuple[int],\n",
    "                      full_summary: bool=True,\n",
    "                      working_dir: str=\".\",\n",
    "                      model_name: Optional[str]=None,\n",
    "                      verbose: bool=1):\n",
    "    \"\"\"\n",
    "    produce a text file with the model summary\n",
    "    \n",
    "    TODO: Add this to Eval Plugins\n",
    "    \n",
    "    log_model_summary(model=model,\n",
    "                  working_dir=working_dir,\n",
    "                  input_size=(1, data_config.channels, *data_config.image_size),\n",
    "                  full_summary=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if full_summary:\n",
    "        col_names=(\"kernel_size\", \"input_size\",\"output_size\", \"num_params\", \"mult_adds\")\n",
    "    else:\n",
    "        col_names=(\"input_size\",\"output_size\", \"num_params\")\n",
    "\n",
    "    model_summary = summary(model.cuda(),\n",
    "                            input_size=input_size,\n",
    "                            row_settings=('depth', 'var_names'),\n",
    "                            col_names=col_names,\n",
    "                            verbose=verbose)\n",
    "\n",
    "    if (model_name is None) and (hasattr(model, \"name\")):\n",
    "        model_name = model.name\n",
    "    if (model_name is None):\n",
    "        summary_path = os.path.join(working_dir, 'model', f'model_summary.txt')        \n",
    "    else:\n",
    "        summary_path = os.path.join(working_dir, 'model', f'{model_name}_model_summary.txt')\n",
    "    \n",
    "    os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(str(model_summary))\n",
    "        \n",
    "    return model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6210ee",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, config = cmdline_args([\"-res\", \"512\", \"-model\", \"resnet50\"])\n",
    "datamodule, model = load_data_and_model(config=config, task_id=0)\n",
    "\n",
    "# model\n",
    "# for k,v in model.named_children():\n",
    "#     print(k, type(v))\n",
    "# for k,v in model.backbone.named_children():\n",
    "#     print(k, type(v))\n",
    "# for k,v in model.backbone.backbone.named_children():\n",
    "#     print(k, type(v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634cafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchnorm_layers = {n:l for n, l in model.backbone.named_parameters() if \"bn\" in n}\n",
    "print(len(batchnorm_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in batchnorm_layers.items():\n",
    "    print(k, v.shape, v.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfeeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiTaskClassifierConfig()\n",
    "\n",
    "\n",
    "cfg = ClassifierConfig(in_features=model.out_features,\n",
    "                 num_classes=config.model.num_classes)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97beff10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_state_dict(state_dict):\n",
    "    s = 0\n",
    "    for _, v in state_dict.items():\n",
    "        s += v.sum()\n",
    "    return s\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.state_dict().items():\n",
    "    print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e32957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in model.backbone.named_children():\n",
    "for k, v in model.named_children():\n",
    "    print(k, type(v))\n",
    "#     print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.backbone.named_children():\n",
    "# for k, v in model.named_children():\n",
    "    print(k, type(v))\n",
    "#     print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500acf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.backbone.backbone.named_children():\n",
    "# for k, v in model.named_children():\n",
    "    print(k, type(v))\n",
    "#     print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03304f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.named_parameters():\n",
    "#     print(k, type(v))\n",
    "    print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e3c68",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, config = cmdline_args([\"-res\", \"512\", \"-model\", \"efficientnet_b3\", \"--pretrained\", \"imagenet\"])\n",
    "datamodule, model = load_data_and_model(config=config, task_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.backbone.backbone.layers:\n",
    "    print(l, type(getattr(model.backbone.backbone, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93186c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.backbone.blocks[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8651d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.backbone.blocks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b533cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a87139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch==1.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1,3,512,512)\n",
    "\n",
    "y = model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze_all_above = False\n",
    "stop_layer = \"blocks.6\"\n",
    "for k, v in model.backbone.named_parameters():\n",
    "    if stop_layer in k:\n",
    "        unfreeze_all_above = True\n",
    "    if unfreeze_all_above:\n",
    "        v.requires_grad = True\n",
    "    else:\n",
    "        v.requires_grad = False\n",
    "\n",
    "    print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.state_dict().items():\n",
    "    print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5839c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in model.backbone.named_children():\n",
    "for k, v in model.named_children():\n",
    "    print(k, type(v))\n",
    "#     print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.backbone.named_children():\n",
    "# for k, v in model.named_children():\n",
    "    print(k, type(v))\n",
    "#     print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.backbone.backbone.named_children():\n",
    "# for k, v in model.named_children():\n",
    "    print(k, type(v))\n",
    "#     print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946eb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.backbone.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.backbone.named_parameters():\n",
    "#     print(k, type(v))\n",
    "    print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e4b92",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e81a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def unfreeze_at(self, layer: str):\n",
    "        assert layer in ResNetBackbone.layers\n",
    "        self.model.requires_grad = True        \n",
    "        for name, param in model.named_parameters():\n",
    "            if layer in name:\n",
    "                break\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1,3,512,512)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3998af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.softmax(-1)(torch.rand(92) - y)\n",
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e829704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n,m in model.named_modules():\n",
    "    print(n, type(m))\n",
    "\n",
    "\n",
    "\n",
    "model.backbone\n",
    "\n",
    "plt.imshow(model.classifier.head.weight.grad)\n",
    "\n",
    "model.classifier.head.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f72fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.scripts.multitask.train import MultiTaskDataModule, LitMultiTaskModule, ImagePredictionLogger, train_task, cmdline_args, CIFAR10DataModule, run_multitask_test, load_data_and_model\n",
    "\n",
    "# args, config = cmdline_args([\"-res\", \"512\", \"-model\", \"efficientnet_b1_pruned\", \"--pretrained\", \"imagenet\"])\n",
    "# args, config = cmdline_args([\"-res\", \"512\", \"-model\", \"efficientnet_b2\", \"--pretrained\", \"imagenet\"])\n",
    "args, config = cmdline_args([\"-res\", \"512\", \"-model\", \"efficientnet_b3\", \"--pretrained\", \"imagenet\"])\n",
    "\n",
    "config.model\n",
    "\n",
    "datamodule, model = load_data_and_model(config=config, task_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_state_dict(state_dict):\n",
    "    s = 0\n",
    "    for _, v in state_dict.items():\n",
    "        s += v.sum()\n",
    "    return s\n",
    "\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5284e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.state_dict().items():\n",
    "    print(f\"v.requires_grad={v.requires_grad} | \", k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = model(torch.ones(1,3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bbddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier.head.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a749e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SOS_dict = sum_of_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SOS_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462bf3b",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Efficientnet_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d17305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.init.kaiming_uniform_\n",
    "\n",
    "model_summary = log_model_summary(model=model,\n",
    "                                  working_dir=\".\",\n",
    "                                  input_size=(1,3,512,512),\n",
    "                                  full_summary=True,\n",
    "                                  verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578ab5c",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Efficientnet_b1_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = log_model_summary(model=model,\n",
    "                                  working_dir=\".\",\n",
    "                                  input_size=(1,3,512,512),\n",
    "                                  full_summary=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46a754",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Efficientnet_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b657f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = log_model_summary(model=model,\n",
    "                                  working_dir=\".\",\n",
    "                                  input_size=(1,3,512,512),\n",
    "                                  full_summary=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2a435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ccb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_summary = log_model_summary(model=model,\n",
    "                                  working_dir=\".\",\n",
    "                                  input_size=(1,3,512,512),\n",
    "                                  full_summary=True,\n",
    "                                  verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940db503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e8579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5971686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in model.named_children():\n",
    "    print(k, type(v))\n",
    "\n",
    "for k,v in model.backbone.named_children():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d246f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,3,512,512).cpu()\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"model.backbone.features(x)\")\n",
    "x = model.backbone.features(x)\n",
    "print(f\"Output shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,3,512,512).cpu()\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"model.backbone.features(x)\")\n",
    "x = model.backbone.backbone(x)\n",
    "print(f\"Output shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945094ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.AdaptiveAvgPool2d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ab16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"pool(1)(x)\")\n",
    "x = pool(x)\n",
    "print(f\"Output shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6effc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"torch.flatten(x)\")\n",
    "x = torch.flatten(x, 1)\n",
    "print(f\"Output shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06370de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"classifier(x)\")\n",
    "x = model.classifier(x)\n",
    "print(f\"Output shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(x[0,:], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(8,3,512,512).cpu()\n",
    "for k,v in model.backbone.backbone.named_children():\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"layer: {k} - {type(v)}\")\n",
    "    x = v(x)\n",
    "    print(f\"Output shape: {x.shape}\")\n",
    "\n",
    "model.backbone.backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.AdaptiveAvgPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c31cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"layer: global_pool - {type(model.global_pool)}\")\n",
    "x = model.global_pool(x)\n",
    "print(f\"Output shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1280*1280*1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"layer: classifier - {type(model.classifier)}\")\n",
    "x = model.classifier(x)\n",
    "print(f\"Output shape: {x.shape}\")\n",
    "\n",
    "for k,v in model.backbone.backbone.named_modules():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1394af82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b131775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97f39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48379f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd3d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafe1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27928d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "# model_name = 'efficientnet_b3'\n",
    "model_name = 'efficientnet_b0'\n",
    "pretrained = \"imagenet\"\n",
    "\n",
    "# model = timm.create_model(model_name, pretrained=pretrained)\n",
    "# model_summary = log_model_summary(model=model,\n",
    "#                                   working_dir=\".\",\n",
    "#                                   input_size=(1,3,512,512),\n",
    "#                                   full_summary=True,\n",
    "#                                   verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "timm.models.efficientnet.default_cfgs\n",
    "\n",
    "\n",
    "# torch.nn.Dropout(p=0.5, inplace=False)\n",
    "# in_features = self.model.get_classifier().in_features\n",
    "# self.model.classifier = nn.Linear(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72cf266",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)\n",
    "\n",
    "model.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)\n",
    "\n",
    "dict(model.named_children()).keys()\n",
    "\n",
    "model.conv_stem\n",
    "\n",
    "??model\n",
    "\n",
    "for n, m in model.named_children():\n",
    "    print(n, type(m))\n",
    "    if isinstance(m, nn.modules.container.Sequential):\n",
    "        for n2, m2 in m.named_children():\n",
    "            print(n, n2, type(m2))\n",
    "\n",
    "for n,m in model.blocks.named_children():\n",
    "    print(n, type(m))\n",
    "\n",
    "from lightning_hydra_classifiers.models.backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.models.backbones.resnet import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_backbone = resnet50()\n",
    "\n",
    "resnet_backbone\n",
    "\n",
    "\n",
    "\n",
    "model_summary = log_model_summary(model=resnet_backbone,\n",
    "                                  working_dir=\".\",\n",
    "                                  input_size=(1,3,512,512),\n",
    "                                  full_summary=True,\n",
    "                                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1,3,512,512)\n",
    "\n",
    "self = resnet_backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d43b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.eval()\n",
    "self.requires_grad = False\n",
    "x = x.cuda()\n",
    "print(x.shape)\n",
    "x = self.stem(x)\n",
    "print(x.shape)\n",
    "x = self.layer1(x)\n",
    "print(x.shape)\n",
    "x = self.layer2(x)\n",
    "print(x.shape)\n",
    "x = self.layer3(x)\n",
    "print(x.shape)\n",
    "x = self.layer4(x)\n",
    "print(x.shape)\n",
    "x = self.avgpool(x)\n",
    "print(x.shape)\n",
    "x = torch.flatten(x, 1)\n",
    "print(x.shape)\n",
    "# x = x.view(-1, self.out_features)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.fc(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b27703",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in resnet_backbone.named_children():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_backbone.fc\n",
    "\n",
    "model_summary = log_model_summary(model=model,\n",
    "                                  working_dir=\".\",\n",
    "                                  input_size=(1,3,512,512),\n",
    "                                  full_summary=True,\n",
    "                                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cb108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a1d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.models.backbones import backbone\n",
    "from lightning_hydra_classifiers.utils.metric_utils import get_per_class_metrics, get_scalar_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb284d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = backbone.build_model(model_name=config.model.model_name,\n",
    "#                                    pretrained=config.model.pretrained,\n",
    "#                                    num_classes=1000)#config.model.num_classes)\n",
    "# # head_key = self.get_default_classifier_key(model)\n",
    "\n",
    "# dict(model.named_children()).keys()\n",
    "\n",
    "# LitMultiTaskModule.get_default_classifier_key(model)\n",
    "\n",
    "# # dict(model.model.named_children()).keys()\n",
    "\n",
    "# dict(model.model.named_children())['avgpool']\n",
    "\n",
    "# layers = OrderedDict(model.named_children())\n",
    "# self.backbone = nn.Sequential(OrderedDict({k:v for k,v in layers.items() if k not in [\"global_pool\", \"classifier\"]}))\n",
    "# self.global_pool = layers[\"global_pool\"]\n",
    "# self.classifier = layers[head_key]\n",
    "# self.model = nn.Sequential(OrderedDict({\"backbone\":self.backbone,\n",
    "#                                         \"global_pool\":self.global_pool,\n",
    "#                                         \"classifier\":self.classifier}))\n",
    "\n",
    "# self.freeze_up_to(layer=config.init_freeze_up_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7058fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "datamodule.setup()\n",
    "\n",
    "\n",
    "datamodule.train_dataset.config\n",
    "\n",
    "model.config\n",
    "\n",
    "# from pytorch_lightning.loggers import CSVLogger\n",
    "csv_logger = pl.loggers.CSVLogger(\"logs\", name=\"unittest_transfer_experiment\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "#                 limit_train_batches=0.1,\n",
    "#                 limit_val_batches=0.1,\n",
    "#             resume_from_checkpoint=config.trainer.resume_from_checkpoint,\n",
    "#             max_epochs=config.trainer.num_epochs,\n",
    "            gpus=1,#config.trainer.gpus,\n",
    "            auto_lr_find=True,#config.stages.lr_tuner,\n",
    "            precision=16,\n",
    "            logger=csv_logger) #config.trainer.precision)#,\n",
    "#             callbacks=[earlystopping,\n",
    "#                        checkpoint_callback,\n",
    "#                        img_prediction_callback],\n",
    "#                 overfit_batches=5,\n",
    "\n",
    "#             logger=wandb_logger,\n",
    "#                 track_grad_norm=2,\n",
    "#             weights_summary='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.scripts.pretrain import lr_tuner\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from lightning_hydra_classifiers.utils.template_utils import get_logger\n",
    "logger = get_logger(name=__name__)\n",
    "task_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962f452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-PNAS-to-PNAS_efficientnet_b1_imagenet_weights/checkpoints/task_0/epoch=08-val_loss=1.3451-val_acc=0.6087.ckpt\"\n",
    "\n",
    "# args, config = cmdline_args([\"-ckpt\", ckpt_path, \"-res\", \"512\"])\n",
    "\n",
    "# DEFAULT_CONFIG = lr_tuner.DEFAULT_CONFIG\n",
    "\n",
    "# tuner_config = OmegaConf.create({\"lr_tuner\":DEFAULT_CONFIG})\n",
    "# tuner_config\n",
    "\n",
    "# \"lr_tuner\" in config.keys()\n",
    "\n",
    "# config.lr_tuner = dict(min_lr=2e-05, num_training=150)\n",
    "# config\n",
    "\n",
    "# if \"lr_tuner\" in config:\n",
    "#     logger.info(f\"Proceeding with overrides merged with default parameters\")\n",
    "#     logger.info(f\"overrides: {config.lr_tuner}\")\n",
    "#     logger.info(f\"defaults: {tuner_config}\")\n",
    "#     tuner_config = OmegaConf.merge(DEFAULT_CONFIG, dict(config.lr_tuner))\n",
    "# else:\n",
    "#     logger.info(f\"Proceeding with default parameters:{tuner_config}\")\n",
    "#     for k, v in DEFAULT_CONFIG.items():\n",
    "#         if k in config:\n",
    "#             tuner_config.update({k:config[k]})\n",
    "            \n",
    "#     config.lr_tuner = tuner_config            \n",
    "# logger.info(f\"tuner_config={tuner_config}\")\n",
    "# logger.info(f\"config={config}\")\n",
    "# # config = OmegaConf.create(tuner_config)\n",
    "\n",
    "# print(f\"tuner_config={tuner_config}\")\n",
    "# print(f\"config={config}\")\n",
    "\n",
    "# tuner_config\n",
    "\n",
    "# config\n",
    "\n",
    "from collections import OrderedDict\n",
    "# # pp(dict(model.model.model.named_children()).keys())\n",
    "\n",
    "# layers = OrderedDict({k:v for k,v in model.model.model.named_children()})# if k not in [\"pool\", \"classifier\"]})\n",
    "# print(layers.keys())\n",
    "# backbone = OrderedDict({k:v for k,v in layers.items() if k not in [\"global_pool\", \"classifier\"]})\n",
    "# global_pool = layers[\"global_pool\"]\n",
    "# classifier = layers[\"classifier\"]\n",
    "\n",
    "# print(global_pool, classifier)\n",
    "# # [(k, v) for k,v in model.model.model.named_children()]\n",
    "# # backbone = nn.Sequential(*[layer for name, layer in layers[:-2]])\n",
    "# layers.keys()\n",
    "\n",
    "# dir(global_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd2951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50df657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_params_requires_grad(model, prefix=''):\n",
    "    for name, p in model.named_parameters():\n",
    "        print(prefix + name, type(p), p.shape, f'requires_grad=={p.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d51891",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.lr_tuner = OmegaConf.structured(lr_tuner.LRTunerConfig(\n",
    "                                        min_lr = 1e-07,\n",
    "                                        max_lr = 1.5,\n",
    "                                        num_training = 30,\n",
    "                                        mode = 'exponential',\n",
    "                                        early_stop_threshold = 8.0\n",
    "                ))\n",
    "\n",
    "pp(config.lr_tuner)\n",
    "\n",
    "logger.info(f\"[Initiating Stage] lr_tuner\")\n",
    "suggestion, lr_tuner_results, config, tuner = lr_tuner.run_lr_tuner(trainer=trainer,\n",
    "                                                             model=model,\n",
    "                                                             datamodule=datamodule,\n",
    "                                                             config=config,\n",
    "                                                             results_dir = config.stages[f\"task_{task_id}\"].lr_tuner_dir,\n",
    "                                                             group=f'{config.model.model_name}_task_{task_id}')\n",
    "#                                                                      run=run)\n",
    "\n",
    "# suggestion#,\n",
    "\n",
    "# lr_tuner_results#,\n",
    "# config#\n",
    "# tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -alh \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-PNAS-to-PNAS_resnet50_imagenet_weights/task_0/lr_tuner\"\n",
    "\n",
    "!rm \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-PNAS-to-PNAS_resnet50_imagenet_weights/task_0/lr_tuner/hparams.yaml\"\n",
    "\n",
    "# suggestion#,\n",
    "lr_tuner_results\n",
    "pp(dict(config))\n",
    "\n",
    "\n",
    "\n",
    "print(x.shape)\n",
    "x = model.backbone(x)\n",
    "print(x.shape)\n",
    "x = model.global_pool(x)\n",
    "print(x.shape)\n",
    "x = x.view(x.size(0),-1)\n",
    "print(x.shape)\n",
    "x = model.classifier(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7062272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f49e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be5e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030a4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700417c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929b2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c7502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcd14027",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### scratch (Thur Sept 9th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93bfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_idx = 9\n",
    "\n",
    "# datamodule.set_task(0)\n",
    "# # datamodule.setup(\"fit\")\n",
    "# # datamodule.setup(\"test\")\n",
    "\n",
    "# item = datamodule.current_task['test'][sample_idx]\n",
    "\n",
    "# toPIL(item.image)\n",
    "\n",
    "# item = datamodule.current_task['test'].fetch_item(sample_idx)\n",
    "# item.image\n",
    "\n",
    "# datamodule.set_task(1)\n",
    "# # datamodule.setup(\"fit\")\n",
    "# # datamodule.setup(\"test\")\n",
    "# # datamodule.current_task['test'][0]\n",
    "\n",
    "# item = datamodule.current_task['test'][sample_idx]\n",
    "# toPIL(item.image)\n",
    "\n",
    "# item = datamodule.current_task['test'].fetch_item(sample_idx)\n",
    "# item.image\n",
    "\n",
    "# dir(MultiTaskDataModule)\n",
    "\n",
    "# MultiTaskDataModule._MultiTaskDataModule__setup(datamodule)\n",
    "\n",
    "# # datamodule.setup(stage=\"fit\")\n",
    "\n",
    "# datamodule.__setup()#stage=)\n",
    "\n",
    "# datamodule.test_dataset\n",
    "# # datamodule.train_dataset\n",
    "\n",
    "\n",
    "# datamodule.has_setup_fit\n",
    "# datamodule.has_setup_predict\n",
    "# datamodule.has_setup_test\n",
    "# datamodule.setup(stage=\"test\")\n",
    "\n",
    "# datamodule.test_dataset\n",
    "# datamodule.train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ba961",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup() #\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, config = cmdline_args([])#[\"--load_from_checkpoint\", \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/Extant-PNAS-to-PNAS_efficientnet_b1_imagenet_weights/checkpoints/task_0/epoch=08-val_loss=1.3451-val_acc=0.6087.ckpt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b35f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.stages.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21dab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22505ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.utils.template_utils import get_logger\n",
    "############################################\n",
    "logger = get_logger(name=__name__)\n",
    "\n",
    "\n",
    "\n",
    "def run_multitask_test(trainer: pl.Trainer,\n",
    "                       model: pl.LightningModule,\n",
    "                       datamodule: pl.LightningDataModule,\n",
    "                       config: argparse.Namespace=None,\n",
    "                       tasks: Union[str, List[int]]=\"all\",\n",
    "                       results_path: str=None,\n",
    "                       run=None):\n",
    "    if tasks == \"all\":\n",
    "        tasks = list(range(len(datamodule.tasks)))\n",
    "    \n",
    "    test_results = {}\n",
    "    fig, ax = plt.subplots(1,len(tasks))\n",
    "    for task_id in tasks:\n",
    "#         datamodule.set_task(task_id)\n",
    "        tag=config.stages[f\"task_{task_id}\"].name\n",
    "#         model.init_metrics(stage='test', tag=tag)\n",
    "        trainer.logger = pl.loggers.CSVLogger(\"logs\", name=tag)\n",
    "        datamodule.setup(task_id=task_id)\n",
    "        logger.info(f\"[TESTING] {tag}\")\n",
    "        ax[task_id].imshow(datamodule.test_dataset.fetch_item(0)[0])\n",
    "        ax[task_id].set_title(f\"{tag}\")\n",
    "        test_results[task_id] = trainer.test(model, datamodule=datamodule)\n",
    "        \n",
    "    return test_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a5391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a32ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219702b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AdamWOptimizerConfig:\n",
    "    lr: float = 0.001\n",
    "    betas: Tuple[float] = (0.9, 0.999)\n",
    "    eps: float = 1e-08\n",
    "    weight_decay: float = 0.01\n",
    "    amsgrad: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_config = AdamWOptimizer()\n",
    "opt_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebf04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = run_multitask_test(trainer=trainer,\n",
    "                                   model=model,\n",
    "                                   datamodule=datamodule,\n",
    "                                   config=config,\n",
    "                                   tasks=\"all\",\n",
    "                                   results_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_reference = f'{entity}/{project}/model-{run_id}:best'\n",
    "checkpoint_reference\n",
    "\n",
    "run = wandb.init(project='MNIST')\n",
    "artifact = run.use_artifact(checkpoint_reference, type='model')\n",
    "artifact_dir = artifact.download()\n",
    "artifact_dir\n",
    "\n",
    "from pathlib import Path\n",
    "list(Path(artifact_dir).glob('*'))\n",
    "\n",
    "model = MNIST_LitModule.load_from_checkpoint(Path(artifact_dir)/'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21adfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9688968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768eaded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5204bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dir(datamodule)\n",
    "\n",
    "datamodule.train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3adf14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e5f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "n_iter=1000\n",
    "for i in range(n_iter):\n",
    "    Batch = namedtuple(\"Batch\", (\"image\", \"target\", \"path\", \"catalog_number\"))\n",
    "    batch = Batch(image=f\"image_{i}\", target=i, path=\"path.jpg\", catalog_number=\"cat_num\")\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b184c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "n_iter=1000\n",
    "Batch = namedtuple(\"Batch\", (\"image\", \"target\", \"path\", \"catalog_number\"))\n",
    "for i in range(n_iter):\n",
    "    batch = Batch(image=f\"image_{i}\", target=i, path=\"path.jpg\", catalog_number=\"cat_num\")\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67142a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=1000\n",
    "Batch = namedtuple(\"Batch\", (\"image\", \"target\", \"path\", \"catalog_number\"), defaults=(None,)*4)\n",
    "# for i in range(n_iter):\n",
    "i=0\n",
    "batch = Batch(image=f\"image_{i}\", target=i, path=\"path.jpg\")#, catalog_number=\"cat_num\")\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f96bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec017861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.path.isfile(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29576e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validating model's layer freezing function, model.freeze_up_to(layer={config.model.init_freeze_up_to})\")\n",
    "[print(f\"{name}.requires_grad: {param.requires_grad}\") for i, (name, param) in enumerate(model.model.named_parameters())]\n",
    "\n",
    "[print(f\"{name}.requires_grad: {param.requires_grad}\") for i, (name, param) in enumerate(model.model.named_children())]\n",
    "    \n",
    "\n",
    "# for i, (name, param) in enumerate(model.model.named_parameters()):\n",
    "#     print(f\"{name}.requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/CIFAR10_resnet50/checkpoints/task_0/epoch=23-val_loss=0.1810-val_acc=0.9528.ckpt\"\n",
    "loaded_model = LitMultiTaskModule.load_from_checkpoint(best_ckpt_path)\n",
    "\n",
    "loaded_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c8d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e94498",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d01fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = list(model.parameters())[-1]\n",
    "loaded_layer = list(loaded_model.parameters())[-1]\n",
    "print(loaded_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4729f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc807b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/CIFAR10_resnet50/checkpoints/task_0/epoch=23-val_loss=0.1810-val_acc=0.9528.ckpt\"\n",
    "loaded_model = model.load_from_checkpoint(best_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/CIFAR10_resnet50/checkpoints/task_0/epoch=23-val_loss=0.1810-val_acc=0.9528.ckpt\"\n",
    "\n",
    "# loaded_model = model.load_from_checkpoint(best_ckpt_path)\n",
    "loaded_ckpt = torch.load(best_ckpt_path)\n",
    "print(type(loaded_ckpt))\n",
    "print(loaded_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitMultiTaskModule(config.model)\n",
    "\n",
    "model.CHECKPOINT_HYPER_PARAMS_KEY\n",
    "\n",
    "model.CHECKPOINT_HYPER_PARAMS_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.CHECKPOINT_HYPER_PARAMS_TYPE\n",
    "\n",
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91243166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28492199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10 #Caltech101\n",
    "\n",
    "dataset = CIFAR10(root=default_root_dir, download=True)#, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root=default_root_dir, train=False)#, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CIFAR10DataModule(task_id=0,\n",
    "                               batch_size=128,\n",
    "                               image_size=224,\n",
    "                               image_buffer_size=32,\n",
    "                               num_workers=4,\n",
    "                               pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be980171",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e28330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "torchmetrics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba26477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "torchmetrics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/CIFAR10_resnet50/image_classification_train/mgxq48fo/checkpoints\"\n",
    "\n",
    "!ls -alh \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/CIFAR10_resnet50/image_classification_train/mgxq48fo/checkpoints/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9118fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "path = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs/Transfer_Experiments/CIFAR10--None_resnet50/lr_finder/lr_tuner_results_loss-vs-lr.png\"\n",
    "img = wandb.Image(path, caption=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903501f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(img)\n",
    "\n",
    "img._image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "OmegaConf.load(OmegaConf.to_yaml({\"test\":490,\n",
    "                  \"suggestion\":0.567,\n",
    "                  \"find\":\"it\"}))\n",
    "\n",
    "\n",
    "\n",
    "toPIL(datamodule.train_dataset[4][0]) \n",
    "\n",
    "\n",
    "\n",
    "toPIL(datamodule.test_dataset[4][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datamodule.test_dataset[4][0].min(), datamodule.test_dataset[4][0].max())\n",
    "\n",
    "idx = 100\n",
    "\n",
    "img = datamodule.test_dataset[idx][0]\n",
    "label = datamodule.test_dataset[idx][1]\n",
    "\n",
    "print(img.min(), img.max())\n",
    "\n",
    "img = (img - img.min()) / (img.max() - img.min())\n",
    "print(f\"label = {datamodule.label_encoder.idx2class[label]}\")\n",
    "print(img.min(), img.max())\n",
    "\n",
    "toPIL(img).resize((224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 8\n",
    "\n",
    "test_transform = datamodule.test_dataset.transform\n",
    "datamodule.test_dataset.transform = None\n",
    "\n",
    "img = totensor(datamodule.test_dataset[idx][0])\n",
    "label = datamodule.test_dataset[idx][1]\n",
    "\n",
    "# print(img.min(), img.max())\n",
    "# img = (img - img.min()) / (img.max() - img.min())\n",
    "print(f\"label = {datamodule.label_encoder.idx2class[label]}\")\n",
    "# print(img.min(), img.max())\n",
    "# img_thumb = toPIL(img)\n",
    "\n",
    "toPIL(img).resize((224,224))\n",
    "datamodule.test_dataset.transform = test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_thumb.resize((224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adce9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.test_dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "toPIL(datamodule.val_dataset[4][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370992a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.train_dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892cd97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.val_dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(datamodule.val_dataset.dataset.__getitem__))\n",
    "\n",
    "print(inspect.getsource(torch.utils.data.dataset.Subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfff74",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Datasets & DataModules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning_hydra_classifiers.experiments.transfer_experiment import TransferExperiment\n",
    "# class PlantDataModule(pl.LightningDataModule):\n",
    "# #     valid_tasks = (0, 1)\n",
    "    \n",
    "#     def __init__(self, \n",
    "#                  batch_size,\n",
    "#                  task_id: int=0,\n",
    "#                  image_size: int=224,\n",
    "#                  image_buffer_size: int=32,\n",
    "#                  num_workers: int=4,\n",
    "#                  pin_memory: bool=True):\n",
    "#         super().__init__()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_workers = num_workers\n",
    "#         self.pin_memory = pin_memory\n",
    "        \n",
    "        \n",
    "#         self.experiment = TransferExperiment()\n",
    "#         self.set_task(task_id)        \n",
    "        \n",
    "#         self.image_size = image_size\n",
    "#         self.image_buffer_size = image_buffer_size\n",
    "#         self.mean = [0.485, 0.456, 0.406]\n",
    "#         self.std = [0.229, 0.224, 0.225]\n",
    "#         # Train augmentation policy\n",
    "        \n",
    "#         self.__init_transforms()\n",
    "                \n",
    "#         self.tasks = self.experiment.get_multitask_datasets(train_transform=self.train_transform,\n",
    "#                                                             val_transform=self.val_transform)\n",
    "\n",
    "\n",
    "#     def __init_transforms(self):\n",
    "        \n",
    "#         self.train_transform = transforms.Compose([\n",
    "#             transforms.RandomResizedCrop(size=self.image_size,\n",
    "#                                          scale=(0.25, 1.2),\n",
    "#                                          ratio=(0.7, 1.3),\n",
    "#                                          interpolation=2),\n",
    "#             torchvision.transforms.ToTensor(),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.Normalize(self.mean, self.std),\n",
    "#             transforms.Grayscale(num_output_channels=3)\n",
    "#         ])\n",
    "\n",
    "#         self.val_transform = transforms.Compose([\n",
    "#             transforms.Resize(self.image_size+self.image_buffer_size),\n",
    "#             torchvision.transforms.ToTensor(),\n",
    "#             transforms.CenterCrop(self.image_size),\n",
    "#             transforms.Normalize(self.mean, self.std),\n",
    "#             transforms.Grayscale(num_output_channels=3)            \n",
    "#         ])\n",
    "\n",
    "#     def set_task(self, task_id: int):\n",
    "#         assert task_id in self.experiment.valid_tasks\n",
    "#         self.task_id = task_id\n",
    "        \n",
    "        \n",
    "        \n",
    "#     @property\n",
    "#     def current_task(self):\n",
    "#         return self.tasks[self.task_id]\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         task = self.current_task\n",
    "#         # Assign train/val datasets for use in dataloaders\n",
    "#         if stage == 'fit' or stage is None:\n",
    "#             self.train_dataset = task['train']\n",
    "#             self.val_dataset = task['val']\n",
    "            \n",
    "#             self.classes = self.train_dataset.classes\n",
    "#             self.num_classes = len(self.train_dataset.label_encoder)\n",
    "            \n",
    "#         elif stage == 'test':\n",
    "#             self.test_dataset = task['test']\n",
    "                        \n",
    "#     def train_dataloader(self):\n",
    "#         return torch.utils.data.DataLoader(self.train_dataset,\n",
    "#                           batch_size=self.batch_size,\n",
    "#                           pin_memory=self.pin_memory,\n",
    "#                           shuffle=True,\n",
    "#                           num_workers=self.num_workers,\n",
    "#                           drop_last=True)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return torch.utils.data.DataLoader(self.val_dataset,\n",
    "#                           batch_size=self.batch_size,\n",
    "#                           pin_memory=self.pin_memory,\n",
    "#                           num_workers=self.num_workers)\n",
    "    \n",
    "#     def test_dataloader(self):\n",
    "#         return torch.utils.data.DataLoader(self.test_dataset,\n",
    "#                           batch_size=self.batch_size,\n",
    "#                           pin_memory=self.pin_memory,\n",
    "#                           num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19168f70",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Model & LightningModules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df247f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomResNet(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  num_classes: int,\n",
    "#                  model_name='resnet18',\n",
    "#                  pretrained=False):\n",
    "#         super().__init__()\n",
    "#         self.num_classes = num_classes\n",
    "#         self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "#         self.in_features = self.model.get_classifier().in_features\n",
    "#         self.model.fc = nn.Linear(self.in_features, self.num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.model(x)\n",
    "#         return x\n",
    "\n",
    "# class LitMultiTaskModule(pl.LightningModule):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "#         self.lr = config['lr']\n",
    "#         self.num_classes = config['num_classes']\n",
    "# #         self.save_hyperparameters()\n",
    "#         self._init_model(config)\n",
    "#         self.metrics = self._init_metrics(stage='all')\n",
    "# #         self.metric = pl.metrics.F1(num_classes=CONFIG['num_classes'])\n",
    "#         self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "#     def forward(self, x, *args, **kwargs):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "#         self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.config['t_max'], eta_min=self.config['min_lr'])\n",
    "\n",
    "#         return {'optimizer': self.optimizer, 'lr_scheduler': self.scheduler}\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         image = batch[0]\n",
    "#         target = batch[1]\n",
    "#         output = self.model(image)\n",
    "#         loss = self.criterion(output, target)\n",
    "# #         scores = self.metrics_train(output.argmax(1), target)\n",
    "#         scores = self.metrics_train(output, target)\n",
    "#         self.log_dict({\"train_loss\": loss, 'lr': self.optimizer.param_groups[0]['lr']},\n",
    "#                       on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         self.log_dict(scores,\n",
    "#                       on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         image = batch[0]\n",
    "#         target = batch[1]\n",
    "#         output = self.model(image)\n",
    "#         loss = self.criterion(output, target)\n",
    "# #         scores = self.metrics_val(output.argmax(1), target)\n",
    "#         scores = self.metrics_val(output, target)\n",
    "        \n",
    "#         self.log(\"val_loss\", loss,\n",
    "#                   on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         self.log_dict(scores,\n",
    "#                       on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "#         return loss\n",
    "    \n",
    "    \n",
    "#     def _init_model(self, config):\n",
    "#         self.model =  CustomResNet(config[\"num_classes\"],\n",
    "#                                    model_name=config[\"model_name\"],\n",
    "#                                    pretrained=config[\"pretrained\"])\n",
    "    \n",
    "#     def _init_metrics(self, stage: str='train'):\n",
    "        \n",
    "#         if stage in ['train', 'all']:\n",
    "#             self.metrics_train = get_scalar_metrics(num_classes=self.num_classes, average='macro', prefix='train')\n",
    "# #             self.metrics_train_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix='train')\n",
    "            \n",
    "#         if stage in ['val', 'all']:\n",
    "#             self.metrics_val = get_scalar_metrics(num_classes=self.num_classes, average='macro', prefix='val')\n",
    "# #             self.metrics_val_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix='val')\n",
    "            \n",
    "#         if stage in ['test', 'all']:\n",
    "#             self.metrics_test = get_scalar_metrics(num_classes=self.num_classes, average='macro', prefix='test')\n",
    "# #             self.metrics_test_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix='test')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d92eae",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Define & Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4b8a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     \"model\":\n",
    "#         {\"backbone\":{\n",
    "#                  \"name\":'resnet50',\n",
    "#                  \"pretrained\":True},\n",
    "config = Munch({\n",
    "    \"seed\":42,\n",
    "    \"model_name\":'resnet50',\n",
    "    \"pretrained\":True,\n",
    "    \"image_size\": 224,\n",
    "    \"image_buffer_size\": 32, \n",
    "    \"num_classes\": None,\n",
    "    \"lr\": 5e-4,\n",
    "    \"min_lr\": 1e-6,\n",
    "    \"t_max\": 20,\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "#     accum = 1,\n",
    "    \"precision\": 16,\n",
    "    \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    \"num_workers\": 4,\n",
    "    \"pin_memory\": True\n",
    "})\n",
    "\n",
    "\n",
    "# Seed everything\n",
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "p = argparse.ArgumentParser(description=\"\")\n",
    "model_parser = p.add_parser('model', help='parser one')\n",
    "\n",
    "\n",
    "# subparsers = p.add_subparsers()\n",
    "# model_parser = subparsers.add_parser('model', help='parser one')\n",
    "# subparser_one = p.add_subparsers()\n",
    "\n",
    "\n",
    "print(p.parse_args([\"-h\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283dbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from munch import Munch\n",
    "from rich import print as pp\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\")\n",
    "subparsers = parser.add_subparsers()\n",
    "\n",
    "p = subparsers.add_parser('', help='model args')\n",
    "# subparser_one = parser_one.add_subparsers()\n",
    "p.add_argument(\"-o\", \"--output_dir\", dest=\"output_dir\", type=str,\n",
    "               default=\"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/experiment_logs\",\n",
    "               help=\"Output root directory for experiment logs.\")\n",
    "p.add_argument(\"-e\", \"--num_epochs\", dest=\"num_epochs\", type=int, default=10,\n",
    "               help=\"Number of training epochs\")\n",
    "p.add_argument(\"-res\", \"--image_size\", dest=\"image_size\", type=int, default=224,\n",
    "               help=\"image_size/model input resolution\")\n",
    "p.add_argument(\"-buffer\", \"--image_buffer_size\", dest=\"image_buffer_size\", type=int, default=32,\n",
    "               help=\"Additional resolution to resize images before either random (train) or center (test) crop to image_size.\")\n",
    "p.add_argument(\"-bsz\", \"--batch_size\", dest=\"batch_size\", type=int, default=32,\n",
    "               help=\"batch_size\")\n",
    "p.add_argument(\"-nproc\", \"--num_workers\", dest=\"num_workers\", type=int, default=4,\n",
    "               help=\"num_workers per dataloader\")\n",
    "\n",
    "p.add_argument(\"-model\", \"--model_name\", dest=\"model_name\", type=str, default=\"resnet50\",\n",
    "               help=\"model backbone architecture\")\n",
    "p.add_argument(\"-pre\", \"--pretrained\", dest=\"pretrained\", choices=[\"imagenet\", True, False],\n",
    "               help=\"Use pretrained imagenet weights or randomly initialize from scratch.\")\n",
    "p.add_argument(\"-lr\", \"--learning_rate\", dest=\"lr\", type=float, default=3e-4,\n",
    "               help=\"Initial learning rate.\")\n",
    "\n",
    "args = parser.parse_args([\"\"])\n",
    "\n",
    "config = Munch({\n",
    "    \"seed\":42,\n",
    "    \"num_epochs\": args.num_epochs,\n",
    "    \"precision\": 16,\n",
    "    \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "})\n",
    "\n",
    "config.data = {\n",
    "    \"image_size\": args.image_size,\n",
    "    \"image_buffer_size\": args.image_buffer_size,\n",
    "    \"batch_size\": args.batch_size,\n",
    "    \"num_workers\": args.num_workers,\n",
    "    \"pin_memory\": True\n",
    "}\n",
    "config.model = {\"model_name\": args.model_name,\n",
    "                \"pretrained\": args.pretrained,\n",
    "                \"lr\": args.lr,\n",
    "                \"num_classes\": None,\n",
    "                \"t_max\": 20,\n",
    "                \"min_lr\": 1e-6}\n",
    "\n",
    "config.callbacks = {\"monitor\": {\"metric\":\"val_loss\",\n",
    "                                \"mode\": \"min\"}\n",
    "                   }\n",
    "\n",
    "out = json.dumps(config, default=str)\n",
    "# pp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05430d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# dir(collections)\n",
    "argparse.Namespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa92e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = PlantDataModule(batch_size=config.batch_size,\n",
    "                             task_id=0,\n",
    "                             image_size=config.image_size,\n",
    "                             image_buffer_size=config.image_buffer_size,\n",
    "                             num_workers=config.num_workers,\n",
    "                             pin_memory=config.pin_memory)\n",
    "\n",
    "datamodule.setup(\"fit\")\n",
    "config.num_classes = datamodule.num_classes\n",
    "\n",
    "pp(config)\n",
    "model = LitMultiTaskModule(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8f599",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddde33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss',\n",
    "                                      save_top_k=1,\n",
    "                                      save_last=True,\n",
    "                                      save_weights_only=True,\n",
    "                                      filename='checkpoint/{epoch:02d}-{val_loss:.4f}-{val_f1:.4f}',\n",
    "                                      verbose=True,\n",
    "                                      mode='min')\n",
    "earlystopping = pl.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa015c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = pl.loggers.WandbLogger(entity = \"jrose\",\n",
    "                           project = \"image_classification_train\",\n",
    "                           job_type = \"train_supervised\",\n",
    "                           config=config,\n",
    "                           group='ResNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1dc03e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "            limit_train_batches=0.1,\n",
    "            limit_val_batches=0.1,\n",
    "            max_epochs=config['num_epochs'],\n",
    "            gpus=1,\n",
    "#             accumulate_grad_batches=CONFIG['accum'],\n",
    "            precision=config['precision'],\n",
    "            callbacks=[earlystopping,\n",
    "                       checkpoint_callback],\n",
    "#                        ImagePredictionLogger(val_samples)],\n",
    "#             checkpoint_callback=checkpoint_callback,\n",
    "            logger=wandb_logger,\n",
    "            weights_summary='top')\n",
    "\n",
    "# datamodule.train_dataset[0]\n",
    "type(datamodule.val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# model_stats = summary(your_model, (1, 3, 28, 28), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7c449",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model ⚡🚅⚡\n",
    "trainer.fit(model, datamodule)\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish() \n",
    "\n",
    "# datamodule.num_classes\n",
    "# sorted(datamodule.classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from pprint import pprint\n",
    "model_names = timm.list_models(\"effi*net*\") #'*resne*t*')\n",
    "pprint(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from pprint import pprint\n",
    "model_names = timm.list_models(\"senet*\") #'*resne*t*')\n",
    "pprint(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.models.backbones.backbone import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "dir(inspect)\n",
    "print(inspect.getsource(build_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ae868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe3e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d038535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4b166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88968816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from typing import *\n",
    "def log_model_summary(model: nn.Module,\n",
    "                      working_dir: str,\n",
    "                      input_size: Tuple[int],\n",
    "                      full_summary: bool=True,\n",
    "                      verbose: bool=1):\n",
    "    \"\"\"\n",
    "    produce a text file with the model summary\n",
    "    \n",
    "    TODO: Add this to Eval Plugins\n",
    "    \n",
    "    log_model_summary(model=model,\n",
    "                  working_dir=working_dir,\n",
    "                  input_size=(1, data_config.channels, *data_config.image_size),\n",
    "                  full_summary=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if full_summary:\n",
    "        col_names=(\"kernel_size\", \"input_size\",\"output_size\", \"num_params\", \"mult_adds\")\n",
    "    else:\n",
    "        col_names=(\"input_size\",\"output_size\", \"num_params\")\n",
    "\n",
    "    model_summary = summary(model.cuda(),\n",
    "                            input_size=input_size,\n",
    "                            row_settings=('depth', 'var_names'),\n",
    "                            col_names=col_names,\n",
    "                            verbose=verbose)\n",
    "\n",
    "    model_summary_file = os.path.join(working_dir, 'model', f'{MODEL_NAME}_model_summary.txt')\n",
    "    os.makedirs(os.path.dirname(model_summary_file), exist_ok=True)\n",
    "\n",
    "    with open(model_summary_file, \"w\") as f:\n",
    "        f.write(str(model_summary))\n",
    "        \n",
    "    return model_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f9443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.models.backbones.backbone import build_model\n",
    "\n",
    "# m_resnet = build_model(\"resnet50\", pretrained=True)\n",
    "# m_efficient = build_model(\"efficientnet_b2\", pretrained=True)\n",
    "# m_se_resnet = build_model(\"se_resnet152\", pretrained=True)\n",
    "\n",
    "model_names = [\"resnet50\", \"efficientnet_b2\", \"se_resnet152\"]\n",
    "models = {}\n",
    "\n",
    "for name in model_names:\n",
    "    models[name] = build_model(name, pretrained=True)\n",
    "\n",
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b19012",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(models[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72554ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6869816",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ce27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[\"se_resnet152\"]\n",
    "working_dir = \"./se_resnet152\"\n",
    "\n",
    "log_model_summary(model=model,\n",
    "                  working_dir=working_dir,\n",
    "                  input_size=(1, 3,224,224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "m = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "m.eval()\n",
    "\n",
    "dir(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "m = timm.create_model('senet154', pretrained=True)\n",
    "m.eval()\n",
    "\n",
    "dir(m)\n",
    "\n",
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87674ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.feature_info\n",
    "\n",
    "# import timm\n",
    "# m = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
    "# m.eval()\n",
    "\n",
    "m.default_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481cd40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34385b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e71129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.utils.common_utils import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "experiment = TransferExperiment()\n",
    "\n",
    "task_0 = experiment.setup_task_0()\n",
    "\n",
    "task_0['train'].label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_0['val'].label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52134f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_0['test'].label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4307fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_class_indices = {\"Nothofagaceae\":\"Fagaceae\"}\n",
    "task_0_label_encoder = task_0['train'].label_encoder\n",
    "print(task_0_label_encoder)\n",
    "\n",
    "task_0_label_encoder.__init__(replace = replace_class_indices)\n",
    "\n",
    "print(task_0_label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(task_0_label_encoder.classes))\n",
    "print(task_0_label_encoder)\n",
    "task_0_label_encoder.fit(task_0['test'].targets)\n",
    "print(len(task_0_label_encoder.classes))\n",
    "print(task_0_label_encoder)\n",
    "task_0_label_encoder.fit(task_0['train'].targets)\n",
    "print(len(task_0_label_encoder.classes))\n",
    "print(task_0_label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task_0_label_encoder)\n",
    "\n",
    "\n",
    "\n",
    "import collections\n",
    "\n",
    "self = task_0_label_encoder\n",
    "y = task_0['test'].targets\n",
    "\n",
    "counts = collections.Counter(y)\n",
    "print(self.num_samples)\n",
    "self.num_samples += sum(counts.values())\n",
    "print(self.num_samples)\n",
    "\n",
    "classes = sorted(counts.keys())\n",
    "print(classes)\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_num_classes = len(self)\n",
    "# print(f\"old_num_classes={old_num_classes}\")\n",
    "# new_classes = sorted([label for label in classes if label not in self.classes])\n",
    "# print(f'new_classes={new_classes}')\n",
    "# print(f'len(new_classes)={len(new_classes)}')\n",
    "\n",
    "old_num_classes = len(self)\n",
    "print(f\"old_num_classes={old_num_classes}\")\n",
    "new_classes = []\n",
    "for label in classes:\n",
    "    if (label not in self.classes) and (label not in self.replace):\n",
    "        new_classes.append(label)\n",
    "# new_classes = sorted([label for label in classes if label not in self.classes])\n",
    "print(f'new_classes={new_classes}')\n",
    "print(f'len(new_classes)={len(new_classes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a992c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(new_classes):\n",
    "    self.class2idx[label] = old_num_classes + i\n",
    "print(self.class2idx)\n",
    "print(len(self.class2idx))\n",
    "\n",
    "self.index2class = {v: k for k, v in self.class2idx.items()}\n",
    "\n",
    "all_classes = []\n",
    "for label in self.class2idx.keys():\n",
    "    if label not in self.replace.keys():\n",
    "        all_classes.append(label)\n",
    "print(f\"all_classes={all_classes}\")\n",
    "print(f\"len(all_classes)={len(all_classes)}\")\n",
    "\n",
    "self.classes = sorted(all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5b49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.classes = [k for k in self.class2idx.keys() if k not in self.replace.keys()] \n",
    "print(len(self.class2idx), len(self.index2class), len(self.classes))\n",
    "\n",
    "self.replace_class2idx_items()\n",
    "print(len(self.class2idx), len(self.index2class), len(self.classes))\n",
    "\n",
    "new_classes = [c for c in new_classes if c not in self.replace.keys()]\n",
    "if len(new_classes):\n",
    "    log.debug(f\"[FITTING] {len(y)} samples with {len(classes)} classes, adding {len(new_classes)} new class labels. Latest num_classes = {len(self)}\")\n",
    "assert len(self) == (old_num_classes + len(new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16cdf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.index2class = {v: k for k, v in self.class2idx.items()}\n",
    "\n",
    "# all_classes = []\n",
    "\n",
    "self.classes = [k for k in self.class2idx.keys() if k not in self.replace.keys()]        \n",
    "self.replace_class2idx_items()\n",
    "\n",
    "new_classes = [c for c in new_classes if c not in self.replace.keys()]\n",
    "if len(new_classes):\n",
    "    log.debug(f\"[FITTING] {len(y)} samples with {len(classes)} classes, adding {len(new_classes)} new class labels. Latest num_classes = {len(self)}\")\n",
    "assert len(self) == (old_num_classes + len(new_classes))\n",
    "\n",
    "(task_0['test'].label_encoder)\n",
    "\n",
    "(task_0['test'].classes)\n",
    "\n",
    "\n",
    "\n",
    "sorted(set(datamodule.train_dataset.targets))\n",
    "\n",
    "len(sorted(set(datamodule.train_dataset.classes)))\n",
    "\n",
    "\n",
    "len(sorted(set(datamodule.train_dataset.classes)))\n",
    "\n",
    "\n",
    "datamodule.num_classes\n",
    "\n",
    "import numpy as np\n",
    "set(np.arange(len(datamodule.train_dataset.classes))) - set(datamodule.train_dataset.targets)\n",
    "\n",
    "import numpy as np\n",
    "set(datamodule.train_dataset.targets) - set(np.arange(len(datamodule.train_dataset.classes)))\n",
    "\n",
    "\n",
    "len(sorted(set(datamodule.train_dataset.targets)))\n",
    "\n",
    "model.metrics_train\n",
    "\n",
    "model.metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6a41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab322cc8",
   "metadata": {},
   "source": [
    "## Export experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4685f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path = \"/media/data/jacob/GitHub/lightning-hydra-classifiers/notebooks/experiments_August_2021/Extant-to-PNAS-512-transfer_benchmark/task_0/test.json\"\n",
    "\n",
    "from lightning_hydra_classifiers.utils.common_utils import LabelEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder.load(encoder_path)\n",
    "\n",
    "encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoder.class2idx)\n",
    "\n",
    "len(encoder.index2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(encoder.class2idx.keys()) - set(encoder.index2class.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285c136",
   "metadata": {},
   "source": [
    "```\n",
    "LabelEncoder:  \n",
    "    ::class2idx  \n",
    "        - Needs to have a many-to-one mapping from text to int. All int labels are unique, but multiple class names can map to 1 int label.\n",
    "    ::idx2class  \n",
    "        - Needs to have a one-to-one mapping from int to text. For ints that map to more than 1 text label, this maps the int only to the correct standardized label for the experiment. As in, it maps only to the label we used to replace another label. e.g. If replacements includes {\"NothoFagaceae\": \"Fagaceae\"}, and both  of them maps to int label 16, then encoder.idx2class[16] must return \"Fagaceae\".\n",
    "    ::classes  \n",
    "    ::num_classes  \n",
    "        - Needs to correspond to actual neural net output size, therefore excludes replaced classes\n",
    "    ::replacements  \n",
    "```\n",
    "\n",
    "* len(idx2class) <= len(class2idx)\n",
    "* num_classes == len(idx2class) <= len(class2idx)\n",
    "\n",
    "\n",
    "\n",
    "1. Initialize blank label encoder\n",
    "    - Provide replacements dict to allow backwards compatible mappings\n",
    "        - e.g. Nothofagaceae (newer, Extant) -> Fagaceae (older, PNAS)\n",
    "\n",
    "2. Fit encoder on $|y_s|$ to have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# x, y = make_classification(n_features=6, n_redundant=2, n_informative=4)\n",
    "import random\n",
    "\n",
    "\n",
    "# y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "loss = np.random.random(15)\n",
    "\n",
    "sorted_idx = np.argsort(loss)\n",
    "\n",
    "sorted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss[sorted_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea89b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_hydra_classifiers.experiments.transfer_experiment import TransferExperiment\n",
    "output_root_dir = \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/experimental_datasets\"\n",
    "experiment = TransferExperiment()\n",
    "experiment.export_experiment_spec(output_root_dir=output_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = experiment\n",
    "\n",
    "replace_class_indices = {\"Nothofagaceae\":\"Fagaceae\"}\n",
    "\n",
    "task_0 = self.setup_task_0()\n",
    "task_1 = self.setup_task_1()\n",
    "\n",
    "#         import pdb;pdb.set_trace()\n",
    "\n",
    "\n",
    "\n",
    "#         print(f\"__init__: {task_0['train'].label_encoder}\")\n",
    "task_0_label_encoder = task_0['train'].label_encoder\n",
    "task_0_label_encoder.__init__(replacements = replace_class_indices)\n",
    "\n",
    "# print(max(task_0_label_encoder.class2idx.values()))\n",
    "task_0_label_encoder.class2idx\n",
    "\n",
    "print(len(set(task_0['test'].targets)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(set(task_0_label_encoder.classes)))\n",
    "task_0_label_encoder.fit(task_0['test'].targets)\n",
    "print(len(set(task_0_label_encoder.classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be320eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(task_0_label_encoder.class2idx.values()))\n",
    "task_0_label_encoder.class2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32953ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(task_0['train'].targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(len(set(task_0_label_encoder.classes)))\n",
    "task_0_label_encoder.fit(task_0['train'].targets)\n",
    "print(len(set(task_0_label_encoder.classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(max(task_0_label_encoder.class2idx.values()))\n",
    "task_0_label_encoder.class2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_0_label_encoder.classes\n",
    "\n",
    "task_0_label_encoder.idx2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r \"/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/experimental_datasets/task_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26635dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self:\n",
    "    idx2class = {0:\"test\",\n",
    "                 1:\"2\"}\n",
    "\n",
    "\n",
    "\n",
    "old_highest_class = max(self.idx2class.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8459d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_highest_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d0f83",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Refactor LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numbers\n",
    "from typing import Union, List, Any, Tuple, Dict, Optional, Sequence\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from lightning_hydra_classifiers.utils import template_utils\n",
    "from lightning_hydra_classifiers.utils.plot_utils import colorbar\n",
    "\n",
    "\n",
    "log = template_utils.get_logger(__name__)\n",
    "\n",
    "\n",
    "# __all__ = [\"LabelEncoder\", \"trainval_split\", \"trainvaltest_split\", \"plot_split_distributions\", \"plot_class_distributions\",\n",
    "#            \"filter_df_by_threshold\", \"compute_class_counts\"]\n",
    "\n",
    "\n",
    "\n",
    "class LabelEncoder(object):\n",
    "    \n",
    "    \"\"\"Label encoder for tag labels.\"\"\"\n",
    "    def __init__(self,\n",
    "                 class2idx: Dict[str,int]=None,\n",
    "                 replacements: Optional[Dict[str,str]]=None):\n",
    "        self.class2idx = class2idx or {}\n",
    "        self.replacements = replacements or {}\n",
    "#         self.idx2class = {v: k for k, v in self.class2idx.items() if k not in self.replacements.keys()}\n",
    "#         self.classes = [k for k in self.class2idx.keys() if k not in self.replacements.keys()]\n",
    "        \n",
    "        assert len(self.classes) == len(self.idx2class) <= len(self.class2idx)\n",
    "        self.num_samples = 0\n",
    "        self.verbose=False\n",
    "        self.replace_class2idx_items()\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def idx2class(self):\n",
    "        return {v: k for k, v in self.class2idx.items() if k not in self.replacements.keys()}\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        return [k for k in self.class2idx.keys() if k not in self.replacements.keys()]\n",
    "    \n",
    "\n",
    "        \n",
    "    def replace_class2idx_items(self):\n",
    "        \"\"\"\n",
    "        Update inplace self.class2idx mappings, so that any class labels in self.replacements.keys()\n",
    "        map to the same int label as their corresponding value in self.replacements.values().\n",
    "        \n",
    "        \"\"\"\n",
    "        if (len(self.replacements) == 0) \\\n",
    "        or (len([k for k in self.replacements.keys() if k in self.class2idx.keys()]) == 0):\n",
    "            # No-op if replacements keys are empty or have zero overlap with class2idx keys.\n",
    "            return\n",
    "        \n",
    "        if self.verbose:\n",
    "            log.info(f'LabelEncoder replacing {len(self.replacements.keys())} class encodings with that other an another class')\n",
    "            log.info('Replacing: ' + str({k:v for k,v in self.replacements.items() if k in self.class2idx}))\n",
    "        for old, new in self.replacements.items():\n",
    "            if old in list(self.class2idx.keys()):\n",
    "                self.class2idx[old] = self.class2idx[new]\n",
    "#         self.idx2class = {v: k for k, v in self.class2idx.items()}\n",
    "#         self.classes = [k for k in self.class2idx.keys() if k not in self.replacements.keys()]                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idx2class)\n",
    "#         return len(self.classes)\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        msg = f\"<LabelEncoder(num_classes={len(self)})>\"\n",
    "        if len(self.replacements) > 0:\n",
    "            msg += \"\\n\" + f\"<num_replaced_classes={len(self.replacements)}>\"\n",
    "        return msg\n",
    "\n",
    "    def fit(self, y):\n",
    "        \n",
    "        counts = collections.Counter(y)\n",
    "        self.num_samples += sum(counts.values())\n",
    "        \n",
    "        classes = sorted(list(counts.keys()))\n",
    "        new_classes = sorted([label for label in classes if label not in self.classes])\n",
    "        \n",
    "        old_num_classes = len(self)\n",
    "        old_highest_class = max(self.idx2class.keys())\n",
    "        for i, label in enumerate(new_classes):\n",
    "            self.class2idx[label] = old_highest_class + i\n",
    "#         self.idx2class = {v: k for k, v in self.class2idx.items()}\n",
    "#         self.classes = [k for k in self.class2idx.keys() if k not in self.replacements.keys()]        \n",
    "        self.replace_class2idx_items()\n",
    "\n",
    "        new_classes = [c for c in new_classes if c not in self.replacements.keys()]\n",
    "        if len(new_classes):\n",
    "            log.debug(f\"[FITTING] {len(y)} samples with {len(classes)} classes, adding {len(new_classes)} new class labels. Latest num_classes = {len(self)}\")\n",
    "        assert len(self) == (old_num_classes + len(new_classes))\n",
    "        assert np.all([label in self.idx2class.values() for label in new_classes])\n",
    "        return self\n",
    "\n",
    "    def encode(self, y):\n",
    "        if not hasattr(y,\"__len__\"):\n",
    "            y = [y]\n",
    "#         print(self.class2idx)\n",
    "        return np.array([self.class2idx[label] for label in y])\n",
    "\n",
    "    def decode(self, y):\n",
    "        if not hasattr(y,\"__len__\"):\n",
    "            y = [y]\n",
    "        return np.array([self.idx2class[label] for label in y])\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, \"w\") as fp:\n",
    "            contents = self.getstate() # {\"class2idx\": self.class2idx}\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, \"r\") as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)\n",
    "    \n",
    "    def getstate(self):\n",
    "        return {\"class2idx\": self.class2idx,\n",
    "                \"replacements\": self.replacements}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        disp = f\"\"\"<{str(type(self)).strip(\"'>\").split('.')[-1]}>:\\n\"\"\"\n",
    "        disp += f\"    num_classes: {len(self)}\\n\"\n",
    "        disp += f\"    fit on num_samples: {self.num_samples}\"\n",
    "        return disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from lightning_hydra_classifiers.utils.common_utils import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "experiment = TransferExperiment()\n",
    "task_0 = experiment.setup_task_0()\n",
    "\n",
    "old_encoder = task_0['train'].label_encoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "print(f\"Old:\", old_encoder)\n",
    "print(\"New, initialized:\", encoder)\n",
    "\n",
    "data = task_0['train']\n",
    "data_df = data.samples_df\n",
    "\n",
    "experiment\n",
    "\n",
    "replace_class_indices = {\"Nothofagaceae\":\"Fagaceae\"}\n",
    "\n",
    "(data.label_encoder.replace)\n",
    "\n",
    "data_df\n",
    "\n",
    "data_df = data_df.rename(columns={\"family\":\"newest_family\"})\n",
    "data_df = data_df.assign(family = data_df.newest_family.replace(replace_class_indices))\n",
    "data_df\n",
    "\n",
    "\n",
    "# data_df = data_df.sort_values(\"family\").astype({\"family\":pd.CategoricalDtype(),\n",
    "#                           \"newest_family\":pd.CategoricalDtype()}) #family.cat #!=data_df.newest_family]\n",
    "\n",
    "data_df = data_df.sort_values(\"family\").astype({\"family\":pd.CategoricalDtype()})\n",
    "data_df = data_df.convert_dtypes()\n",
    "data_df.info()\n",
    "\n",
    "dir(data_df.family[~data_df.family.duplicated(keep='first')])\n",
    "# data_df.family.duplicated(keep='first')\n",
    "\n",
    "y_col = \"family\"\n",
    "category_df = data_df[y_col][~data_df[y_col].duplicated(keep='first')]   #.to_list()\n",
    "class2index = dict(zip(category_df.to_list(), category_df.cat.codes.to_list()))\n",
    "# data_df.family.duplicated(keep='first')\n",
    "\n",
    "class2index\n",
    "\n",
    "data_df.info()\n",
    "\n",
    "dir(data_df.family.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(data_df.family.cat.codes))\n",
    "\n",
    "data_df.family.cat.ordered\n",
    "\n",
    "data_df.family.cat.codes\n",
    "\n",
    "data_df[data_df.family!=data_df.newest_family]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954090ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1c627",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BYOL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb25b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load and preprocess pre-formatted csv datasets and create train val test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47399bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "\n",
    "from torchvision.models import mobilenet_v2, resnet50\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from lightning_hydra_classifiers.train_BYOL import *\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from munch import Munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "if 'TOY_DATA_DIR' not in os.environ: \n",
    "    print(f\"Setting env variable $TOY_DATA_DIR={os.environ['TOY_DATA_DIR']}\")\n",
    "    os.environ['TOY_DATA_DIR'] = \"/media/data_cifs/projects/prj_fossils/data/toy_data\"\n",
    "default_root_dir = os.environ['TOY_DATA_DIR']\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "config = Munch({\"dataset_name\":\"Extant-PNAS\",\n",
    "                \"model\":{\n",
    "                    \"backbone\":\"resnet50\"}\n",
    "               })\n",
    "# config = Munch({\"dataset_name\":\"STL10\"})\n",
    "print(f\"config: {config}\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose([ToTensor(),\n",
    "                               normalize])\n",
    "\n",
    "\n",
    "\n",
    "if config.dataset_name == \"STL10\":\n",
    "    from torchbearer.cv_utils import DatasetValidationSplitter\n",
    "\n",
    "    train_data = STL10(os.environ['TOY_DATA_DIR'], split='train', transform=transform, download=True)\n",
    "    test_data = STL10(os.environ['TOY_DATA_DIR'], split='test', transform=transform, download=True)\n",
    "    \n",
    "    splitter = DatasetValidationSplitter(len(train_data), 0.1)\n",
    "    train_set = splitter.get_train_dataset(train_data)\n",
    "    val_set = splitter.get_val_dataset(train_data)\n",
    "    \n",
    "else:\n",
    "    exp = TransferExperiment()\n",
    "    task_0, task_1 = exp.get_multitask_datasets(train_transform=transform,\n",
    "                                                val_transform=transform)\n",
    "    train_data, val_data, test_data = task_0[\"train\"], task_0[\"val\"], task_0[\"test\"]\n",
    "    train_set, val_set = train_data, val_data\n",
    "\n",
    "classes = train_data.classes\n",
    "num_classes = len(classes)\n",
    "print('\\n List of all classes: ')\n",
    "print(classes)\n",
    "print(f\"len(classes)={len(classes)}\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "num_workers = 2\n",
    "pin_memory = False\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(train_set, pin_memory=pin_memory, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)\n",
    "val_gen = torch.utils.data.DataLoader(val_set, pin_memory=pin_memory, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "test_gen = torch.utils.data.DataLoader(test_data, pin_memory=pin_memory, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca10ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = PyCM().on_val().to_html_file('cm.{epoch}')\n",
    "\n",
    "# We copy the final layer form MobileNetV2 and replace the linear layer with one to 10 channels\n",
    "\n",
    "if config.model.backbone == \"mobilenet_v2\":\n",
    "    model = mobilenet_v2(pretrained=True, progress=False)\n",
    "    model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(model.last_channel, num_classes),\n",
    "            )\n",
    "\n",
    "elif config.model.backbone == \"resnet50\":\n",
    "    model = resnet50(pretrained=True, progress=False)\n",
    "    model.fc = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(model.fc.in_features, num_classes+1),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcea810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchsummary\n",
    "# import torchinfo\n",
    "import torch.optim as optim\n",
    "# import torchbearer\n",
    "# from torchbearer import Trial\n",
    "# from torchbearer.callbacks import PyCM\n",
    "\n",
    "# for k, m in model.named_modules():\n",
    "#     if (k.startswith(\"features\")) or (k.startswith(\"layer\")):\n",
    "#         print(f\"Freezing: {k}\")\n",
    "#         m.requires_grad = False\n",
    "#     else:\n",
    "#         print(f\"Unfreezing: {k}\")\n",
    "#         m.requires_grad = True\n",
    "        \n",
    "#     print(f\"{k} : {m.requires_grad}\")\n",
    "\n",
    "freeze_at = \"fc\"\n",
    "\n",
    "freeze_current=False\n",
    "for k, m in model.named_modules():\n",
    "    if (k == freeze_at) or freeze_current:\n",
    "        freeze_current=True\n",
    "        print(f\"Unfreezing: {k}\")\n",
    "        m.requires_grad = True\n",
    "    else:\n",
    "        print(f\"Freezing: {k}\")\n",
    "        m.requires_grad = False\n",
    "        \n",
    "    print(f\"{k} : {m.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c810b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = PyCM().on_val().to_pyplot(normalize=True, title='Confusion Matrix: {epoch}')\n",
    "# cm_csv = PyCM().on_val().to_csv_file(\"cm_{epoch}\")\n",
    "# model = SimpleModel()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# trial = Trial(model, optimizer, loss, metrics=['acc', 'loss'], callbacks=[cm,cm_csv]).to(device)\n",
    "# trial.with_generators(train_generator=train_gen, val_generator=val_gen)\n",
    "# history = trial.run(epochs=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0280be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelfSupervisedLearner(\n",
    "    resnet,\n",
    "    image_size = IMAGE_SIZE,\n",
    "    hidden_layer = 'avgpool',\n",
    "    projection_size = 256,\n",
    "    projection_hidden_size = 4096,\n",
    "    moving_average_decay = 0.99\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus = NUM_GPUS,\n",
    "    max_epochs = EPOCHS,\n",
    "    accumulate_grad_batches = 1,\n",
    "    sync_batchnorm = True\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d752d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata\n",
    "\n",
    "# class UnsupervisedDatasetWrapper(torchdata.datasets.Files):\n",
    "class UnsupervisedDatasetWrapper(torchvision.datasets.ImageFolder):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index][0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = \"<UnsupervisedDatasetWrapper>\\n\"\n",
    "        out += self.dataset.__repr__()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_0['test'][0]\n",
    "\n",
    "for subset in [\"train\",\"val\",\"test\"]:\n",
    "    task_0[subset] = UnsupervisedDatasetWrapper(task_0[subset])\n",
    "    task_1[subset] = UnsupervisedDatasetWrapper(task_1[subset])\n",
    "\n",
    "task_0['test'][0]\n",
    "\n",
    "type(task_0['test'].dataset)\n",
    "\n",
    "task_0['test']#.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3965e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c457186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from typing import *\n",
    "\n",
    "\n",
    "totensor: Callable = torchvision.transforms.ToTensor()\n",
    "\n",
    "def toPIL(img: torch.Tensor, mode=\"RGB\") -> Callable:\n",
    "    return torchvision.transforms.ToPILImage(mode)\n",
    "\n",
    "\n",
    "def normalize_transform(mean = [0.485, 0.456, 0.406],\n",
    "                        std = [0.229, 0.224, 0.225]) -> Callable:\n",
    "    return transforms.Normalize(mean=mean,\n",
    "                                std=std)\n",
    "\n",
    "def default_train_transforms(image_size: int=224,\n",
    "                             normalize: bool=True, \n",
    "                             augment:bool=True,\n",
    "                             grayscale: bool=True,\n",
    "                             channels: Optional[int]=3,\n",
    "                             mean = [0.485, 0.456, 0.406],\n",
    "                             std = [0.229, 0.224, 0.225]):\n",
    "    \"\"\"Subclasses can override this or user can provide custom transforms at runtime\"\"\"\n",
    "    transform_list = []\n",
    "#         transform_jit_list = []\n",
    "    resize_PIL = not augment\n",
    "    if augment:\n",
    "        transform_list.extend([transforms.RandomResizedCrop(size=image_size,\n",
    "                                                            scale=(0.25, 1.2),\n",
    "                                                            ratio=(0.7, 1.3),\n",
    "                                                            interpolation=2),\n",
    "                               totensor\n",
    "                             ])\n",
    "    return default_eval_transforms(image_size=image_size,\n",
    "                                        normalize=normalize,\n",
    "                                        resize_PIL=resize_PIL,\n",
    "                                        grayscale=grayscale,\n",
    "                                        channels=channels,\n",
    "                                        transform_list=transform_list,\n",
    "                                        mean=mean,\n",
    "                                        std=std)\n",
    "\n",
    "def default_eval_transforms(image_size: int=224,\n",
    "                            image_buffer_size: int=32,\n",
    "                            normalize: bool=True,\n",
    "                            resize_PIL: bool=True,\n",
    "                            grayscale: bool=True,\n",
    "                            channels: Optional[int]=3,\n",
    "                            transform_list: Optional[List[Callable]]=None,\n",
    "                            mean = [0.485, 0.456, 0.406],\n",
    "                            std = [0.229, 0.224, 0.225]):\n",
    "    \"\"\"Subclasses can override this or user can provide custom transforms at runtime\"\"\"\n",
    "    transform_list = transform_list or []\n",
    "    transform_jit_list = []\n",
    "\n",
    "    if resize_PIL:\n",
    "        # if True, assumes input images are PIL.Images (But need to check if this even matters.)\n",
    "        # if False, expects input images to already be torch.Tensors\n",
    "        transform_list.extend([transforms.Resize(image_size+image_buffer_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               totensor])\n",
    "    if normalize:\n",
    "        transform_jit_list.append(normalize_transform(mean, std))\n",
    "\n",
    "    if grayscale:\n",
    "        transform_jit_list.append(transforms.Grayscale(num_output_channels=channels))\n",
    "\n",
    "    return transforms.Compose([*transform_list, *transform_jit_list])\n",
    "\n",
    "\n",
    "def get_default_transforms(image_size: int=224,\n",
    "                           normalize: bool=True,\n",
    "                           augment:bool=True,\n",
    "                           grayscale: bool=True,\n",
    "                           channels: Optional[int]=3,\n",
    "                           mean = [0.485, 0.456, 0.406],\n",
    "                           std = [0.229, 0.224, 0.225]):\n",
    "\n",
    "    \n",
    "    train_transform = default_train_transforms(image_size=image_size,\n",
    "                                               normalize=normalize,\n",
    "                                               augment=augment,\n",
    "                                               grayscale=grayscale,\n",
    "                                               channels=channels,\n",
    "                                               mean=mean,\n",
    "                                               std=std)\n",
    "    eval_transform = default_eval_transforms(image_size=image_size,\n",
    "                                             image_buffer_size=32,\n",
    "                                             normalize=normalize,\n",
    "                                             resize_PIL=not augment,\n",
    "                                             grayscale=grayscale,\n",
    "                                             channels=channels,\n",
    "                                             transform_list=None,\n",
    "                                             mean=mean,\n",
    "                                             std=std)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_transform, eval_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dedb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, val_transform = get_default_transforms(image_size=224,\n",
    "                                                         normalize=True,\n",
    "                                                         augment=True,\n",
    "                                                         grayscale=True,\n",
    "                                                         channels=3,\n",
    "                                                         mean = [0.485, 0.456, 0.406],\n",
    "                                                         std = [0.229, 0.224, 0.225])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c0686ae52b501c5138d1ad3c292b1aad199ba0a61e288abba1616901d57bf21"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('sequoia': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd01c0686ae52b501c5138d1ad3c292b1aad199ba0a61e288abba1616901d57bf21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
